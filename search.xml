<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2022/06/03/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>PCA</title>
    <url>/2022/06/03/PCA/</url>
    <content><![CDATA[<h1 id="1-原理简述"><a href="#1-原理简述" class="headerlink" title="1    原理简述"></a>1    原理简述</h1><p>&emsp;&emsp;Self-Attention Layer 一次检查同一句子中的所有单词的注意力，这使得它成为一个简单的矩阵计算，并且能够在计算单元上并行计算。 此外，Self-Attention Layer 可以使用下面提到的 Multi-Head 架构来拓宽视野，也就是多头注意力机制。Self-Attention Layer 基本结构如下：<br><img src="https://img-blog.csdnimg.cn/fc65b9f0024549318aad9019931c293a.png" alt="在这里插入图片描述"></p>
<p>对于每个输入 $\boldsymbol{x}$，首先经过 <strong>Embedding</strong> 层对每个输入进行编码得到 $\boldsymbol{a_1,a_2,a_3,a_4}$，后将输入特征经过三个全连接层分别得到 <strong>Query，Key，Value</strong>：</p>
<ul>
<li>$\boldsymbol{q^i(Query) = W^q a^i}$；</li>
<li>$\boldsymbol{k^i(Key) = W^k a^i}$；</li>
<li>$\boldsymbol{v^i(Value) = W^v a^i}$。</li>
</ul>
<p>$\boldsymbol{W^q, W^k,W^v}$ 由网络训练而来。注意力矩阵是由 Query 和 Key 计算得到，方式由许多种，如点积、缩放点积等。Value 可以看作是信息提取器，将根据单词的注意力提取一个唯一的值，也即某个特征有多少成分被提取出来。下面计算一种注意力矩阵的方式：缩放点积。<br><img src="https://img-blog.csdnimg.cn/6385301128964680b30be08f2ac35638.gif#pic_center" alt="在这里插入图片描述"><br>注意力矩阵 $\boldsymbol{A}$ 定义为 Query (giver) 和 Key (receiver) 的内积除以其维度的平方根。 每个单词通过提供 Query 来匹配作为注意力的目标单词的 Key，从而对所有单词产生注意力。为防止注意力分数随维度增大而增大，让注意力矩阵除以向量的维度的开方。 然后对得到的注意力矩阵 $\boldsymbol{A}$ 进行 <strong>Softmax</strong> 归一化得到 $\boldsymbol{\hat{A}}$，最后将  $\boldsymbol{\hat{A}}$ 乘以 Value 矩阵并相加得到最终的特征 $\boldsymbol{b}$。<br><img src="https://img-blog.csdnimg.cn/9c375b3ad70240f1b2014231cbdd5e14.gif#pic_center" alt="在这里插入图片描述"></p>
<p>矩阵化如下：<br><img src="https://img-blog.csdnimg.cn/8b635a6a21ef402e8702d8f191da661a.png" alt="在这里插入图片描述"></p>
<p>在上述的 self-attention 中，我们最终只得到一个注意力矩阵，也就是说这个注意力矩阵所关注的信息只偏句子之间的一种关系，但是在时序序列中，往往特征之间不止一种关系，所以我们要提取多个注意力矩阵，这样可以捕获更多的信息，这种注意力机制也就是 <strong>多头注意力机制(Multi-Heads)</strong>。在实现过程中，我们只需要将原始的 $\boldsymbol{q^i,k^i,v^i}$ 分裂为 $\boldsymbol{n}$ 个就得到 $\boldsymbol{n}$ 头自注意力机制了。<br><img src="https://img-blog.csdnimg.cn/6ba45518a73649e9818594897369ff57.gif#pic_center" alt="在这里插入图片描述"></p>
<h1 id="2-PyTorch-实现"><a href="#2-PyTorch-实现" class="headerlink" title="2    PyTorch 实现"></a>2    PyTorch 实现</h1><p>定义 num_attention_heads 为注意力机制的头数，input_size 为输入特征维度，hidden_size 为 $\boldsymbol{q^i,k^i,v^i}$ 的总维度，这样每个头的维度也可以求出，定义为 attention_head_size：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.num_attention_heads = num_attention_heads</span><br><span class="line">self.attention_head_size = <span class="built_in">int</span>(hidden_size / num_attention_heads)</span><br><span class="line">self.all_head_size = hidden_size</span><br></pre></td></tr></table></figure>
<p>定义 $\boldsymbol{W^q, W^k,W^v}$，通过全连接网络生成：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.key_layer = nn.Linear(input_size, hidden_size)</span><br><span class="line">self.query_layer = nn.Linear(input_size, hidden_size)</span><br><span class="line">self.value_layer = nn.Linear(input_size, hidden_size)</span><br></pre></td></tr></table></figure>
<p>使用输入特征乘 $\boldsymbol{W^q, W^k,W^v}$ 得到 <strong>Query，Key，Value</strong> 矩阵，维度为 $(batch\_size,seq\_len, hidden\_size)$：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">key = self.key_layer(x)</span><br><span class="line">query = self.query_layer(x)</span><br><span class="line">value = self.value_layer(x)</span><br></pre></td></tr></table></figure>
<p>求多头注意力机制的 $\boldsymbol{W^q, W^k,W^v}$，头数为 num_attention_heads，并要调换维度，即将 $seq\_len$ 维度与 $num\_attention\_heads$ 维度对换，最终 $\boldsymbol{W^q, W^k,W^v}$ 维度为 $(batch\_size,num\_attention\_heads,seq\_len,attention\_head\_size)$：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trans_to_multiple_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">    new_size = x.size()[ : -<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">    x = x.view(new_size)</span><br><span class="line">    <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">key_heads = self.trans_to_multiple_heads(key)</span><br><span class="line">query_heads = self.trans_to_multiple_heads(query)</span><br><span class="line">value_heads = self.trans_to_multiple_heads(value)</span><br></pre></td></tr></table></figure>
<p>将 $\boldsymbol{Q}$ 和 $\boldsymbol{K}$ 矩阵做点积运算，并进行缩放，得到注意力矩阵的维度为 $(batch\_size,num\_attention\_heads,seq\_len,seq\_len)$：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_scores = torch.matmul(query_heads, key_heads.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br></pre></td></tr></table></figure>
<p>对注意力矩阵进行归一化，归一化的维度为 3，矩阵的维度不发生变化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention_probs = F.softmax(attention_scores, dim = -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>将注意力矩阵乘以矩阵 $\boldsymbol{V}$，得到输出特征，维度为 $(batch\_size,num\_attention\_heads,seq\_len,attention\_head\_size)$：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">context = torch.matmul(attention_probs, value_heads)</span><br></pre></td></tr></table></figure>
<p>将各头的注意力矩阵进行拼接，contiguous() 是将 tensor 的内存变成连续的，否则进行 view 操作时会报错，至于原因可参考：<a href="https://blog.csdn.net/kdongyi/article/details/108180250"><strong>https://blog.csdn.net/kdongyi/article/details/108180250</strong></a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">context = context.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">new_size = context.size()[ : -<span class="number">2</span>] + (self.all_head_size , )</span><br><span class="line">context = context.view(*new_size)</span><br></pre></td></tr></table></figure>
<p>全部代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">selfAttention</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_attention_heads, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(selfAttention, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span> :</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;the hidden size %d is not a multiple of the number of attention heads&quot;</span></span><br><span class="line">                <span class="string">&quot;%d&quot;</span> % (hidden_size, num_attention_heads)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(hidden_size / num_attention_heads)</span><br><span class="line">        self.all_head_size = hidden_size</span><br><span class="line"></span><br><span class="line">        self.key_layer = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.query_layer = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.value_layer = nn.Linear(input_size, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">trans_to_multiple_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        new_size = x.size()[ : -<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">        x = x.view(new_size)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        key = self.key_layer(x)</span><br><span class="line">        query = self.query_layer(x)</span><br><span class="line">        value = self.value_layer(x)</span><br><span class="line"></span><br><span class="line">        key_heads = self.trans_to_multiple_heads(key)</span><br><span class="line">        query_heads = self.trans_to_multiple_heads(query)</span><br><span class="line">        value_heads = self.trans_to_multiple_heads(value)</span><br><span class="line"></span><br><span class="line">        attention_scores = torch.matmul(query_heads, key_heads.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">        attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line"></span><br><span class="line">        attention_probs = F.softmax(attention_scores, dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        context = torch.matmul(attention_probs, value_heads)</span><br><span class="line">        context = context.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_size = context.size()[ : -<span class="number">2</span>] + (self.all_head_size , )</span><br><span class="line">        context = context.view(*new_size)</span><br><span class="line">        <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure>
<p>测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features = torch.rand((<span class="number">32</span>, <span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">attention = selfAttention(<span class="number">2</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">result = attention.forward(features)</span><br><span class="line"><span class="built_in">print</span>(result.shape)</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">32</span>, <span class="number">20</span>, <span class="number">20</span>])</span><br></pre></td></tr></table></figure>
<p>参考：<br><a href="https://blog.csdn.net/beilizhang/article/details/115282604"><strong><em>https://blog.csdn.net/beilizhang/article/details/115282604</em></strong></a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>主成分分析</tag>
      </tags>
  </entry>
</search>
