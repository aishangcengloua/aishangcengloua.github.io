<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Adult 数据集分析及四种模型实现</title>
    <url>/2022/06/05/Adult-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E6%9E%90%E5%8F%8A%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h1 id="一、数据集"><a href="#一、数据集" class="headerlink" title="一、数据集"></a>一、数据集</h1><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>Adult数据集是一个经典的数据挖掘项目的的数据集，该数据从美国1994年人口普查数据库中抽取而来，因此也称作“人口普查收入”数据集，共包含48842条记录，年收入大于 50k 的占比23.93%年收入小于 50k 的占比76.07%，数据集已经划分为训练数据32561条和测试数据16281条。该数据集类变量为年收入是否超过 50k ，属性变量包括年龄、工种、学历、职业等14类重要信息，其中有8类属于类别离散型变量，另外6类属于数值连续型变量。该数据集是一个分类数据集，用来预测年收入是否超过50k。下载地址<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/adult/">点这里</a>。<br><img src="https://img-blog.csdnimg.cn/f8c920dc998447a291c4f6636dedef53.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f836e82e69cd4328a60298deab1d6b67.png" alt="在这里插入图片描述"></p>
<h2 id="数据集预处理及分析"><a href="#数据集预处理及分析" class="headerlink" title="数据集预处理及分析"></a>数据集预处理及分析</h2><p>因为是csv数据，所以主要采用pandas和numpy库来进行预处理，首先数据读取以及查看是否有缺失：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;adult.csv&#x27;</span>, header = <span class="literal">None</span>, names = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>,  <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;capital-gain&#x27;</span>, <span class="string">&#x27;capital-loss&#x27;</span>, <span class="string">&#x27;hours-per-week&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line">df.head()</span><br><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/c01c6b52f812443e94e7cde5f41d2fcf.png" alt="在这里插入图片描述"><br>虽然上面查看数据是没有缺失值的，但其实是因为缺失值的是” ?”，而info()检测的是NaT或者Nan的缺失值。注意问号前面还有空格。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.apply(<span class="keyword">lambda</span> x : np.<span class="built_in">sum</span>(x == <span class="string">&quot; ?&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2150770204c94f58a9806104e990b98f.png" alt="在这里插入图片描述"><br>分别是居民的工作类型workclass（离散型）缺1836、职业occupation（离散型）缺1843和国籍native-country（离散型)缺583。离散值一般填充众数，但是在此之前要先将缺失值转化成nan或者NaT。同时因为收入可以分为两种类型，则将&gt;50K的替换成1，&lt;=50K的替换成0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.replace(<span class="string">&quot; ?&quot;</span>, pd.NaT, inplace = <span class="literal">True</span>)</span><br><span class="line">df.replace(<span class="string">&quot; &gt;50K&quot;</span>, <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">df.replace(<span class="string">&quot; &lt;=50K&quot;</span>, <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">trans = &#123;<span class="string">&#x27;workclass&#x27;</span> : df[<span class="string">&#x27;workclass&#x27;</span>].mode()[<span class="number">0</span>], <span class="string">&#x27;occupation&#x27;</span> : df[<span class="string">&#x27;occupation&#x27;</span>].mode()[<span class="number">0</span>], <span class="string">&#x27;native-country&#x27;</span> : df[<span class="string">&#x27;native-country&#x27;</span>].mode()[<span class="number">0</span>]&#125;</span><br><span class="line">df.fillna(trans, inplace = <span class="literal">True</span>)</span><br><span class="line">df.describe()</span><br></pre></td></tr></table></figure>
<center>

<p><img src="https://img-blog.csdnimg.cn/ea9b9fcacbad4891b1a0a2027c344ac6.png" alt="在这里插入图片描述"><br>由上表可知，75%以上的人是没有资本收益和资本输出的，所以这两列是属于无关属性的，此外还包括序号列，应删除这三列。所以我们只需关注这三列之外的数据即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop(<span class="string">&#x27;fnlwgt&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">df.drop(<span class="string">&#x27;capital-gain&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">df.drop(<span class="string">&#x27;capital-loss&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/cf2d1072b2ad4d4ab88ce9eb8d06c96d.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(df[<span class="string">&quot;income&quot;</span>], df[<span class="string">&quot;age&quot;</span>])</span><br><span class="line">plt.grid(b = <span class="literal">True</span>, which = <span class="string">&quot;major&quot;</span>, axis = <span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Income distribution by age (1 is &gt;50K)&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/0206fafefe1543feb5a484bd6746363a.png" alt="在这里插入图片描述"><br>能看出对于中高年龄的人来说收入&gt;50K是比&lt;=50K的少。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&quot;workclass&quot;</span>].value_counts()</span><br><span class="line"></span><br><span class="line">income_0 = df[<span class="string">&quot;workclass&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[<span class="string">&quot;workclass&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&quot; &gt;50K&quot;</span> : income_1, <span class="string">&quot; &lt;=50K&quot;</span> : income_0&#125;)</span><br><span class="line">df1.plot(kind = <span class="string">&#x27;bar&#x27;</span>, stacked = <span class="literal">True</span>)</span><br><span class="line">plt.title(<span class="string">&quot;income distribution by Workclass&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;workclass&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;number of person&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/39f0c148721948ad873132c0f3fbf369.png" alt="在这里插入图片描述"><br>观察工作类型对年收入的影响。工作类别为Private的人在两种年收入中都是最多的，但是&gt;50K和&lt;=50K的比例最高的是Self-emp-inc。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = df[<span class="string">&quot;hours-per-week&quot;</span>].groupby(df[<span class="string">&quot;workclass&quot;</span>]).agg([<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>])</span><br><span class="line">df1.sort_values(by = <span class="string">&#x27;mean&#x27;</span>, ascending = <span class="literal">False</span>)</span><br><span class="line">df1</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/d002e268383d4b6d8548777969aaeba1.png" alt="在这里插入图片描述"><br>用工作类别对每周工作时间进行分组，计算每组的均值，最大、小值，并且按均值进行排序。能看出工作类别是Federal-gov的人平均工作时间最长，但其的高收入占比并不是最高的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">income_0 = df[<span class="string">&quot;education&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[<span class="string">&quot;education&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&quot; &gt;50K&quot;</span> : income_1, <span class="string">&quot; &lt;=50K&quot;</span> : income_0&#125;)</span><br><span class="line">df1.plot(kind = <span class="string">&#x27;bar&#x27;</span>, stacked = <span class="literal">True</span>)</span><br><span class="line">plt.title(<span class="string">&quot;income distribution by Workclass&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;education&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;number of person&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/3e29faaf457b4a219b78bd23d824042b.png" alt="在这里插入图片描述"><br>统计受教育程度对年收入的影响，对于程度是Bachelors来说，两种收入的人数是比较接近的，收入比也是最大的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">income_0 = df[<span class="string">&quot;education-num&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>]</span><br><span class="line">income_1 = df[<span class="string">&quot;education-num&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>]</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df1.plot(kind = <span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;education of income&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;education-num&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/e9b3bec32ed744faa009e8d4d8aa8b4f.png" alt="在这里插入图片描述"><br>统计受教育时间对收入的影响的概率密度图。大约在时间的中值的时段，收入&gt;50K的人是比&lt;=50K的概率要低一些，而在中值偏右的时段是相反的，在其余时段，两种收入大约是处于平衡的状态。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fig, ([[ax1, ax2, ax3], [ax4, ax5, ax6]]) = plt.subplots(2, 3, figsize=(15, 10))</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">231</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; White&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; White&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df1.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax1)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;income of relationship by race_White&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">232</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Black&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Black&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df2.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax2)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;income of relationship by race_Black&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax3 = fig.add_subplot(<span class="number">233</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Asian-Pac-Islander&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Asian-Pac-Islander&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df3 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df3.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax3)</span><br><span class="line">ax3.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax3.set_title(<span class="string">&#x27;income of relationship by race_Asian-Pac-Islander&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax4 = fig.add_subplot(<span class="number">234</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Amer-Indian-Eskimo&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Amer-Indian-Eskimo&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df4 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df4.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax4)</span><br><span class="line">ax4.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax4.set_title(<span class="string">&#x27;income of relationship by race_Amer-Indian-Eskimo&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax5 = fig.add_subplot(<span class="number">235</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Other&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;race&quot;</span>] == <span class="string">&#x27; Other&#x27;</span>][<span class="string">&quot;relationship&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df5 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df5.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax5)</span><br><span class="line">ax5.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax5.set_title(<span class="string">&#x27;income of relationship by race_Other&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/3a6aa0d93f3c48199bbf24d418d7ac9e.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/c685ffc9d0bf4dc68c4e5acd0be849f6.png" alt="在这里插入图片描述"><br>这里主要是做了不同种族扮演的社会角色的收入状况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fig, ([[ax1, ax2, ax3], [ax4, ax5, ax6]]) = plt.subplots(2, 3, figsize=(10, 5))</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">121</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;sex&quot;</span>] == <span class="string">&#x27; Male&#x27;</span>][<span class="string">&quot;occupation&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;sex&quot;</span>] == <span class="string">&#x27; Male&#x27;</span>][<span class="string">&quot;occupation&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df1.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax1)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;income of occupation by sex_Male&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">122</span>) </span><br><span class="line">income_0 = df[df[<span class="string">&quot;sex&quot;</span>] == <span class="string">&#x27; Female&#x27;</span>][<span class="string">&quot;occupation&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">0</span>].value_counts()</span><br><span class="line">income_1 = df[df[<span class="string">&quot;sex&quot;</span>] == <span class="string">&#x27; Female&#x27;</span>][<span class="string">&quot;occupation&quot;</span>][df[<span class="string">&quot;income&quot;</span>] == <span class="number">1</span>].value_counts()</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">&#x27; &gt;50K&#x27;</span> : income_1, <span class="string">&#x27; &lt;=50K&#x27;</span> : income_0&#125;)</span><br><span class="line">df2.plot(kind = <span class="string">&#x27;bar&#x27;</span>, ax = ax2)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;number of person&#x27;</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;income of occupation by sex_Female&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/e6a6b90390384671b67b8b757f0ef48d.png" alt="在这里插入图片描述"><br>这里主要是做了不同性别的职业的收入状况。在男性中，职业为Exec-managerial的人中，收入&gt;50K的人要比&lt;=50K的人要多，而这种情况在女性中刚好相反。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_object_col = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> df[col].dtype.name == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line">df_int_col = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> df[col].dtype.name != <span class="string">&#x27;object&#x27;</span> <span class="keyword">and</span> col != <span class="string">&#x27;income&#x27;</span>]</span><br><span class="line">target = df[<span class="string">&quot;income&quot;</span>]</span><br><span class="line">dataset = pd.concat([df[df_int_col], pd.get_dummies(df[df_object_col])], axis = <span class="number">1</span>)</span><br><span class="line">dataset.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/d2d184c6574a456399f59b6c06b1c35b.png" alt="在这里插入图片描述"><br>先对数据类型进行统计，对非数值型的数据进行独热编码，再将两者进行拼接。最后将收入与其他数据分开分别作为标签和训练集或者测试集。</p>
<h1 id="二、四种模型对上述数据集进行预测"><a href="#二、四种模型对上述数据集进行预测" class="headerlink" title="二、四种模型对上述数据集进行预测"></a>二、四种模型对上述数据集进行预测</h1><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p><em>导入相关包</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br></pre></td></tr></table></figure>
<p>数据预处理，要注意的是训练集和测试集进行独热编码之后可能形状不一样，所以要将他们进行配对；再者是因为我们要给缺失某列的数据进行增加全为零的列，奇怪的是当从DataFrame类型转到Numpy类型时全为零的列会全部变成nan，所以还要重新nan的列转成零。否则在预测的过程网络的输出会全部为nan。本次实验将训练集进行2 : 8的数据划分，2份作为验证集。且要对数据集进行归一化，效果会好很多。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_missing_columns</span>(<span class="params">d, columns</span>) :</span><br><span class="line">    missing_col = <span class="built_in">set</span>(columns) - <span class="built_in">set</span>(d.columns)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> missing_col :</span><br><span class="line">        d[col] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fix_columns</span>(<span class="params">d, columns</span>):  </span><br><span class="line">    add_missing_columns(d, columns)</span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">set</span>(columns) - <span class="built_in">set</span>(d.columns) == <span class="built_in">set</span>())</span><br><span class="line">    d = d[columns]</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_process</span>(<span class="params">df, model</span>) :</span><br><span class="line">    df.replace(<span class="string">&quot; ?&quot;</span>, pd.NaT, inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> model == <span class="string">&#x27;train&#x27;</span> :</span><br><span class="line">        df.replace(<span class="string">&quot; &gt;50K&quot;</span>, <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">        df.replace(<span class="string">&quot; &lt;=50K&quot;</span>, <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> model == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        df.replace(<span class="string">&quot; &gt;50K.&quot;</span>, <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">        df.replace(<span class="string">&quot; &lt;=50K.&quot;</span>, <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    trans = &#123;<span class="string">&#x27;workclass&#x27;</span> : df[<span class="string">&#x27;workclass&#x27;</span>].mode()[<span class="number">0</span>], <span class="string">&#x27;occupation&#x27;</span> : df[<span class="string">&#x27;occupation&#x27;</span>].mode()[<span class="number">0</span>], <span class="string">&#x27;native-country&#x27;</span> : df[<span class="string">&#x27;native-country&#x27;</span>].mode()[<span class="number">0</span>]&#125;</span><br><span class="line">    df.fillna(trans, inplace = <span class="literal">True</span>)</span><br><span class="line">    df.drop(<span class="string">&#x27;fnlwgt&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    df.drop(<span class="string">&#x27;capital-gain&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    df.drop(<span class="string">&#x27;capital-loss&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    df_object_col = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> df[col].dtype.name == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line">    df_int_col = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> df[col].dtype.name != <span class="string">&#x27;object&#x27;</span> <span class="keyword">and</span> col != <span class="string">&#x27;income&#x27;</span>]</span><br><span class="line">    target = df[<span class="string">&quot;income&quot;</span>]</span><br><span class="line">    dataset = pd.concat([df[df_int_col], pd.get_dummies(df[df_object_col])], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> target, dataset</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Adult_data</span>(<span class="title class_ inherited__">Dataset</span>) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>) :</span><br><span class="line">        <span class="built_in">super</span>(Adult_data, self).__init__()</span><br><span class="line">        self.model = model</span><br><span class="line">        </span><br><span class="line">        df_train = pd.read_csv(<span class="string">&#x27;adult.csv&#x27;</span>, header = <span class="literal">None</span>, names = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>,  <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;capital-gain&#x27;</span>, <span class="string">&#x27;capital-loss&#x27;</span>, <span class="string">&#x27;hours-per-week&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line">        df_test = pd.read_csv(<span class="string">&#x27;data.test&#x27;</span>, header = <span class="literal">None</span>, skiprows = <span class="number">1</span>, names = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>,  <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;capital-gain&#x27;</span>, <span class="string">&#x27;capital-loss&#x27;</span>, <span class="string">&#x27;hours-per-week&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line">        </span><br><span class="line">        train_target, train_dataset = data_process(df_train, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">        test_target, test_dataset = data_process(df_test, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         进行独热编码对齐</span></span><br><span class="line">        test_dataset = fix_columns(test_dataset, train_dataset.columns)</span><br><span class="line"><span class="comment">#         print(df[&quot;income&quot;])</span></span><br><span class="line">        train_dataset = train_dataset.apply(<span class="keyword">lambda</span> x : (x - x.mean()) / x.std())</span><br><span class="line">        test_dataset = test_dataset.apply(<span class="keyword">lambda</span> x : (x - x.mean()) / x.std())</span><br><span class="line"><span class="comment">#         print(train_dataset[&#x27;native-country_ Holand-Netherlands&#x27;])</span></span><br><span class="line">        </span><br><span class="line">        train_target, test_target = np.array(train_target), np.array(test_target)</span><br><span class="line">        train_dataset, test_dataset = np.array(train_dataset, dtype = np.float32), np.array(test_dataset, dtype = np.float32)</span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">&#x27;test&#x27;</span> :</span><br><span class="line">            isnan = np.isnan(test_dataset)</span><br><span class="line">            test_dataset[np.where(isnan)] = <span class="number">0.0</span></span><br><span class="line"><span class="comment">#             print(test_dataset[ : , 75])</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            self.target = torch.tensor(test_target, dtype = torch.int64)</span><br><span class="line">            self.dataset = torch.FloatTensor(test_dataset)</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line"><span class="comment">#           前百分之八十的数据作为训练集，其余作为验证集</span></span><br><span class="line">            <span class="keyword">if</span> model == <span class="string">&#x27;train&#x27;</span> : </span><br><span class="line">                self.target = torch.tensor(train_target, dtype = torch.int64)[ : <span class="built_in">int</span>(<span class="built_in">len</span>(train_dataset) * <span class="number">0.8</span>)]</span><br><span class="line">                self.dataset = torch.FloatTensor(train_dataset)[ : <span class="built_in">int</span>(<span class="built_in">len</span>(train_target) * <span class="number">0.8</span>)]</span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                self.target = torch.tensor(train_target, dtype = torch.int64)[<span class="built_in">int</span>(<span class="built_in">len</span>(train_target) * <span class="number">0.8</span>) : ] </span><br><span class="line">                self.dataset = torch.FloatTensor(train_dataset)[<span class="built_in">int</span>(<span class="built_in">len</span>(train_dataset) * <span class="number">0.8</span>) : ]</span><br><span class="line">        <span class="built_in">print</span>(self.dataset.shape, self.target.dtype)   </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>) :</span><br><span class="line">        <span class="keyword">return</span> self.dataset[item], self.target[item]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) :</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dataset)</span><br><span class="line">    </span><br><span class="line">train_dataset = Adult_data(model = <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">val_dataset = Adult_data(model = <span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">test_dataset = Adult_data(model = <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size = <span class="number">64</span>, shuffle = <span class="literal">True</span>, drop_last = <span class="literal">False</span>)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size = <span class="number">64</span>, shuffle = <span class="literal">False</span>, drop_last = <span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size = <span class="number">64</span>, shuffle = <span class="literal">False</span>, drop_last = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>构建网络，因为是简单的二分类，这里使用了两层感知机网络，后面做对结果进行softmax归一化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Adult_Model</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) :</span><br><span class="line">        <span class="built_in">super</span>(Adult_Model, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">102</span>, <span class="number">64</span>), </span><br><span class="line">                                nn.ReLU(), </span><br><span class="line">                                nn.Linear(<span class="number">64</span>, <span class="number">32</span>), </span><br><span class="line">                                nn.ReLU(),</span><br><span class="line">                                nn.Linear(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">                                )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>) :</span><br><span class="line">        out = self.net(x) </span><br><span class="line"><span class="comment">#         print(out)</span></span><br><span class="line">        <span class="keyword">return</span> F.softmax(out)</span><br></pre></td></tr></table></figure>
<p>训练及验证，每经过一个epoch，就进行一次损失比较，当val_loss更小时，保存最好模型，直至迭代结束。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = Adult_Model().to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr = <span class="number">0.001</span>, momentum = <span class="number">0.9</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">max_epoch = <span class="number">30</span></span><br><span class="line">classes = [<span class="string">&#x27; &lt;=50K&#x27;</span>, <span class="string">&#x27; &gt;50K&#x27;</span>]</span><br><span class="line">mse_loss = <span class="number">1000000</span></span><br><span class="line">os.makedirs(<span class="string">&#x27;MyModels&#x27;</span>, exist_ok = <span class="literal">True</span>)</span><br><span class="line">writer = SummaryWriter(log_dir = <span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch) :</span><br><span class="line">    </span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> x, label <span class="keyword">in</span> train_loader :</span><br><span class="line">        x, label = x.to(device), label.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        out = model(x)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        loss.backward()</span><br><span class="line">        </span><br><span class="line">        _, pred = torch.<span class="built_in">max</span>(out, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#         print(pred)</span></span><br><span class="line">        num_correct = (pred == label).<span class="built_in">sum</span>().item()</span><br><span class="line">        acc = num_correct / x.shape[<span class="number">0</span>]</span><br><span class="line">        train_acc += acc</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch : <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, train_loss : <span class="subst">&#123;train_loss / <span class="built_in">len</span>(train_loader.dataset)&#125;</span>, train_acc : <span class="subst">&#123;train_acc / <span class="built_in">len</span>(train_loader)&#125;</span>&#x27;</span>)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, train_loss / <span class="built_in">len</span>(train_loader.dataset), epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad() :</span><br><span class="line">        total_loss = []</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">for</span> x, label <span class="keyword">in</span> val_loader :</span><br><span class="line">            x, label = x.to(device), label.to(device)</span><br><span class="line">            out = model(x)</span><br><span class="line">            loss = criterion(out, label)</span><br><span class="line">            total_loss.append(loss.item())</span><br><span class="line">        </span><br><span class="line">        val_loss = <span class="built_in">sum</span>(total_loss) / <span class="built_in">len</span>(total_loss)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> val_loss &lt; mse_loss :</span><br><span class="line">        mse_loss = val_loss </span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;MyModels/Deeplearning_Model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> model</span><br></pre></td></tr></table></figure>
<p>下载在训练过程保存的最好模型进行预测并保存结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_model = Adult_Model().to(device)</span><br><span class="line">ckpt = torch.load(<span class="string">&#x27;MyModels/Deeplearning_Model.pth&#x27;</span>, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">best_model.load_state_dict(ckpt)</span><br><span class="line"></span><br><span class="line">test_loss = <span class="number">0.0</span></span><br><span class="line">test_acc = <span class="number">0.0</span></span><br><span class="line">best_model.<span class="built_in">eval</span>()</span><br><span class="line">result = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, label <span class="keyword">in</span> test_loader :</span><br><span class="line">    x, label = x.to(device), label.to(device)</span><br><span class="line">    </span><br><span class="line">    out = best_model(x)</span><br><span class="line">    loss = criterion(out, label)</span><br><span class="line">    test_loss += loss.item()</span><br><span class="line">    _, pred = torch.<span class="built_in">max</span>(out, dim = <span class="number">1</span>)</span><br><span class="line">    result.append(pred.detach())</span><br><span class="line">    num_correct = (pred == label).<span class="built_in">sum</span>().item()</span><br><span class="line">    acc = num_correct / x.shape[<span class="number">0</span>]</span><br><span class="line">    test_acc += acc</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;test_loss : <span class="subst">&#123;test_loss / <span class="built_in">len</span>(test_loader.dataset)&#125;</span>, test_acc : <span class="subst">&#123;test_acc / <span class="built_in">len</span>(test_loader)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">result = torch.cat(result, dim = <span class="number">0</span>).cpu().numpy()</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;Predict/Deeplearing.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline = <span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file :</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;pred_result&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> i, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(result) :</span><br><span class="line">        writer.writerow([i, classes[pred]])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/5f31fb9f77b649888e08068c28dd3185.png" alt="在这里插入图片描述"><br>正确率达到0.834还是蛮不错的。</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>数据处理，跟深度学习的过程基本一致，只是返回值不一样而已。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_missing_columns</span>(<span class="params">d, columns</span>) :</span><br><span class="line">    missing_col = <span class="built_in">set</span>(columns) - <span class="built_in">set</span>(d.columns)</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> missing_col :</span><br><span class="line">        d[col] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fix_columns</span>(<span class="params">d, columns</span>):  </span><br><span class="line">    add_missing_columns(d, columns)</span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">set</span>(columns) - <span class="built_in">set</span>(d.columns) == <span class="built_in">set</span>())</span><br><span class="line">    d = d[columns]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_process</span>(<span class="params">df, model</span>) :</span><br><span class="line">    df.replace(<span class="string">&quot; ?&quot;</span>, pd.NaT, inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> model == <span class="string">&#x27;train&#x27;</span> :</span><br><span class="line">        df.replace(<span class="string">&quot; &gt;50K&quot;</span>, <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">        df.replace(<span class="string">&quot; &lt;=50K&quot;</span>, <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> model == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        df.replace(<span class="string">&quot; &gt;50K.&quot;</span>, <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">        df.replace(<span class="string">&quot; &lt;=50K.&quot;</span>, <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    trans = &#123;<span class="string">&#x27;workclass&#x27;</span> : df[<span class="string">&#x27;workclass&#x27;</span>].mode()[<span class="number">0</span>], <span class="string">&#x27;occupation&#x27;</span> : df[<span class="string">&#x27;occupation&#x27;</span>].mode()[<span class="number">0</span>], <span class="string">&#x27;native-country&#x27;</span> : df[<span class="string">&#x27;native-country&#x27;</span>].mode()[<span class="number">0</span>]&#125;</span><br><span class="line">    df.fillna(trans, inplace = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    df.drop(<span class="string">&#x27;fnlwgt&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    df.drop(<span class="string">&#x27;capital-gain&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    df.drop(<span class="string">&#x27;capital-loss&#x27;</span>, axis = <span class="number">1</span>, inplace = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#         print(df)</span></span><br><span class="line"></span><br><span class="line">    df_object_col = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> df[col].dtype.name == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line">    df_int_col = [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> df[col].dtype.name != <span class="string">&#x27;object&#x27;</span> <span class="keyword">and</span> col != <span class="string">&#x27;income&#x27;</span>]</span><br><span class="line">    target = df[<span class="string">&quot;income&quot;</span>]</span><br><span class="line">    dataset = pd.concat([df[df_int_col], pd.get_dummies(df[df_object_col])], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> target, dataset</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Adult_data</span>() :</span><br><span class="line"></span><br><span class="line">    df_train = pd.read_csv(<span class="string">&#x27;adult.csv&#x27;</span>, header = <span class="literal">None</span>, names = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>,  <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;capital-gain&#x27;</span>, <span class="string">&#x27;capital-loss&#x27;</span>, <span class="string">&#x27;hours-per-week&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line">    df_test = pd.read_csv(<span class="string">&#x27;data.test&#x27;</span>, header = <span class="literal">None</span>, skiprows = <span class="number">1</span>, names = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>,  <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;capital-gain&#x27;</span>, <span class="string">&#x27;capital-loss&#x27;</span>, <span class="string">&#x27;hours-per-week&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    train_target, train_dataset = data_process(df_train, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    test_target, test_dataset = data_process(df_test, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line"><span class="comment">#         进行独热编码对齐</span></span><br><span class="line">    test_dataset = fix_columns(test_dataset, train_dataset.columns)</span><br><span class="line">    columns = train_dataset.columns</span><br><span class="line"><span class="comment">#         print(df[&quot;income&quot;])</span></span><br><span class="line"></span><br><span class="line">    train_target, test_target = np.array(train_target), np.array(test_target)</span><br><span class="line">    train_dataset, test_dataset = np.array(train_dataset), np.array(test_dataset)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> train_dataset, train_target, test_dataset, test_target, columns</span><br><span class="line"></span><br><span class="line">train_dataset, train_target, test_dataset, test_target, columns = Adult_data()</span><br><span class="line"><span class="built_in">print</span>(train_dataset.shape, test_dataset.shape, train_target.shape, test_target.shape)</span><br></pre></td></tr></table></figure>
<p>GridSearchCV 类可以用来对分类器的指定参数值进行详尽搜索，这里搜索最佳的决策树的深度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># params = &#123;&#x27;max_depth&#x27; : range(1, 20)&#125;</span></span><br><span class="line"><span class="comment"># best_clf = GridSearchCV(DecisionTreeClassifier(criterion = &#x27;entropy&#x27;, random_state = 20), param_grid = params)</span></span><br><span class="line"><span class="comment"># best_clf = best_clf.fit(train_dataset, train_target)</span></span><br><span class="line"><span class="comment"># print(best_clf.best_params_)</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/ad5ae5d78b784f469b6716623cec87c4.png" alt="在这里插入图片描述"><br>用决策数进行分类，采用‘熵’作为决策基准，决策深度由上步骤得到8，分裂一个节点所需的样本数至少设为5，并保存预测结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># clf = DecisionTreeClassifier() score:0.7836742214851667</span></span><br><span class="line">classes = [<span class="string">&#x27; &lt;=50K&#x27;</span>, <span class="string">&#x27; &gt;50K&#x27;</span>]</span><br><span class="line">clf = DecisionTreeClassifier(criterion = <span class="string">&#x27;entropy&#x27;</span>, max_depth = <span class="number">8</span>, min_samples_split = <span class="number">5</span>)</span><br><span class="line">clf = clf.fit(train_dataset, train_target)</span><br><span class="line">pred = clf.predict(test_dataset)</span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line">score = clf.score(test_dataset, test_target)</span><br><span class="line"><span class="comment"># pred = clf.predict_proba(test_dataset)</span></span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="comment"># print(np.argmax(pred, axis = 1))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;Predict/DecisionTree.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline = <span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file :</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;result_pred&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> i, result <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred) :</span><br><span class="line">        writer.writerow([i, classes[result]])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/04f1e6373e304a5fa2adcdabeffa9f0e.png" alt="在这里插入图片描述"><br>结果有0.835跟深度学习差不多，可视化决策树结构：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dot_data = export_graphviz(clf, out_file = <span class="literal">None</span>, feature_names = columns, class_names = classes, filled = <span class="literal">True</span>, rounded = <span class="literal">True</span>)</span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/925caff0520045e28c02d84f58e66179.png" alt="在这里插入图片描述"></p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><p>因数据处理方式与决策树相同，这里不再张贴，只粘贴模型部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">classes = [<span class="string">&#x27; &lt;=50K&#x27;</span>, <span class="string">&#x27; &gt;50K&#x27;</span>]</span><br><span class="line">clf = svm.SVC(kernel = <span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">clf = clf.fit(train_dataset, train_target)</span><br><span class="line">pred = clf.predict(test_dataset)</span><br><span class="line">score = clf.score(test_dataset, test_target)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;Predict/SupportVectorMachine.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline = <span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file :</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;result_pred&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> i, result <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred) :</span><br><span class="line">        writer.writerow([i, classes[result]])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/140e5b770d7440bd8e4202e0d3ca4b0a.png" alt="在这里插入图片描述"></p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classes = [<span class="string">&#x27; &lt;=50K&#x27;</span>, <span class="string">&#x27; &gt;50K&#x27;</span>]</span><br><span class="line">rf = RandomForestClassifier(n_estimators = <span class="number">100</span>, random_state = <span class="number">0</span>)</span><br><span class="line">rf = rf.fit(train_dataset, train_target)</span><br><span class="line">score = rf.score(test_dataset, test_target)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"></span><br><span class="line">pred = rf.predict(test_dataset)</span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;Predict/RandomForest.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline = <span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> file :</span><br><span class="line">    writer = csv.writer(file)</span><br><span class="line">    writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;result_pred&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> i, result <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred) :</span><br><span class="line">        writer.writerow([i, classes[result]])</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/15755e762a0147079f488671b3569dee.png" alt="在这里插入图片描述"></p>
<h1 id="三、结果分析"><a href="#三、结果分析" class="headerlink" title="三、结果分析"></a>三、结果分析</h1><p>经过在Adult数据集的测试集的预测结果可知，深度学习模型、决策树、支持向量机和随机森林的正确率分别达到0.834、0.834、0.834和0.817，四种模型的正确率差不多。正确率并不是很高的原因可能有：<br>1、模型的鲁棒性不够。<br>2、数据集存在大量的离散类型数据，在经过独热编码之后，数据高度稀疏。<br>解决方法：<br>1、对模型再进行搜索性地调参，可以考虑增加模型复杂度，过程中需要注意过拟合。<br>2、不选择独热编码的方式对数据进行降维，可以考虑Embedding</p>
<blockquote>
<p>所有的代码都可以从我的 <a href="https://github.com/aishangcengloua/Data-Mining/tree/master/Adult_predicted">Github仓库</a> 获取，欢迎您的start</p>
</blockquote>
<p>最后，如果您对Adult数据集的处理和模型实现有收获的话，还要麻烦给点个赞，不甚感激。</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy 实现 K_means 算法</title>
    <url>/2022/06/05/Numpy-%E5%AE%9E%E7%8E%B0-K-means-%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>﻿<a href="https://github.com/aishangcengloua/Python/blob/master/Lab4/Lab4.dat">代码所需数据</a><br>聚类就是根据数据之间的相似度将数据集划分为多个类别或组，使类别内的数据相似度较大而类别间的数据相似度较小。如下图所示，左边是原始数据，右边是聚类之后的效果，不同的颜色代表不同的类别。<br><img src="https://img-blog.csdnimg.cn/798a5b8629b348c29c9b58c281d4d1cc.png" alt="在这里插入图片描述"></p>
<p>对于本次代码聚类步骤如下（聚类算法大体步骤，可根据需求进行修改）：</p>
<blockquote>
<p><strong><em>1</em></strong>.设置初始类别中心和类别数，初始化是要注意在题目所给数据的x、y的<strong>最小值和最大值</strong>进行。<br><strong><em>2</em></strong>.根据类别中心对全部数据进行类别划分：每个点分到离自己<strong>距离最小</strong>的那个类<br><strong><em>3</em></strong>.重新计算当前类别划分下每个类的中心：例如可以取每个类别里所有的点的平均值作为新的中心。如何求多个点的平均值？ 分别计算X坐标的平均值，y坐标的平均值，从而得到新的点。注意：类的中心可以不是真实的点，虚拟的点也不影响。<br><strong><em>4</em></strong>.在新的类别中心下继续进行类别划分;</p>
</blockquote>
<p>如果连续两次的类别划分结果不变则停止算法; 否则循环2～5。例如当类的中心不再变化时，跳出循环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(<span class="string">&#x27;Lab4.dat&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calSSE</span>(<span class="params">X, cidx, ctrs</span>) :</span><br><span class="line">    SSE = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, ctr <span class="keyword">in</span> <span class="built_in">enumerate</span>(ctrs) :</span><br><span class="line">        SSE += np.<span class="built_in">sum</span>(np.square(X[np.where(cidx == i + <span class="number">1</span>)] - ctr))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> SSE / X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kmeans</span>(<span class="params">X, K</span>) :</span><br><span class="line">    center_point = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K) :</span><br><span class="line">        point_x = np.random.uniform(np.<span class="built_in">min</span>(X, axis = <span class="number">0</span>)[<span class="number">0</span>], np.<span class="built_in">max</span>(X, axis = <span class="number">0</span>)[<span class="number">0</span>])<span class="comment">#随机初始化簇心</span></span><br><span class="line">        point_y = np.random.uniform(np.<span class="built_in">min</span>(X, axis = <span class="number">0</span>)[<span class="number">1</span>], np.<span class="built_in">max</span>(X, axis = <span class="number">0</span>)[<span class="number">1</span>])</span><br><span class="line">        center_point.append([point_x, point_y])</span><br><span class="line">    center_point = np.array(center_point)</span><br><span class="line">    cluter = np.zeros(X.shape[<span class="number">0</span>]).astype(np.int32)<span class="comment">#建立簇类初始化为0</span></span><br><span class="line">    item = <span class="number">5</span></span><br><span class="line">    <span class="keyword">while</span> item &gt; <span class="number">0</span>:<span class="comment">#迭代</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]) :<span class="comment">#计算每一组数据与每个簇心的欧氏距离，距离最小者记为此组数据为所标类别</span></span><br><span class="line">            distance = center_point</span><br><span class="line">            distance = np.<span class="built_in">sum</span>(np.square(distance - X[i]), axis = <span class="number">1</span>)<span class="comment">#注意x， y都计算所以要求和，注意求和维度</span></span><br><span class="line">            cluter[i] = np.argmin(distance) + <span class="number">1</span><span class="comment">#最小值的下标</span></span><br><span class="line">    </span><br><span class="line">        New_center_point = np.zeros((K, <span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K) :<span class="comment">#更新簇心，取每一簇类的平均值作为新簇心</span></span><br><span class="line">            New_center_point[i][<span class="number">0</span>] = np.mean(X[np.where(cluter == i + <span class="number">1</span>), <span class="number">0</span>])</span><br><span class="line">            New_center_point[i][<span class="number">1</span>] = np.mean(X[np.where(cluter == i + <span class="number">1</span>), <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> (New_center_point - center_point &lt; <span class="number">1e-7</span>).<span class="built_in">all</span>() :<span class="comment">#当新簇心与之前的簇心近似相等时退出迭代</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        center_point = New_center_point</span><br><span class="line">        item -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> cluter, center_point<span class="comment">#返回每一组数据所对应的簇类和簇心</span></span><br><span class="line"></span><br><span class="line">SSE = []</span><br><span class="line">mark = [ <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;g&#x27;</span>]</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line"><span class="keyword">for</span> K <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">7</span>) :</span><br><span class="line">    cidx, ctrs = kmeans(data, K)</span><br><span class="line">    ctrs_set.append(ctrs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;K为<span class="subst">&#123;K&#125;</span>时的簇心 : \n <span class="subst">&#123;ctrs&#125;</span>&#x27;</span>)</span><br><span class="line">    SSE.append(calSSE(data, cidx, ctrs))<span class="comment">#手肘法求最好分类的K值</span></span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">3</span>, K - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K) :</span><br><span class="line">        plt.scatter(data[np.where(cidx == i + <span class="number">1</span>), <span class="number">0</span>], data[np.where(cidx == i + <span class="number">1</span>), <span class="number">1</span>], marker = <span class="string">&#x27;.&#x27;</span>, color = mark[i])<span class="comment">#作图</span></span><br><span class="line">    plt.scatter(ctrs[ : , <span class="number">0</span>], ctrs[ : , <span class="number">1</span>], marker = <span class="string">&#x27;*&#x27;</span>, color = <span class="string">&#x27;g&#x27;</span>)<span class="comment">#做出簇心</span></span><br><span class="line">    plt.title(<span class="string">f&#x27;K is <span class="subst">&#123;K&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.xticks([]), plt.yticks([])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">7</span>)), SSE, <span class="string">&#x27;+--&#x27;</span>)</span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/713872beed8f409ba91d8cfe5bba3e1a.png#pic_center" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/cc79c8e8a4cf4f5b9f7c4ed34e162344.png#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM 预测股票</title>
    <url>/2022/06/05/LSTM-%E9%A2%84%E6%B5%8B%E8%82%A1%E7%A5%A8/</url>
    <content><![CDATA[<p>﻿tushare是一个开源的金融数据源，目前维护的数据非常丰富，质量也很高，对于一般的分析已经足够，可以省去自己到处去爬数据。我这里下载沪深300指数数据进行预测每日的最高价</p>
<blockquote>
<p><em>首先使用pip install tushare安装tushare工具包 ，github地址为：<br><a href="https://github.com/aishangcengloua/MLData/blob/master/PyTorch/NLP/Forecast_stock/LSTM.ipynb">https://github.com/aishangcengloua/MLData/blob/master/PyTorch/NLP/Forecast_stock/LSTM.ipynb</a></em></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">cons = ts.get_apis()<span class="comment">#建立连接</span></span><br><span class="line">df = ts.bar(<span class="string">&#x27;000300&#x27;</span>, conn = cons, asset = <span class="string">&#x27;INDEX&#x27;</span>, start_date = <span class="string">&#x27;2010-01-01&#x27;</span>, end_date = <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">df.info()<span class="comment">#查看没有缺失值之后保存</span></span><br><span class="line">df.columns<span class="comment">#可知沪深300指数（000300）的信息包括交易日期，开盘价，收盘价，最高价，最低价，交易量，成交金额，涨跌幅。</span></span><br><span class="line"><span class="comment"># df.to_csv(&#x27;sh300.csv&#x27;)</span></span><br></pre></td></tr></table></figure>
<center>


<p><img src="https://img-blog.csdnimg.cn/f2d9f88f105e4c3286b86d871441eb60.png#pic_center" alt="在这里插入图片描述"></p>
<p>导入所需的包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>数据处理，这里我的思路是取[ ‘open’, ‘close’, ‘low’, ‘vol’, ‘amount’, ‘p_change’]六列作为模型的feature进行练，’high’列作为标签。此次我使用LSTM进行预测，所以要注意batch，TIME_STEP，input_size的划分，因为有六列feature，所以input_size为6；对于时间序列TIME_STEP，可以任意指定可以通过前n天的参数来预测今天的最高价。</p>
<p>比如：n = 3，X=[ [ ‘open1’, ‘close1’, ‘low1’, ‘vol1’, ‘amount1’, ‘p_change1’] , [ ‘open2’, ‘close2’, ‘low2’, ‘vol2’, ‘amount2’, ‘p_change2’]，   [ ‘open3’, ‘close3’, ‘low3’, ‘vol3’, ‘amount3’, ‘p_change3’] ]，Y=[ high4 ]。</p>
<p>我们要确保我们输入网络的的数据的维度是[batch，TIME_STEP，input_size]。其次是我将数据划分为8 ：2，2份作为预测数据。要注意的是，生成迭代数据的时候，batch_size 取值要大一些，否则训练时损失振幅会很大，导致预测效果不好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TIME_STEP = <span class="number">5</span><span class="comment">#指定序列长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dataset</span>() :</span><br><span class="line">    df = pd.read_csv(<span class="string">&#x27;sh300.csv&#x27;</span>)</span><br><span class="line">    columns = df.columns</span><br><span class="line">    df_index = df[<span class="string">&#x27;datetime&#x27;</span>]<span class="comment">#获取日期，方便后面作图</span></span><br><span class="line">    df = df[[ <span class="string">&#x27;open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>, <span class="string">&#x27;high&#x27;</span>, <span class="string">&#x27;low&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>, <span class="string">&#x27;amount&#x27;</span>, <span class="string">&#x27;p_change&#x27;</span>]]</span><br><span class="line">    min_high, max_high = <span class="built_in">min</span>(df[<span class="string">&#x27;high&#x27;</span>]), <span class="built_in">max</span>(df[<span class="string">&#x27;high&#x27;</span>])<span class="comment">#保存标签的最大，最小值以便后续恢复真实值</span></span><br><span class="line">    df = df.apply(<span class="keyword">lambda</span> x : (x - <span class="built_in">min</span>(x)) / (<span class="built_in">max</span>(x) - <span class="built_in">min</span>(x)))<span class="comment">#将数据进行归一化</span></span><br><span class="line">    df1 = df[[ <span class="string">&#x27;open&#x27;</span>, <span class="string">&#x27;close&#x27;</span>, <span class="string">&#x27;low&#x27;</span>, <span class="string">&#x27;vol&#x27;</span>, <span class="string">&#x27;amount&#x27;</span>, <span class="string">&#x27;p_change&#x27;</span>]]</span><br><span class="line">    data = []</span><br><span class="line">    target = []</span><br><span class="line">    index = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(df.shape[<span class="number">0</span>] - TIME_STEP) :</span><br><span class="line">        data.append(df1.iloc[i : i + TIME_STEP].values)<span class="comment">#实现时间序列数据的提取</span></span><br><span class="line">        target.append(df[<span class="string">&#x27;high&#x27;</span>].iloc[i + TIME_STEP])<span class="comment">#保存今天的真实值，因为我们是用前n天来预测今天的最高值</span></span><br><span class="line">        index.append(df_index.iloc[i + TIME_STEP])</span><br><span class="line">    </span><br><span class="line">    target = torch.tensor(target, dtype = torch.float32)</span><br><span class="line">    data = torch.tensor(data, dtype = torch.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> min_high, max_high, data, target, index</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Stock_dataset</span>(<span class="title class_ inherited__">Dataset</span>) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, target, model = <span class="string">&#x27;train&#x27;</span></span>) :</span><br><span class="line">        <span class="built_in">super</span>(Stock_dataset, self).__init__()</span><br><span class="line">        self.model = model</span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">&#x27;train&#x27;</span> :</span><br><span class="line">            self.data = data[ : <span class="built_in">int</span>(data.shape[<span class="number">0</span>] * <span class="number">0.8</span>)]</span><br><span class="line">            self.target = target[ : <span class="built_in">int</span>(target.shape[<span class="number">0</span>] * <span class="number">0.8</span>)]</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            self.data = data[<span class="built_in">int</span>(data.shape[<span class="number">0</span>] * <span class="number">0.8</span>) : ]</span><br><span class="line">            self.target = target[<span class="built_in">int</span>(target.shape[<span class="number">0</span>] * <span class="number">0.8</span>) : ]</span><br><span class="line">         </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>) :</span><br><span class="line">        <span class="keyword">return</span> self.data[item], self.target[item]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) :</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line">    </span><br><span class="line">min_high, max_high, data, target, index = dataset()</span><br><span class="line"><span class="built_in">print</span>(target.shape, <span class="built_in">len</span>(index))</span><br><span class="line">train_data = Stock_dataset(data, target, model = <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">test_data = Stock_dataset(data, target, model = <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">test_index = index[<span class="built_in">int</span>(target.shape[<span class="number">0</span>] * <span class="number">0.8</span>) : ]</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_data, batch_size = <span class="number">64</span>, shuffle = <span class="literal">True</span>, drop_last = <span class="literal">False</span>)<span class="comment">#生成可迭代数据</span></span><br><span class="line">test_loader = DataLoader(test_data, batch_size = <span class="number">64</span>, shuffle = <span class="literal">False</span>, drop_last = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<center>


<p><img src="https://img-blog.csdnimg.cn/5620c0833a484a4fb1499fc927f433b8.png" alt="在这里插入图片描述"></p>
<p>​    </p>
<p>构建网络，使用LSTM，后将LSTM网络的输出经过线性神经元进行输出。要注意的是LSTM输入和输出的隐含状态为（h， c），当将输出输入到linear网络时，只取最后一次TIME_STEP的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, INPUT_SIZE, HIDDEN_SIZE</span>) :</span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.lstm = nn.LSTM(input_size = INPUT_SIZE, hidden_size = HIDDEN_SIZE, batch_first = <span class="literal">True</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>) :</span><br><span class="line">        lstm_out, (h, c) = self.lstm(x)</span><br><span class="line">        out = self.linear(lstm_out[ : , -<span class="number">1</span>, : ])<span class="comment">#降维，最后一次TIME_STEP的输出</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>设置超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TIME_STEP = <span class="number">5</span></span><br><span class="line">INPUT_SIZE = <span class="number">6</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">64</span></span><br><span class="line">EPOCH = <span class="number">180</span></span><br><span class="line">model = LSTM(INPUT_SIZE = INPUT_SIZE, HIDDEN_SIZE = HIDDEN_SIZE).cuda()</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr = <span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<p>训练，用Tensorboard可视化，关于 Tensorboard 的使用<a href="https://blog.csdn.net/weixin_53598445/article/details/121301078">点这里</a>。model 的输出是一个矩阵，为了计算loss准确所以要对输出进行降维</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter(log_dir = <span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH) :</span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x, label <span class="keyword">in</span> train_loader :</span><br><span class="line">        x, label = x.cuda(), label.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(x)</span><br><span class="line">        loss = criterion(torch.squeeze(out), label)<span class="comment">#降维</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"><span class="comment">#         train_loss += loss.item()</span></span><br><span class="line"><span class="comment">#     print(f&#x27;train_loss : &#123;train_loss / len(train_loader.dataset) : 0.4f&#125;&#x27;)</span></span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;sh300&#x27;</span>, loss, epoch)<span class="comment">#可视化</span></span><br></pre></td></tr></table></figure>
<center>


<p><img src="https://img-blog.csdnimg.cn/4b56fc5755a74737939144eaaa5fbaa4.PNG" alt="在这里插入图片描述"></p>
<p>预测及可视化，在可视化中，为了更好的观察预测效果，我只选择了一百天进行可视化，且为了时间的有序，将它们进行逆序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">pred = []</span><br><span class="line">test_target = []</span><br><span class="line"><span class="keyword">for</span> x, label <span class="keyword">in</span> test_loader :</span><br><span class="line">    x, label = x.cuda(), label.cuda()</span><br><span class="line">    out = model(x)</span><br><span class="line"><span class="comment">#     print(out)</span></span><br><span class="line">    test_target.append(label.detach().cpu())</span><br><span class="line">    pred.append(out.detach().cpu())</span><br><span class="line">test_target = torch.cat(test_target, dim = <span class="number">0</span>).numpy().squeeze()<span class="comment">#将各个batch的输出进行拼接，转成数组再进行降维</span></span><br><span class="line">pred = torch.cat(pred, dim = <span class="number">0</span>).numpy().squeeze()</span><br><span class="line"></span><br><span class="line">test_target = test_target * (max_high - min_high) + min_high<span class="comment">#还原原始的数据</span></span><br><span class="line">pred = pred * (max_high - min_high) + min_high</span><br><span class="line">test_index = np.array(test_index)</span><br><span class="line">plt.plot(test_index[ : <span class="number">101</span>][ : : -<span class="number">1</span>], test_target[ : <span class="number">101</span>][ : : -<span class="number">1</span>], <span class="string">&#x27;r&#x27;</span>, label = <span class="string">&#x27;test_target&#x27;</span>)</span><br><span class="line">plt.plot(test_index[ : <span class="number">101</span>][ : : -<span class="number">1</span>], pred[ : <span class="number">101</span>][ : : -<span class="number">1</span>], <span class="string">&#x27;b&#x27;</span>, label = <span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">plt.legend(loc = <span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>,<span class="number">101</span>,<span class="number">25</span>), [test_index[<span class="number">100</span>], test_index[<span class="number">75</span>], test_index[<span class="number">50</span>], test_index[<span class="number">25</span>], test_index[<span class="number">0</span>]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<center>


<p><img src="https://img-blog.csdnimg.cn/7898c62257c64232a598b577ff4f3f74.png" alt="在这里插入图片描述"></p>
<p>看上去效果是非常好的。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>LSTM</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/06/03/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>pip 安装 tensorflow 可能出现的问题</title>
    <url>/2022/06/05/pip-%E5%AE%89%E8%A3%85-tensorflow-%E5%8F%AF%E8%83%BD%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>﻿解决安装用pip tensorflow过程中的一些问题：<br>1.超时问题，使用镜像源下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install tensorflow -i https://pypi.mirrors.ustc.edu.cn/simple/</span><br></pre></td></tr></table></figure>
<p>2  TensorFlow 软件包依赖项出现冲突，要安装以下包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip3 install six numpy wheel</span><br><span class="line">pip3 install keras_applications==<span class="number">1.0</span><span class="number">.6</span> --no-deps</span><br><span class="line">pip3 install keras_preprocessing==<span class="number">1.0</span><span class="number">.5</span> --no-deps</span><br></pre></td></tr></table></figure>
<p>3.出现管理员权限问题<br><img src="https://img-blog.csdnimg.cn/00dc61c6fa204f72821627baeafefb9d.PNG#pic_center" alt=""><br>pip install …加入—user为pip install —user …</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>pip 安装 tensorflow</title>
    <url>/2022/06/05/pip-%E5%AE%89%E8%A3%85-tensorflow/</url>
    <content><![CDATA[<p>﻿<strong>ModuleNotFoundError: No module named ‘tensorboard’</strong><br>解决方法：用pip安装两个包</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">pip install tb-nightly或者pip install tb-nightly-gpu</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install future</span><br></pre></td></tr></table></figure>
<p>但安装过程中可能会出现问题超时等问题</p>
<p>所以建议使用镜像下载，我这里用的是豆瓣镜像：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">pip install tf-nightly-gpu -i http:<span class="comment">//pypi.douban.com/simple --trusted-host pypi.douban.com</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">pip install future -i http:<span class="comment">//pypi.douban.com/simple --trusted-host pypi.douban.com</span></span><br></pre></td></tr></table></figure>
<p>速度贼快（不过有一点是就算是很快也要等起码半小时左右，且一次装好几乎不可能，因为与他配件的其他包可能版本过高或者过低，祝你好运，嘻嘻。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title>【OpenCv】Canny 算子边缘检测</title>
    <url>/2022/06/05/%E3%80%90OpenCv%E3%80%91Canny-%E7%AE%97%E5%AD%90%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>﻿Canny 算子和 Marr（LoG）边缘检测方法类似，也属于是先平滑后求导数的方法John Canny研究了最优边缘检测方法所需的特性，给出了评价边缘检测性能优劣的三个指标：</p>
<ul>
<li>好的信噪比，即将非边缘点判定为边缘点的概率要低，将边缘点判为非边缘点的概率要低；</li>
<li>高的定位性能，即检测出的边缘点要尽可能在实际边缘的中心；</li>
<li>对单一边缘仅有唯一响应，即单个边缘产生多个响应的概率要低，并且虚假响应边缘应该得到最大抑制。</li>
</ul>
<p>步骤：</p>
<ul>
<li>减少噪音：由于边缘检测易受图像中的噪声影响，因此第一步是使用5x5高斯滤波器去除图像中的噪声.<br><img src="https://img-blog.csdnimg.cn/e3024b07247346be98463b755d346e6f.png#pic_center" alt="在这里插入图片描述"></li>
<li>计算图像梯度：对平滑后的图像使用sobel算子在水平与竖直方向上计算一阶导数，得到图像梯度（Gx和Gy）。根据梯度图找到边界梯度和方向<br><img src="https://img-blog.csdnimg.cn/0e5f4cccb84949258fc09df97191bc55.png#pic_center" alt="在这里插入图片描述"><br>根据角度对幅值进行非极大值抑制：将模糊的边界变得清晰（sharp）</li>
<li>将其梯度方向近似为以下值中的一个（0,45,90,135,180,225,270,315）（即上下左右和45度方向）；</li>
<li>比较该像素点，和其梯度方向正负方向的像素点的梯度强度；</li>
<li>如果该像素点梯度强度最大则保留，否则抑制（删除，即置为0）。<br><img src="https://img-blog.csdnimg.cn/116fa846d23a4a39873025fc11b8e35a.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/35c89ab7be494532a47c1723b1edd970.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/93823c91a0e94d81bc20b71d4332a9fe.png#pic_center" alt="在这里插入图片描述"><br>edge = cv2.Canny(image, threshold1, threshold2) 必要参数：</li>
<li>第一个参数是需要处理的原图像，该图像必须为单通道的灰度图；</li>
<li>第二个参数是阈值1；</li>
<li>第三个参数是阈值2。</li>
</ul>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = cv.imread(<span class="string">&#x27;girl.png&#x27;</span>, cv.IMREAD_GRAYSCALE)</span><br><span class="line">gaussion = cv.GaussianBlur(img, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">0</span>)</span><br><span class="line">canny = cv.Canny(gaussion, <span class="number">32</span>, <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">fig.<span class="built_in">set</span>(alpha = <span class="number">0.2</span>)</span><br><span class="line">plt.subplot2grid((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">plt.imshow(img, <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">plt.imshow(canny, <span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/cc8336aa6b2641f682b9c7f0c7f88af6.png#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#实现在线手动调整threshold1和threshold2的值</span></span><br><span class="line">img = cv.imread(<span class="string">&#x27;girl.png&#x27;</span>, cv.IMREAD_GRAYSCALE)</span><br><span class="line">img = cv.resize(img, (img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">threshold1_min = <span class="number">0</span></span><br><span class="line">threshold1_max = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">threshold2_min = <span class="number">100</span></span><br><span class="line">threshold2_max = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">canny_threshold1</span>(<span class="params">x</span>) :</span><br><span class="line">    gaussion = cv.GaussianBlur(img, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">0</span>)</span><br><span class="line">    canny = cv.Canny(gaussion, x, threshold2_min)</span><br><span class="line">    cv.imshow(<span class="string">&#x27;Canny&#x27;</span>, canny)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">canny_threshold2</span>(<span class="params">x</span>) :</span><br><span class="line">    gaussion = cv.GaussianBlur(img, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">0</span>)</span><br><span class="line">    canny = cv.Canny(gaussion, threshold1_min, x)</span><br><span class="line">    cv.imshow(<span class="string">&#x27;Canny&#x27;</span>, canny)</span><br><span class="line"></span><br><span class="line">cv.namedWindow(<span class="string">&#x27;Canny&#x27;</span>, cv.WINDOW_NORMAL | cv.WINDOW_KEEPRATIO)</span><br><span class="line"><span class="comment">#创建可调整大小的窗口并在调整窗口大小时保持图像比例不变</span></span><br><span class="line">cv.createTrackbar(<span class="string">&#x27;threshold1&#x27;</span>, <span class="string">&#x27;Canny&#x27;</span>, threshold1_min, threshold1_max, canny_threshold1)</span><br><span class="line">cv.createTrackbar(<span class="string">&#x27;threshold2&#x27;</span>, <span class="string">&#x27;Canny&#x27;</span>, threshold2_min, threshold2_max, canny_threshold2)</span><br><span class="line"><span class="comment">#绑定滑动条和窗口， 且设置滑动条的值</span></span><br><span class="line"><span class="comment">#cv2.createTrackbar(“scale”, “display”, 0, 100, self.opencv_calibration_node.on_scale)</span></span><br><span class="line"><span class="comment"># 第一个参数表示滑动条的名称，</span></span><br><span class="line"><span class="comment"># 第二个表示绑定的窗口名字，</span></span><br><span class="line"><span class="comment"># 第三个和第四设置滑动条的范围，注意的是第三个参数即滑动条的最小值固定为一，调整只是为了从你设定的那个数开始而已</span></span><br><span class="line"><span class="comment"># 第五个参数是回调函数，每次滑动都会调用回调函数。</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/fa11d0a9a5d44610b948f469113c9c3c.png#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器视觉</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
        <tag>边缘检测</tag>
      </tags>
  </entry>
  <entry>
    <title>【OpenCv】Marr 算子边缘检测</title>
    <url>/2022/06/05/%E3%80%90OpenCv%E3%80%91Marr-%E7%AE%97%E5%AD%90%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>﻿Marr算子: Laplacian of a Gaussian（LOG）</p>
<ul>
<li>Marr算子是在Laplacian算子的基础上实现的，它得益于对人的视觉机理的研究，有一定的生物学和生理学意义。</li>
<li>由于Laplacian算子对噪声比较敏感，为了减少噪声影响，提出了将高斯滤<br>波和拉普拉斯检测算子结合在一起进行边缘检测的方法：先对图像进行平滑，然后再用Laplacian算子检测边缘。</li>
<li>平滑函数应能反映不同远近的周围点对给定像素具有不同的平滑作用，因此，平滑函数采用正态分布的高斯函数，即：<br><img src="https://img-blog.csdnimg.cn/689572f9726448c3b82d02949431e800.png#pic_center" alt="在这里插入图片描述"><br>卷积操作具有结合律，因此我们先将高斯平滑滤波器与拉普拉斯滤波器进行卷积，然后利用得到的混合滤波器去对图片进行卷积以得到所需的结果。<br>两个优点：</li>
<li>由于高斯和拉普拉斯核通常都比图像小得多，所以这种方法通常只需要很少的算术运算。</li>
<li>LoG (‘ Laplacian of Gaussian’)内核的参数可以预先计算，因此在运行时只需要对图像执行一遍的卷积即可。<br><img src="https://img-blog.csdnimg.cn/bd40ed4914ef4ebaa5fd705721f23519.png#pic_center" alt="在这里插入图片描述">算法步骤如下：</li>
<li>滤波：首先对图像f(x,y)进行平滑滤波，其滤波函数根据人类视觉特性选为高斯函数，</li>
<li>增强：对平滑图像进行拉普拉斯运算，</li>
<li>检测：利用二阶导数算子过零点的性质，可确定图像中阶跃边缘的位置</li>
</ul>
<p>由于的平滑性质能减少噪声的影响，所以当边缘模糊或噪声较大时，利用检测过零点能提供较可靠的边缘位置。在该算子中，σ 的选择很重要， σ 小时边缘位置精度高，但边缘细节变化多； σ 大时平滑作用大，但细节损失大，边缘点定位精度低。应根据噪声水平和边缘点定位精度要求适当选取 σ。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Marr算子：Lalpacian of Gaussion(LOG)</span></span><br><span class="line"><span class="comment">#cv2.GuassianBlur(img, ksize,sigmaX,sigmaY),sigmaX和sigmaY表示x和y方向上的高斯核标准差</span></span><br><span class="line">gaussion = cv.GaussianBlur(img_clean, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">0</span>)<span class="comment">#先用高斯滤波对图像进行平滑处理</span></span><br><span class="line">dst = cv.Laplacian(gaussion, cv.CV_16S, ksize = <span class="number">3</span>)<span class="comment">#再用拉普拉斯算子进行边缘检测，第二个参数CV_16s表示图像中的数据是16位无符号整数</span></span><br><span class="line">log = cv.convertScaleAbs(dst)  <span class="comment">#convertScaleAbs函数是一个位深转化函数，可将任意类型的数据转化为CV_8UC1</span></span><br><span class="line"><span class="comment">#                                 (0)CV_8UCV1表示8位无符号整数，且通道为一</span></span><br><span class="line"><span class="comment">#                                 (1). 对于src*alpha+beta的结果如果是负值且大于-255，则直接取绝对值；</span></span><br><span class="line"><span class="comment">#                                 (2). 对于src*alpha+beta的结果如果大于255，则取255；</span></span><br><span class="line"><span class="comment">#                                 (3). 对于src*alpha+beta的结果是负值，且小于-255，则取255；</span></span><br><span class="line"><span class="comment">#                                 (4). 对于src*alpha+beta的结果如果在0-255之间，则保持不变；</span></span><br><span class="line"> </span><br><span class="line">fig = plt.figure(figsize = (<span class="number">10</span>, <span class="number">5</span>))<span class="comment">#作图</span></span><br><span class="line">fig.<span class="built_in">set</span>(alpha = <span class="number">0.2</span>)</span><br><span class="line">plt.subplot2grid((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">plt.imshow(img_clean, <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">plt.imshow(log, <span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/d81987e5dd434b8f929c74cd6c6849e0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5oiR5piv6I-c6bih5L2G5oiR6L-Y6KaB6K-0,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器视觉</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
        <tag>边缘检测</tag>
      </tags>
  </entry>
  <entry>
    <title>【OpenCv】kirsch 算子边缘检测</title>
    <url>/2022/06/05/%E3%80%90OpenCv%E3%80%91kirsch-%E7%AE%97%E5%AD%90%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>﻿Kirsch算子是R.Kirsch提出来一种边缘检测算法，它采用8个模板对图像上的每一个像素点进行卷积求导数，这8个模板代表8个方向，对图像上的8个特定边缘方向作出最大响应，运算中取最大值作为图像的边缘输出。<br><img src="https://img-blog.csdnimg.cn/b9bf43ef7ce24050b03fd22c438d597b.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b9951f6416954240a2cd2898d9e7b3be.png#pic_center" alt="在这里插入图片描述"><br>Kirsch算子特点<br>• 在计算边缘强度的同时可以得到边缘的方向<br>• 各方向间的夹角为45º</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib</span><br><span class="line"><span class="comment">#kirsch算子</span></span><br><span class="line"><span class="comment">#自定义卷积核，八个方向</span></span><br><span class="line">m1 = np.array([[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], [-<span class="number">3</span>,<span class="number">0</span>,-<span class="number">3</span>], [-<span class="number">3</span>,-<span class="number">3</span>,-<span class="number">3</span>]])</span><br><span class="line">m2 = np.array([[-<span class="number">3</span>, <span class="number">5</span>,<span class="number">5</span>], [-<span class="number">3</span>,<span class="number">0</span>,<span class="number">5</span>], [-<span class="number">3</span>,-<span class="number">3</span>,-<span class="number">3</span>]])</span><br><span class="line">m3 = np.array([[-<span class="number">3</span>,-<span class="number">3</span>,<span class="number">5</span>], [-<span class="number">3</span>,<span class="number">0</span>,<span class="number">5</span>], [-<span class="number">3</span>,-<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">m4 = np.array([[-<span class="number">3</span>,-<span class="number">3</span>,-<span class="number">3</span>], [-<span class="number">3</span>,<span class="number">0</span>,<span class="number">5</span>], [-<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>]])</span><br><span class="line">m5 = np.array([[-<span class="number">3</span>, -<span class="number">3</span>, -<span class="number">3</span>], [-<span class="number">3</span>,<span class="number">0</span>,-<span class="number">3</span>], [<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>]])</span><br><span class="line">m6 = np.array([[-<span class="number">3</span>, -<span class="number">3</span>, -<span class="number">3</span>], [<span class="number">5</span>,<span class="number">0</span>,-<span class="number">3</span>], [<span class="number">5</span>,<span class="number">5</span>,-<span class="number">3</span>]])</span><br><span class="line">m7 = np.array([[<span class="number">5</span>, -<span class="number">3</span>, -<span class="number">3</span>], [<span class="number">5</span>,<span class="number">0</span>,-<span class="number">3</span>], [<span class="number">5</span>,-<span class="number">3</span>,-<span class="number">3</span>]])</span><br><span class="line">m8 = np.array([[<span class="number">5</span>, <span class="number">5</span>, -<span class="number">3</span>], [<span class="number">5</span>,<span class="number">0</span>,-<span class="number">3</span>], [-<span class="number">3</span>,-<span class="number">3</span>,-<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">filterlist = [m1, m2, m3, m4, m5, m6, m7, m8]<span class="comment">#将各个方向的卷积核放到一起便于统一操作</span></span><br><span class="line">filtered_list = np.zeros((<span class="number">8</span>, img_clean.shape[<span class="number">0</span>], img_clean.shape[<span class="number">1</span>]))<span class="comment">#建立三维数组，第0维表示各个方向卷积后的值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>) :</span><br><span class="line">    out = cv.filter2D(img_clean, cv.CV_16S, filterlist[k])<span class="comment">#自定义卷积，其实里面的步骤跟Sobel算子是差不多的</span></span><br><span class="line">    filtered_list[k] = out</span><br><span class="line"></span><br><span class="line">final = np.<span class="built_in">max</span>(filtered_list, axis = <span class="number">0</span>)<span class="comment">#取八个方向中的最大值，也就是取第0维的最大值作为图像该点，滤波之后的新的像素值</span></span><br><span class="line">final[ np.where(final &gt;= <span class="number">255</span>)] = <span class="number">255</span><span class="comment">#令像素值大于255的点等于255</span></span><br><span class="line">final[ np.where(final &lt; <span class="number">255</span>) ] = <span class="number">0</span><span class="comment">#令像素值小于255的点等于0</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">10</span>, <span class="number">5</span>))<span class="comment">#显示图像</span></span><br><span class="line">fig.<span class="built_in">set</span>(alpha = <span class="number">0.2</span>)</span><br><span class="line">plt.subplot2grid((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">plt.imshow(img_clean, <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">plt.imshow(final, <span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/678a0bbfe3d94918bcf2eaefebe238a3.png#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器视觉</category>
      </categories>
      <tags>
        <tag>机器视觉</tag>
        <tag>边缘检测</tag>
      </tags>
  </entry>
  <entry>
    <title>前向传播</title>
    <url>/2022/06/05/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<h1 id="一、Learning-rate"><a href="#一、Learning-rate" class="headerlink" title="一、Learning rate"></a>一、Learning rate</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>顾名思义，就是要给每个参数不同的learning rate，上一篇笔记中，我们提到了在沿着Loss函数我们可能会陷入local minima等一些gradient为零从而导致参数无法更新，Loss也就不再下降。但事实是当Loss不再下降的时候，gradient不一定很小，如下图<img src="https://img-blog.csdnimg.cn/d01b9835e2264c7b9e81ed8e431e8824.png#pic_center" alt="在这里插入图片描述"><br>当Loss很小时，gradient仍有在某时候是很大的，我们可以想象下面这样的情形导致的<img src="https://img-blog.csdnimg.cn/f5c537eea08244889a835027671b8d14.png#pic_center" alt="在这里插入图片描述"><br>由图知道Loss不再下降并不是因为卡在local minima或者saddle point，而是因为此时的learning rate太大导致点更新的步伐过大使gradient在“山谷”两侧“反复横跳”导致Loss不再下降。再如下面这个例子：假设只有两个参数不断计算gradient来降低Loss<img src="https://img-blog.csdnimg.cn/9e68d8dd538a43ccb978343394cfb365.png#pic_center" alt="在这里插入图片描述"><br>我们的目的就是能够使黑点到达黄色点的地方，此时Loss最小，设黄色点为山谷最低端，两旁是山壁，前文有说到当Loss不再下降时可能是因为“反复横跳”：<img src="https://img-blog.csdnimg.cn/3026ef9a40be4970a47e20184b7e2458.png#pic_center" alt="在这里插入图片描述"><br>接下来可能就有一个疑问就是，为什么不把learning rate设低一点让黑点跨的步伐小一点进入中间地区呢？嗯，是个好疑问，我们把learning rate设为10e-7看看<img src="https://img-blog.csdnimg.cn/d30a77a1a71848ddb1e40d45d9149c34.png#pic_center" alt="在这里插入图片描述"><br>从图可以看出，虽然黑点不再反复横跳，但他仍然不能到达黄点，这是因为此时的Loss函数已经十分平缓，而learning rate又太小而导致他不能继续再往前，所以我们需要更加特殊的gradient descent</p>
<h2 id="1-1-不同的参数需要不同的learning-rate"><a href="#1-1-不同的参数需要不同的learning-rate" class="headerlink" title="1.1 不同的参数需要不同的learning rate"></a>1.1 不同的参数需要不同的learning rate</h2><p>当在某个地方的方向十分陡峭时我们就需要小的learning rate，反之需要大的learning rate<img src="https://img-blog.csdnimg.cn/ee5b846ca140495c89eb308077217486.png#pic_center" alt="在这里插入图片描述"><br>按照普通的gradient descent，我们更新参数的方法是这样的<img src="https://img-blog.csdnimg.cn/74f768cc249f499aa97ef276c10e4ec4.png#pic_center" alt="在这里插入图片描述"><br>而现在我们因为需要同时更新learning rate，所以将learning rate除以依赖于某个对应参数的未知参数δ<img src="https://img-blog.csdnimg.cn/2950f70ec1ba48ed8fd06236df08722c.png#pic_center" alt="在这里插入图片描述"><br>下面来计算δ的值</p>
<h3 id="1-1-1-Root-Mean-Square计算δ"><a href="#1-1-1-Root-Mean-Square计算δ" class="headerlink" title="1.1.1 Root Mean Square计算δ"></a>1.1.1 Root Mean Square计算δ</h3><p><img src="https://img-blog.csdnimg.cn/e44708fa01b64c6eab2d79fdff90bf0d.png#pic_center" alt="在这里插入图片描述"><br>依照刚刚的δ的计算方法，当在Loss函数曲线较平缓的地方的gradient较小，因为δ跟gradient是呈正比关系，则算出来的δ也小，则learning rate较大，那便可以跨的步伐更大使Loss变化大一点<img src="https://img-blog.csdnimg.cn/d20cf402a93c48cfa6861b99059dd2e2.png#pic_center" alt="在这里插入图片描述"><br>在较陡的地方则相反<img src="https://img-blog.csdnimg.cn/036bf3a6f06b4ce7887944c753e1fe4a.png#pic_center" alt="在这里插入图片描述"><br>但是的话上面的方法还是不够好的，从上面的图中gradient的值都是差不多，都是朝着一个方向呈单调性变化的，但是在现实实验中我们可能会遇到的gradient在一个方向变化的程度可以是很大的<img src="https://img-blog.csdnimg.cn/b7a114266f664c81a52dfd3139dc1469.png#pic_center" alt="在这里插入图片描述"><br>所以我们就需要能够动态改变learning rate的值</p>
<h3 id="1-1-2-RMSProp计算δ"><a href="#1-1-2-RMSProp计算δ" class="headerlink" title="1.1.2 RMSProp计算δ"></a>1.1.2 RMSProp计算δ</h3><p><img src="https://img-blog.csdnimg.cn/7f3d35962c424a00af9605f6b4f7b8c5.png#pic_center" alt="在这里插入图片描述"><br>δ的值取决于gradient的大小，当gradient比较大的时候说明loss很陡峭需要刹车，所以δ的值也会变大，使得learning rate变小，达到一个刹车的目的，否则反之<br><img src="https://img-blog.csdnimg.cn/df8ed34532034b039c27eda2e796004f.png#pic_center" alt="在这里插入图片描述"><br>我们结合RMSProp和Momentum就可以构成最常用的优化器：Adam</p>
<h2 id="1-2-检验结果"><a href="#1-2-检验结果" class="headerlink" title="1.2 检验结果"></a>1.2 检验结果</h2><p>没有动态调整learning rate<br><img src="https://img-blog.csdnimg.cn/158a6ed277ca46a68fb7946c78fe8180.png#pic_center" alt="在这里插入图片描述"><br>之所以 出现红色圆圈里的现象是因为，随着gradient在y轴方向积累了很多小的值，使得δ在y轴方向很小导致learning rate太大出现井喷现象，但不会永远做简谐运动，因为随着learning rate还在不断更新黑点又会重新回到中间，我们有一种方法可以解决这个问题，就是Learning Rate Scheduling<img src="https://img-blog.csdnimg.cn/84200e0269764827aada97b055868bec.png#pic_center" alt="在这里插入图片描述"><br>使随着时间的增加让learning rate变的小一些<img src="https://img-blog.csdnimg.cn/b3d8aef1a911433da070ddda757a4b78.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="二、Classification"><a href="#二、Classification" class="headerlink" title="二、Classification"></a>二、Classification</h1><p>在第一篇的笔记中有写过Regression的output是一个数值，Classification的output是一个类别，如果结合起来看，我们是否可以将Classification当作Regression去训练呢？<br>这期间要求我们做得变化是将类别变成one-hot vector（独热向量）：<br>Class 1 = [1 0 0]T<br>Class 2 = [0 1 0]T<br>Class 3 = [0 0 1]T<br>所以我们就能像做Regression一样得到三组数据<br><img src="https://img-blog.csdnimg.cn/88ce9b37a3d74ef5a2d8e9a06e9398aa.png#pic_center" alt="在这里插入图片描述"><br>最后将得到的y值经过softmax函数得到y’再计算与y^的距离<img src="https://img-blog.csdnimg.cn/843878427f184940a40d2e9337c2a743.png#pic_center" alt="在这里插入图片描述"><br>因为经过神经网络我们得到的y值可能是任何值，但是我们的target只有0和1，所以需要做normalization（归一化）将y值限制到0-1之间，softmax的工作就是这个，具体过程如下：<br><img src="https://img-blog.csdnimg.cn/20ffebbcfb72426590151b6691096fce.png#pic_center" alt="在这里插入图片描述"><br>当我们在做两个类别分类时，可以直接使用sigmoid函数，因为sigmoid函数的取值范围是在0-1之间。<br>对于Classification时我们常用的Loss函数是Cross-entropy，表达式为：Error = -∑y^i*lny’i，在pytorch中，Cross-entropy常常是与softmax结合在一起的。</p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><p>首先我们先来看两个参数w1、w2对Loss的影响：<br><img src="https://img-blog.csdnimg.cn/c0485ddfd05f430ebe212ec3ef195d32.png#pic_center" alt="在这里插入图片描述"><br>且对应的神经网络是十分简单的：<br><img src="https://img-blog.csdnimg.cn/920d7fb08e504d15a8ae92bbe04e7ad2.png#pic_center" alt="在这里插入图片描述"><br>x1改变一点后，最终L值会改变，但是由于x1的输入都很小时，其实对L的影响变不大。但是如果w2的输入很大，虽然w2只是改变一点点，最终也会对L造成很大影响<img src="https://img-blog.csdnimg.cn/e7d9ff9b971d477b8dfc48aa688d893f.png#pic_center" alt="在这里插入图片描述"><br>所以我们应该让不同维度的数值都有一个相同范围。<br>假设R笔数据分布是这样的<br><img src="https://img-blog.csdnimg.cn/428644aa65a24bf49e06a89e57264e2b.png#pic_center" alt="在这里插入图片描述"><br>我们先把同一维不同笔资料的mean计算出来，用那一维的数减去mean，再除以standard deviation（标准偏差）得到新的x’再放回原来位置，最终feature值都在0左右。考虑深度学习神经网络做normalization<br><img src="https://img-blog.csdnimg.cn/79230d73a6854f4eab4d0c1c1710c1e0.png#pic_center" alt="在这里插入图片描述"><br>开始时我们对input vector做了normalization之后得到z1，z2，z3，而他们又是后一层网络的输入，但是我们得到z值的时候他们的分布并不是分布在0和1之间的，这样会导致第二层训练起来比较困难。所以我们要对z做normalization，一样的步骤，计算mean，std：<br><img src="https://img-blog.csdnimg.cn/44a18541c94c4c13bc08443d391fa954.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/37d00de5708245989c58a6b02ede0642.png#pic_center" alt="在这里插入图片描述"><br>上面这一大部分我们可以看成一层网络，这一层网络时是十分复杂的，因为里面有多个input和output，假如我们一次性把全部的资料读进去的话，计算量十分大，所以我们要考虑每次只对一部分的资料normalization，这就是Batch Normalization。值得注意的是Batch要足够多才行，只有这样才能算出u和δ的值<br>上面都是训练过程，现在来看一下testing：<br>上面我们知道u和    δ只能从比较大的Batch算出来，但是testing时，我们的数据量是不一定能够达到所需Batch值，<img src="https://img-blog.csdnimg.cn/d8c92380d5164808b2760b8e4f94a3c9.png#pic_center" alt="在这里插入图片描述"><br>这里有一个解决方法是，在训练时会把我们每次normalization得到的u1，u2，u3……拿出来计算得到平均值：<br><img src="https://img-blog.csdnimg.cn/263bf80c401647f1becfa9084d788548.png#pic_center" alt="在这里插入图片描述"><br>所以我们在testing时就不用计算u和δ的值，直接拿平均值即可</p>
<h1 id="三、结语"><a href="#三、结语" class="headerlink" title="三、结语"></a>三、结语</h1><p>以上是我本人学习机器学习的学习笔记的第三篇，有错误的地方还望指出，共勉！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树练习</title>
    <url>/2022/06/05/%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%83%E4%B9%A0/</url>
    <content><![CDATA[<p>﻿数据预处理分析，最后面附有决策树算法的实现<br>原始数据：<br><a href="https://github.com/aishangcengloua/Data-Mining/blob/master/HomeWork1/Data.xlsx">原数据地址</a><br><img src="https://img-blog.csdnimg.cn/810c34d959d0490383d5dd87183a7659.png#pic_center" alt="在这里插入图片描述"><br>计算第一次决策如果<br>分别对在14天各个属性下是否进行施肥的统计情况且计算该属性的基尼指数，同一种属性不同表现的基尼指数表示为M，加权平均之后为节点的基尼指数，用N表示<br>天气：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#encoding = utf-8</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">Base_file = pd.read_excel(<span class="string">&#x27;Data.xlsx&#x27;</span>)</span><br><span class="line">Base_file.head(<span class="number">15</span>)</span><br><span class="line"><span class="comment">#Base_file.head()</span></span><br><span class="line">Weather_Sunny = Base_file[Base_file[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;晴天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line"><span class="comment">#print(Weather_Sunny)[&#x27;否&#x27; &#x27;否&#x27; &#x27;否&#x27; &#x27;是&#x27; &#x27;是&#x27;]</span></span><br><span class="line">Weather_Rainy = Base_file[Base_file[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;雨天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Overcast = Base_file[Base_file[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;阴天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Overcast[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Weather_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Weather_Sunny[<span class="string">&#x27;是&#x27;</span>], Weather_Sunny[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Weather_Rainy[<span class="string">&#x27;是&#x27;</span>], Weather_Rainy[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Weather_Overcast[<span class="string">&#x27;是&#x27;</span>], Weather_Overcast[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index=[<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;阴天&#x27;</span>])</span><br><span class="line">Weather_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/ff10b766deb94b239cab9d021788cf01.png" alt="在这里插入图片描述"><br>晴天：M1 = 2 <em> 2/5 </em> (1 - 2/5) = 0.444444445<br>雨天：M2 = 2 <em> 3/5 </em> (1 - 3/5) = 0.48<br>阴天：M3 = 0<br>N1 = 5/14 <em> M1 + 5/14 </em> M2 = 0.343<br>温度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Hot = Base_file[Base_file[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;炎热&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cool = Base_file[Base_file[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;温&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold = Base_file[Base_file[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;冷&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Temperature_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Hot[<span class="string">&#x27;是&#x27;</span>], Hot[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cool[<span class="string">&#x27;是&#x27;</span>], Cool[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cold[<span class="string">&#x27;是&#x27;</span>], Cold[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>])</span><br><span class="line">Temperature_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/f05b14015fcf4abaadc25035109cde0a.png" alt="在这里插入图片描述"><br>炎热：M1 = 2 <em> 2/4 </em> (1 - 2/4) = 0.5<br>温 ： M2 = 2 <em> 2/6 </em> (1 - 2/6) = 0.44444445<br>冷：  M3 = 2 <em> 3/4 </em> (1 - 3/4) = 0.375<br>N2 = 4/14 <em> M1 + 6/14 </em> M2 + 4/14 * M3 = 0.440</p>
<p>湿度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Humidity_high = Base_file[Base_file[<span class="string">&#x27;湿度&#x27;</span>] == <span class="string">&#x27;高&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Humidity_mid = Base_file[Base_file[<span class="string">&#x27;湿度&#x27;</span>] == <span class="string">&#x27;中&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Humidity_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Humidity_high[<span class="string">&#x27;是&#x27;</span>], Humidity_high[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Humidity_mid[<span class="string">&#x27;是&#x27;</span>], Humidity_mid[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;中&#x27;</span>])</span><br><span class="line">Humidity_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/92fcd8995d9c4f0398eea4ab5f9e21f1.png" alt="在这里插入图片描述"><br>高：M1 = 2 <em> 3/4 </em> (1 - 3/4) = 0.375<br>中：M2 = 2 <em> 6/7 </em> (1 - 6/7) = 0.245<br>N3 = 1/2 <em> M1 + 1/2 </em> M2 = 0.310</p>
<p>风力：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Wind_strong = Base_file[Base_file[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;强风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_weak = Base_file[Base_file[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;弱风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Wind_strong[<span class="string">&#x27;是&#x27;</span>], Wind_strong[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Wind_weak[<span class="string">&#x27;是&#x27;</span>], Wind_weak[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>])</span><br><span class="line">Wind_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/af12704b2e4447faaa8e547b580d8ded.png" alt="在这里插入图片描述"><br>强风：M1 = 2 <em> 3/6 </em> (1 - 3/6) = 0.5<br>弱风：M2 = 2 <em> 6/8 </em> (1 - 6/8) = 0.375<br>N4 = 6/14 <em> M1 + 8/14 </em> M2 = 0.429<br>因为N2 &gt; N4 &gt; N1 &gt; N3，所以第一次决策应根据湿度来分类：<br><img src="https://img-blog.csdnimg.cn/548dfadd9a1048e79cc824aa8cffbf2f.png#pic_center" alt="在这里插入图片描述"><br>因为此次分类之后，然未出现叶子节点，所以需要分别对第二排的两个节点进行分类，过程与第一次决策类似，计算各个属性下是否进行施肥的统计情况且计算该属性的基尼指数<br>左右节点的数据分别如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Base_file = pd.read_excel(<span class="string">&#x27;Data.xlsx&#x27;</span>)</span><br><span class="line">Base_file.head(<span class="number">15</span>)</span><br><span class="line">Temperature_df_high = Base_file[Base_file[<span class="string">&#x27;湿度&#x27;</span>] == <span class="string">&#x27;高&#x27;</span>]</span><br><span class="line">Temperature_df_high.head(<span class="number">14</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/673df477a0364e0cb1838f047d1d0f50.png#pic_center" alt="在这里插入图片描述"></p>
<p>先对左边节点分析：<br>天气：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weather_Sunny = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;晴天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Sunny[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#print(Weather_Sunny)[&#x27;否&#x27; &#x27;否&#x27; &#x27;否&#x27; &#x27;是&#x27; &#x27;是&#x27;]</span></span><br><span class="line">Weather_Rainy = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;雨天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Overcast = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;阴天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Overcast[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Weather_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Weather_Sunny[<span class="string">&#x27;是&#x27;</span>], Weather_Sunny[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Weather_Rainy[<span class="string">&#x27;是&#x27;</span>], Weather_Rainy[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Weather_Overcast[<span class="string">&#x27;是&#x27;</span>], Weather_Overcast[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index=[<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;阴天&#x27;</span>])</span><br><span class="line">Weather_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/18ffcc15e36b4ad997ae445099cb48bf.png" alt="在这里插入图片描述"><br>晴天：M1 = 0<br>雨天：M2 = 0.5<br>阴天：M3 = 0<br>N1 = 2/7 * M2 = 0.143<br>温度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Hot = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;炎热&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cool = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;温&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;冷&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cold[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Temperature_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Hot[<span class="string">&#x27;是&#x27;</span>], Hot[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cool[<span class="string">&#x27;是&#x27;</span>], Cool[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cold[<span class="string">&#x27;是&#x27;</span>], Cold[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>])</span><br><span class="line">Temperature_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/08ea7732db2b43aea8876d02fe17b5b0.png" alt="在这里插入图片描述"><br>炎热：M1 = 0.44444444445<br>温：  M2 = 0.5<br>冷：  M3 = 0<br>N2 = 3/7 <em> M1 + 4/7 </em> M3 = 0.476</p>
<p>风力：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Wind_strong = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;强风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_weak = Temperature_df_high[Temperature_df_high[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;弱风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Wind_strong[<span class="string">&#x27;是&#x27;</span>], Wind_strong[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Wind_weak[<span class="string">&#x27;是&#x27;</span>], Wind_weak[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>])</span><br><span class="line">Wind_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/4419fb6e26a24f6a8bcb0fd1ef0ffeb3.png" alt="在这里插入图片描述"><br>强风：M1 = 0.44444444445<br>弱风：M2 =0.5<br>N3 = 3/7 <em> M1 + 4/7 </em> M2 = 0.476</p>
<p>N1 &gt; N2 = N3<br>所以左边的节点来说应该根据天气情况来分类<br>对右边节点分析：<br>天气：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weather_Sunny = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;晴天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Sunny[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Weather_Rainy = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;雨天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Overcast = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;阴天&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Weather_Overcast[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Weather_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Weather_Sunny[<span class="string">&#x27;是&#x27;</span>], Weather_Sunny[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Weather_Rainy[<span class="string">&#x27;是&#x27;</span>], Weather_Rainy[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Weather_Overcast[<span class="string">&#x27;是&#x27;</span>], Weather_Overcast[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index=[<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;阴天&#x27;</span>])</span><br><span class="line">Weather_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/48783b2c05b943d18eca303e936829dd.png" alt="在这里插入图片描述"><br>晴天：M1 = 0<br>雨天：M2 = 0.444444444445<br>阴天：M3 = 0<br>N1 = 3/7 * M2 = 0.190</p>
<p>温度：</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Hot = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;炎热&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Hot[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cool = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;温&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cool[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cold = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;冷&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line"></span><br><span class="line">Temperature_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Hot[<span class="string">&#x27;是&#x27;</span>], Hot[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cool[<span class="string">&#x27;是&#x27;</span>], Cool[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cold[<span class="string">&#x27;是&#x27;</span>], Cold[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>])</span><br><span class="line">Temperature_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/291ad482ad4d4717aef3403865a56133.png" alt="在这里插入图片描述"><br>炎热：M1 = 0<br>温：  M2 = 0<br>冷：  M3 = 2 <em> 3/4 </em> (1 - 3/4) = 0.375<br>N2 = 3/7 * M3 = 0.214</p>
<p>风力：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Wind_strong = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;强风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_weak = Temperature_df_mid[Temperature_df_mid[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;弱风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_weak[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Wind_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Wind_strong[<span class="string">&#x27;是&#x27;</span>], Wind_strong[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Wind_weak[<span class="string">&#x27;是&#x27;</span>], Wind_weak[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>])</span><br><span class="line">Wind_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/1554bd1f61ec4b1da67841305c95e111.png" alt="在这里插入图片描述"><br>强风：M1 = 2 <em> 2/3 </em> (1 - 2/3) = 0.44444445<br>弱风：M2 = 0<br>N3 = 3/7 * M2 = 0.190</p>
<p>N1 = N3 &gt; N2<br>这里可以有两种分类决策方法，这里选择使用天气属性对右边节点进行分类，结合对左边节点的分析，对第二层的分类如下：<br><img src="https://img-blog.csdnimg.cn/38a2759681c844159c960655f0178c43.png#pic_center" alt="在这里插入图片描述"><br>经过第二次分类之后，出现了叶子节点，只剩下两个节点需要继续分类，且只剩下温度和风力两个属性，下面是第二次分类之后的左右两个节点数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Weather_df_Rainy1 = Base_file[Base_file[<span class="string">&#x27;湿度&#x27;</span>] == <span class="string">&#x27;高&#x27;</span>][Base_file[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;雨天&#x27;</span>]</span><br><span class="line">Weather_df_Rainy1.head()</span><br><span class="line">Weather_df_Rainy2 = Base_file[Base_file[<span class="string">&#x27;湿度&#x27;</span>] == <span class="string">&#x27;中&#x27;</span>][Base_file[<span class="string">&#x27;天气&#x27;</span>] == <span class="string">&#x27;雨天&#x27;</span>]</span><br><span class="line">Weather_df_Rainy2.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/f4af06ecd7604f528bf833f5f17c9f96.png" alt="在这里插入图片描述"><br>对于左边节点：<br>温度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Hot = Weather_df_Rainy1[Weather_df_Rainy1[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;炎热&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Hot[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Hot[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cool = Weather_df_Rainy1[Weather_df_Rainy1[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;温&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold = Weather_df_Rainy1[Weather_df_Rainy1[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;冷&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cold[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">Temperature_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Hot[<span class="string">&#x27;是&#x27;</span>], Hot[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cool[<span class="string">&#x27;是&#x27;</span>], Cool[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cold[<span class="string">&#x27;是&#x27;</span>], Cold[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>])</span><br><span class="line">Temperature_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/e2205a76332e4092a40b97d5482a3620.png" alt="在这里插入图片描述"><br>炎热：M1 = 0<br>温：  M2 = 2 <em> 1/2 </em> (1 – 1/2) = 0.5<br>冷：  M3 = 0<br>N1 = M2 = 0.5</p>
<p>风力：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Hot = Weather_df_Rainy1[Weather_df_Rainy1[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;炎热&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Hot[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Hot[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cool = Weather_df_Rainy1[Weather_df_Rainy1[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;温&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold = Weather_df_Rainy1[Weather_df_Rainy1[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;冷&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cold[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cold[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">Temperature_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Hot[<span class="string">&#x27;是&#x27;</span>], Hot[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cool[<span class="string">&#x27;是&#x27;</span>], Cool[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cold[<span class="string">&#x27;是&#x27;</span>], Cold[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>])</span><br><span class="line">Temperature_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/04908ef755dd418f97ec0442752a6e8f.png" alt="在这里插入图片描述"><br>强风：M1 = 0<br>弱风：M2 = 0<br>N2 = 0</p>
<p>N1 &gt; N2<br>所以左边的节点应用风力属性继续往后分类<br>对右边节点分析：<br>温度：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> Hot = Weather_df_Rainy2[Weather_df_Rainy2[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;炎热&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Hot[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Hot[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cool = Weather_df_Rainy2[Weather_df_Rainy2[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;温&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Cool[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Cold = Weather_df_Rainy2[Weather_df_Rainy2[<span class="string">&#x27;温度&#x27;</span>] == <span class="string">&#x27;冷&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Temperature_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Hot[<span class="string">&#x27;是&#x27;</span>], Hot[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cool[<span class="string">&#x27;是&#x27;</span>], Cool[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Cold[<span class="string">&#x27;是&#x27;</span>], Cold[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>])</span><br><span class="line">Temperature_df.head()</span><br></pre></td></tr></table></figure>
<p> <img src="https://img-blog.csdnimg.cn/28bff67159c84a8ca00d84972e1ae892.png" alt="在这里插入图片描述"><br>炎热：M1 = 0<br>温：  M2 = 0<br>冷：  M3 = 2 <em> 1/2 </em> (1 – 1/2) = 0.5<br>N1 = M3 = 0.5</p>
<p>风力：</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> Wind_strong = Weather_df_Rainy2[Weather_df_Rainy2[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;强风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_strong[<span class="string">&#x27;是&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Wind_weak = Weather_df_Rainy2[Weather_df_Rainy2[<span class="string">&#x27;风力&#x27;</span>] == <span class="string">&#x27;弱风&#x27;</span>][<span class="string">&#x27;是否施肥&#x27;</span>].value_counts()</span><br><span class="line">Wind_weak[<span class="string">&#x27;否&#x27;</span>] = <span class="number">0</span></span><br><span class="line">Wind_df = pd.DataFrame([</span><br><span class="line">    pd.Series([Wind_strong[<span class="string">&#x27;是&#x27;</span>], Wind_strong[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]),</span><br><span class="line">    pd.Series([Wind_weak[<span class="string">&#x27;是&#x27;</span>], Wind_weak[<span class="string">&#x27;否&#x27;</span>]], index = [<span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>])</span><br><span class="line">], index = [<span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>])</span><br><span class="line">Wind_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/7c9143d8b31a42b6be9b72fc65a6f6ce.png" alt="在这里插入图片描述"><br>强风：M1 = 0<br>弱风：M2 = 0<br>N2 = 0</p>
<p>N1 &gt; N2<br>所以右边边的节点应用风力属性继续往后分类，决策图如下：<br><img src="https://img-blog.csdnimg.cn/dc5aea1458624abda02b14f7538c57b9.png#pic_center" alt="在这里插入图片描述"></p>
<p>可以看出第二次分类再经过风力的分类之后，此时决策树最后一排的节点全部变为了叶子节点，说明至此，分类完成。<br>算法实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#encoding = utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CalcGiNiIndex</span>(<span class="params">DataSet</span>) :</span><br><span class="line">    Num_length = <span class="built_in">len</span>(DataSet)</span><br><span class="line">    labelcounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> DataSet :</span><br><span class="line">        currentlabel = feature[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#用字典统计类别及其数目</span></span><br><span class="line">        <span class="keyword">if</span> currentlabel <span class="keyword">not</span> <span class="keyword">in</span> labelcounts.keys() :</span><br><span class="line">            labelcounts[currentlabel] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            labelcounts[currentlabel] += <span class="number">1</span></span><br><span class="line">    GiNi_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelcounts.keys() :</span><br><span class="line">        <span class="comment">#二分类求基尼指数：GiNi = 2 * p * （1 - p）</span></span><br><span class="line">        GiNi_index = <span class="number">2</span> * (<span class="built_in">float</span>(labelcounts[key]) / Num_length) * (<span class="number">1</span> - <span class="built_in">float</span>(labelcounts[key]) / Num_length)</span><br><span class="line">    <span class="keyword">return</span> GiNi_index</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>() :</span><br><span class="line">    DataSet = [[<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;否&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;否&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;阴天&#x27;</span>, <span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;否&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;阴天&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;否&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;冷&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;晴天&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;阴天&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;阴天&#x27;</span>, <span class="string">&#x27;炎热&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;弱风&#x27;</span>, <span class="string">&#x27;是&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;雨天&#x27;</span>, <span class="string">&#x27;温&#x27;</span>, <span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;强风&#x27;</span>, <span class="string">&#x27;否&#x27;</span>]]</span><br><span class="line"><span class="comment">#     file = pd.read_excel(&#x27;Data.xlsx&#x27;)</span></span><br><span class="line"><span class="comment">#     DataSet = file.iloc[ : , 1 : ]</span></span><br><span class="line"><span class="comment">#     DataSet = np.array(DataSet)</span></span><br><span class="line">    labels = [<span class="string">&#x27;天气&#x27;</span>, <span class="string">&#x27;温度&#x27;</span>, <span class="string">&#x27;湿度&#x27;</span>, <span class="string">&#x27;风力&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DataSet, labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">DataSet, axis, value</span>): </span><br><span class="line">    <span class="comment">#计算以某个特征分类后剩下的数据量。</span></span><br><span class="line">    <span class="comment">#axis表示第i个特征，value表示在改特征的情况下的具体表现，如天气特征有雨天。阴天等。</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="comment">#创建一个新列表，准备提取数据</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> DataSet :</span><br><span class="line">        <span class="comment">#剔除在数据集中需要被分类的特征的列行</span></span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value :</span><br><span class="line">            reducedFeatVec = featVec[ : axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis + <span class="number">1</span> : ])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ChooseBestFeatureToSplit</span>(<span class="params">DataSet</span>) :</span><br><span class="line">    <span class="comment">#计算父亲节点的GiNi指数</span></span><br><span class="line">    FatherGiNi = CalcGiNiIndex(DataSet)</span><br><span class="line">    BestIoFoGain = <span class="number">0</span></span><br><span class="line">    BestFeature = -<span class="number">1</span></span><br><span class="line">    numFeature = <span class="built_in">len</span>(DataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    <span class="comment">#去掉最后一列</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeature) :</span><br><span class="line">        <span class="comment">#每个特征下的具体表现形式</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> DataSet]</span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)<span class="comment">#去重复</span></span><br><span class="line">        newGiNi = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals :</span><br><span class="line">            <span class="comment">#计算每一种表现形式GiNi指数的权重和</span></span><br><span class="line">            subData = splitDataSet(DataSet, i, value)</span><br><span class="line">            prob = <span class="built_in">len</span>(subData) / <span class="built_in">float</span>(<span class="built_in">len</span>(DataSet))</span><br><span class="line">            newGiNi += prob * CalcGiNiIndex(subData)</span><br><span class="line">        infogain = FatherGiNi - newGiNi<span class="comment">#计算信息增益</span></span><br><span class="line">        <span class="keyword">if</span> infogain &gt; BestIoFoGain :</span><br><span class="line">            <span class="comment">#比较信息增益的大小，更新最佳分类的特征</span></span><br><span class="line">            BestIoFoGain = infogain</span><br><span class="line">            BestFeature = i</span><br><span class="line">    <span class="keyword">return</span> BestFeature</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):    <span class="comment">#按分类后类别数量排序，比如：最后分类为2yes1no，则判定为yes；</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">            classCount[vote]=<span class="number">0</span></span><br><span class="line">        classCount[vote]+=<span class="number">1</span></span><br><span class="line">    <span class="comment">#字典逆序排序</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCount.items(), key = <span class="keyword">lambda</span> x : x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">DataSet, labels</span>) :</span><br><span class="line">    ConditionList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> DataSet]</span><br><span class="line">    <span class="comment">#list.count(element) 方法用于统计某个元素在列表中出现的次数。</span></span><br><span class="line">    <span class="keyword">if</span> ConditionList.count(ConditionList[<span class="number">0</span>]) == <span class="built_in">len</span>(ConditionList) :<span class="comment">#yes or no</span></span><br><span class="line">        <span class="keyword">return</span> ConditionList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#dataSet[0]取矩阵第一行，dataSet[0][0]取矩阵第一行第一列元素</span></span><br><span class="line">    <span class="comment">#递归终止条件2：使用完所有特征，则返回最后出现次数最多的那个标签</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>( DataSet[<span class="number">0</span>] ) == <span class="number">1</span>:   </span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    </span><br><span class="line">     <span class="comment">#以上两个终止条件都不满足，开始选择最优特征划分，已经有了一个方框，准备往方框中写入判断问题</span></span><br><span class="line">    BestFeature = ChooseBestFeatureToSplit(DataSet)</span><br><span class="line">    BestFeatLabel = labels[BestFeature]</span><br><span class="line">    mytree = &#123;BestFeatLabel : &#123;&#125;&#125;</span><br><span class="line">    <span class="comment">#用过了该特征，将该特征从所有特征列表中删除</span></span><br><span class="line">    <span class="keyword">del</span> (labels[BestFeature])</span><br><span class="line">    featValues = [example[BestFeature] <span class="keyword">for</span> example <span class="keyword">in</span> DataSet]</span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[ : ]</span><br><span class="line">        splitdata = splitDataSet(DataSet, BestFeature, value)</span><br><span class="line">        <span class="comment">#递归</span></span><br><span class="line">        mytree[BestFeatLabel][value] = createTree(splitdata, subLabels)</span><br><span class="line">    <span class="keyword">return</span> mytree</span><br><span class="line"></span><br><span class="line">DataSet, labels = createDataSet()<span class="comment"># 创造示列数据</span></span><br><span class="line"><span class="built_in">print</span>(createTree(DataSet, labels))<span class="comment"># 输出决策树模型结果</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/4c4883247d234b14a0660cf8a75baf70.png" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>传统算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络（一）</title>
    <url>/2022/06/05/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1    简介"></a>1    简介</h1><p>&emsp;&emsp;<strong>卷积网络</strong>（convolutional network）(LeCun, 1989)，也叫做 <strong>卷积神经网络</strong>（convolutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网格）。卷积网络在诸多应用领域都表现优异。“卷积神经网络’’ 一词表明该网络使用了 <strong>卷积</strong>（convolution）这种数学运算。卷积是一种特殊的线性运算。卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。<br>&emsp;&emsp;卷积网络为什么会出现？得先看看更早时期提出的 <strong>全连接网络</strong>（Fully Connected Neural Network，FC）在处理类似图片数据时会出现的问题：</p>
<ul>
<li>参数过多；</li>
<li>破坏图像的空间分布和像素之间的距离关系；</li>
</ul>
<p>针对上述问题，图像的主要特点：</p>
<ul>
<li>图像的关键特征可能只是图像的一小部分；</li>
<li>相同的特征可能出现在不同位置；</li>
<li>对一张图像进行抽样，不改变预测目标。</li>
</ul>
<p><strong>CNN</strong> 正是针对图像的特点来解决 <strong>FC</strong> 的缺点的。卷积神经网络通常由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括 <strong>关联权重</strong> 和 <strong>池化层</strong>（Pooling Layer）等。下图就是一个卷积神经网络架构。</p>
<p><img src="https://img-blog.csdnimg.cn/0f19278124b24dc8bf173254b12592a0.png" alt="在这里插入图片描述"><br>与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比其他深度、前馈神经网络，卷积神经网络可以用更少的参数，却获得更高的性能。卷积神经网络的一般结构包括卷积神经网络的常用层， 如卷积层、池化层、全连接层和输出层；有些还包括其他层，如正则化层、 高级层等。</p>
<h1 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2    卷积层"></a>2    卷积层</h1><p>&emsp;&emsp;针对FC神经网络处理图片数据的两个缺点，CNN提出的解决方案是 <strong>权值共享</strong> 和 <strong>卷积操作</strong>。首先要先理解一个概念：<strong>感受野</strong>，感受野表示当前特征图的 <strong>每个像素</strong> 表示 <strong>原始图像</strong> 的哪部分，看图说话：</p>
<p><img src="https://img-blog.csdnimg.cn/d67ed130fab64b38873845e7e13a6114.png#pic_center" alt="在这里插入图片描述"></p>
<p>输入图像  $\boldsymbol{X}$，维度为  $\boldsymbol{5×5}$，<strong>Layer2</strong> 的输出特征维度是 $\boldsymbol{3×3}$，则其中的任一像素表征着  $\boldsymbol{X}$ 的 9 个像素即 <strong>Layer1</strong> 中的绿色部分，则 <strong>Layer2</strong> 的感受野为 3，<strong>Layer3</strong> 的输出特征维度是 $\boldsymbol{1×1}$，只有一个像素，表征着 <strong>Layer2</strong> 整张特征图，对应到  $\boldsymbol{X}$ 也是表征  $\boldsymbol{X}$ 的整幅特征图，因此 <strong>Layer3</strong> 的感受野是 5。总结成一个规律就是，特征图越小，其感受野越大。 </p>
<h2 id="2-1-权值共享"><a href="#2-1-权值共享" class="headerlink" title="2.1    权值共享"></a>2.1    权值共享</h2><p>&emsp;&emsp;由我们上面介绍的感受野的概念，网络层的每个输出节点（输出像素）仅与感受野区域内 $\boldsymbol{k × k}$ 个输入节点相连接，假如输出节点数为 $\boldsymbol{J}$，则当前层的参数量为 $\boldsymbol{k × k×J}$，相对于全连接层的 $\boldsymbol{I×J}$，$\boldsymbol{k}$ 一般取值较小，如 1、 3、5 等，$\boldsymbol{k×k}$ 远小于 $\boldsymbol{I}$，因此成功地将参数量减少了很多。<strong>注：</strong> 上述假设以及在后面的示例中只有一层卷积层，这是为了用感受野容易表示卷积图像位置，如果是多层卷积，则不能随便使用感受野这个名词。<br>&emsp;&emsp;通过权值共享的思想，对于每个输出节点 $\boldsymbol{o_j}$，均使用相同的权值矩阵 $\boldsymbol{W}$，那么无论输出节点的数量 $\boldsymbol{J}$ 是多少，网络层的参数量总是 $\boldsymbol{k × k}$。如下图所示，在计算左上角位置的输出像素时，使用权值矩阵 $\boldsymbol{W}$：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
w_{00} & w_{01} & w_{02} \\
 w_{10} & w_{11} & w_{12}\\
 w_{020} & w_{21} & w_{22}
\end{bmatrix}</script><p>与对应感受野内部的像素相乘累加，作为左上角像素的输出值；在计算右下方感受野区域时，共享权值参数 $\boldsymbol{W}$，即使用相同的权值参数 $\boldsymbol{W}$相乘累加，得到右下角像素的输出值，此时网络层的参数量只有 $\boldsymbol{3 × 3 = 9}$ 个，且与输入、输出节点数无关。</p>
<p><img src="https://img-blog.csdnimg.cn/295c94aed75b4daca2c7aebdcff2d248.png#pic_center" alt="在这里插入图片描述"><br>通过运用局部相关性和权值共享的思想，我们成功把全连接网络的参数量从 $\boldsymbol{I×J}$ 减少到  $\boldsymbol{𝑘 × 𝑘}$ (准确地说，是在单输入通道、单卷积核的条件下)。这种共享权值的“局部连接层”网络其实就是卷积神经网络。</p>
<h2 id="2-2-卷积操作"><a href="#2-2-卷积操作" class="headerlink" title="2.2    卷积操作"></a>2.2    卷积操作</h2><p>&emsp;&emsp;卷积层是卷积神经网络的核心层，而卷积（Convolution）又是卷积层的核心。对卷积直观的理解，就是两个函数的一种运算，这种运算就称为卷积运算。下图就是一个简单的二维空间卷积运算示例，虽然简单，但却包含了卷积的核心内容。</p>
<p><img src="https://img-blog.csdnimg.cn/090c5cd557af42a586f92eb8f5c4e774.png#pic_center" alt="在这里插入图片描述"><br>上图中，输入和卷积核都是张量，卷积运算就是用卷积分别乘以输入张量中的每个元素，然后输出一个代表每个输入信息的张量。其中卷积核 （kernel）又称权重过滤器，简称为过滤器（filter）。我们可以将输入、卷积 核推广到更高维空间上，输入由 $\boldsymbol{2×2}$ 矩阵，拓展为 $\boldsymbol{5×5}$ 矩阵，卷积核由一个标量拓展为一个 $\boldsymbol{3×3}$ 矩阵，卷积结果如下：</p>
<p><img src="https://img-blog.csdnimg.cn/69d40f9aa9764aae88c092127b785b7f.png#pic_center" alt="在这里插入图片描述"><br>用卷积核中每个元素，乘以对应输入矩阵中的对应元素，这点还是一 样，但输入张量为 $\boldsymbol{5×5}$ 矩阵，而卷积核为 $\boldsymbol{3×3}$ 矩阵，所以这里首先就要解决 一个如何对应的问题，这个问题解决了，这个推广也就完成了。把卷积核作为在输入矩阵上的一个移动窗口，对应关系就迎刃而解了。了解上述的卷积方式之后，下面开始介绍的卷积是单通道输入、单卷积核的情况，然后推广至多通道输入、单卷积核，最后讨论最常用，也是最复杂的多通道输入、多个卷积核的卷积层实现。动图如下：<br><img src="https://img-blog.csdnimg.cn/f6e87cd35b984715bac41a76f68578b3.gif#pic_center" alt="在这里插入图片描述"><br>从数学上来讲，深度学习里面所谓的卷积运算，其实它被称为 <strong>互相关</strong>（cross-correlation）运算：将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，最后的结果组成一个矩阵，其中没有对核进行翻转。</p>
<h2 id="2-3-单通道输入和单卷积核"><a href="#2-3-单通道输入和单卷积核" class="headerlink" title="2.3    单通道输入和单卷积核"></a>2.3    单通道输入和单卷积核</h2><p>&emsp;&emsp;首先讨论单通道输入 $\boldsymbol{C_{in} = 1}$，如灰度图片只有灰度值一个通道，单个卷积核 $\boldsymbol{C_{out} = 1}$ 的情况（卷积核的个数决定卷积层的输出通道数）。以输入 $\boldsymbol{X}$ 为 $\boldsymbol{5×5}$ 的矩阵，卷积核为 $\boldsymbol{3×3}$ 的矩阵为例，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/ec9c330197fe4a93ac71a10041ce27f3.png#pic_center" alt="在这里插入图片描述"><br>计算过程如下：与卷积核同大小的感受野(输入 $\boldsymbol{X}$ 上方的绿色方框)首先移动至输入 $\boldsymbol{X}$ 最左上方，选中输入 $\boldsymbol{X}$ 上 $\boldsymbol{3×3}$ 的感受野元素，与卷积核(图片中间 $\boldsymbol{3×3}$ 方框)对应元素相乘：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
1 & -1 & 0\\
-1 & -2 & 2\\
1 & 2 & -2 \\
\end{bmatrix}⨀
\begin{bmatrix}
-1 & 1 & 2\\
1 & -1 & 3\\
0 & -1 & -2 \\
\end{bmatrix}=
\begin{bmatrix}
-1 & -1 & 0\\
-1 & 2 & 6\\
0 & -2 & 4 \\
\end{bmatrix}</script><p>⨀ 符号表示哈达马积(Hadamard Product)，即矩阵的对应元素相乘。运算后得到 $\boldsymbol{3×3}$ 的矩阵，这 9 个数值全部相加：</p>
<script type="math/tex; mode=display">
− 1−1 + 0−1 + 2 +6 + 0− 2 + 4=7</script><p>得到标量 7，写入输出矩阵第一行、第一列的位置。完成第一个感受野区域的特征提取后，感受野窗口向右移动一个步长单位(Strides，记 为𝑠，默认为 1)，选中下图中绿色方框中的 9 个感受野元素，按照同样的计算方法，与卷积核对应元素相乘累加，得到输出 10，写入第一行、第二列位置。</p>
<p><img src="https://img-blog.csdnimg.cn/8c855c50c2b54b0a9fd64aa02c76dd07.png#pic_center" alt="在这里插入图片描述"><br>感受野窗口再次向右移动一个步长单位，选中下图中绿色方框中的元素，并与卷积核相乘累加，得到输出 3，并写入输出的第一行、第三列位置。</p>
<p><img src="https://img-blog.csdnimg.cn/86b6e06a014e421cacd6b9e94abcb36a.png#pic_center" alt="在这里插入图片描述"><br>此时感受野已经移动至输入 $\boldsymbol{X}$ 的有效像素的最右边，无法向右边继续移动(在不填充无效元素的情况下），因此感受野窗口向下移动一个步长单位(𝑠 = 1)，并回到当前行的行首位置，继续选中新的感受野元素区域，与卷积核运算得到输出-1。此时的感受野由于经过向下移动一个步长单位，因此输出值 -1 写入第二行、第一列位置。</p>
<p><img src="https://img-blog.csdnimg.cn/51bbf50f32194180b073f7850c3ae09d.png#pic_center" alt="在这里插入图片描述"><br>以此类推得到最终的结果：</p>
<p><img src="https://img-blog.csdnimg.cn/7c1985e4181b409db125e6d05012e8cb.png#pic_center" alt="在这里插入图片描述"><br>最终输出我们得到一个 $\boldsymbol{3×3}$ 的矩阵，比输入 $\boldsymbol{5×5}$ 略小，这是因为感受野不能超出元素边界的缘故，由此我们也能知道，在没有扩展输入图像的情况下卷积，输出图像的维度一定会变小，我们将这种卷积方式称作 <strong>Vaild卷积</strong>。可以观察到，卷积运算的输出矩阵大小由卷积核的大小 $\boldsymbol{k}$ ，输入 $\boldsymbol{X}$ 的高宽，移动步长 $\boldsymbol{s}$，是否填充等因素共同决定。</p>
<h2 id="2-4-多通道输入和单卷积核"><a href="#2-4-多通道输入和单卷积核" class="headerlink" title="2.4    多通道输入和单卷积核"></a>2.4    多通道输入和单卷积核</h2><p>&emsp;&emsp;多通道输入的卷积层更为常见，比如彩色的图片包含了 <strong>R/G/B</strong> 三个通道，每个通道上<br>面的像素值表示 <strong>R/G/B</strong> 色彩的强度。下面我们以 3 通道输入、单个卷积核为例，将单通道输入的卷积运算方法推广到多通道的情况。在多通道输入的情况下，卷积核的通道数需要和输入 $\boldsymbol{X}$ 的通道数量相匹配，卷积核的第𝑖个通道和 $\boldsymbol{X}$ 的第 $\boldsymbol{i}$ 个通道运算，得到第 $\boldsymbol{i}$ 个中间矩阵，此时可以视为单通道输入与单卷积核的情况，所有通道的中间矩阵对应元素再次相加，作为最终输出。</p>
<p><img src="https://img-blog.csdnimg.cn/0c3558365d93465ba921c1dbc0a4f675.png#pic_center" alt="在这里插入图片描述"><br>上图中：每行的最左边 $\boldsymbol{5×5}$ 的矩阵表示输入 $\boldsymbol{X}$ 的 <strong>1~3</strong> 通道，第 <strong>2</strong> 列的 $\boldsymbol{3×3}$ 矩阵分别表示卷积核的 <strong>1~3</strong> 通道，第 <strong>3</strong> 列的矩阵表示当前通道上运算结果的中间矩阵，最右边一个矩阵表示卷积层运算的最终输出。在初始状态，每个通道上面的感受野窗口同步落在对应通道上面的最左边、最上方位置，每个通道上感受野区域元素与卷积核对应通道上面的矩阵相乘累加，分别得到三个通道上面的输出 7、-11、-1 的中间变量，这些中间<br>变量相加得到输出-5，写入对应位置。<br>&emsp;&emsp;随后，感受野窗口同步在 $\boldsymbol{X}$ 的每个通道上向右移动 $\boldsymbol{s = 1}$ 个步长单位，此时感受野区域元素如下图所示，每个通道上面的感受野与卷积核对应通道上面的矩阵相乘累加，得到中间变量 10、20、20，全部相加得到输出 50，写入第一行、第二列元素位置。</p>
<p><img src="https://img-blog.csdnimg.cn/75d121f60ebf471ba5aa22a45738b09a.png#pic_center" alt="在这里插入图片描述"><br>以此方式同步移动感受野窗口，直至最右边、最下方位置，此时全部完成输入和卷积核的卷积运算，得到 $\boldsymbol{3×3}$ 的输出矩阵。</p>
<p><img src="https://img-blog.csdnimg.cn/e329b80f1ddd4148ba61e7ef0d39c237.png#pic_center" alt="在这里插入图片描述"><br>输入的每个通道处的感受野均与卷积核的对应通道相乘累加，得到与通道数量相等的中间变量，这些中间变量全部相加即得到当前位置的输出值。输入通道的通道数量决定了卷积核的通道数。一个卷积核只能得到一个输出矩阵，无论输入 $\boldsymbol{X}$ 的通道数量。</p>
<p><img src="https://img-blog.csdnimg.cn/189c48be4e384941977d6f405be730cd.png#pic_center" alt="在这里插入图片描述"><br>一般来说，一个卷积核只能完成某种逻辑的特征提取，当需要同时提取多种逻辑特征时，可以通过增加多个卷积核来得到多种特征，提高神经网络的表达能力，这就是多通道输入、多卷积核的情况。</p>
<h2 id="2-5-多通道输入和多卷积核"><a href="#2-5-多通道输入和多卷积核" class="headerlink" title="2.5    多通道输入和多卷积核"></a>2.5    多通道输入和多卷积核</h2><p>&emsp;&emsp;多通道输入、多卷积核是卷积神经网络中最为常见的形式，前面已经介绍了单卷积核的运算过程，每个卷积核和输入 $\boldsymbol{X}$ 做卷积运算，得到一个输出矩阵。当出现多卷积核时，第 $\boldsymbol{i}$  ( $\boldsymbol{ 𝑖 ∈ 𝑛 }$，𝑛为卷积核个数)个卷积核与输入 $\boldsymbol{X}$ 运算得到第𝑖个输出矩阵(也称为输出张量 $\boldsymbol{O}$ 的通道 $\boldsymbol{i}$），最后全部的输出矩阵在通道维度上进行拼接(Stack 操作，创建输出通道数的新维度)，产生输出张量 $\boldsymbol{O}$ ，$\boldsymbol{O}$ 包含了 $\boldsymbol{n}$ 个通道数。<br>&emsp;&emsp;以 <strong>3</strong> 通道输入、<strong>2</strong> 个卷积核的卷积层为例。第一个卷积核与输入 $\boldsymbol{X}$ 运算得到输出 $\boldsymbol{O}$ 的第一个通道，第二个卷积核与输入𝑿运算得到输出𝑶的第二个通道，输出的两个通道拼接在一起形成了最终输出 $\boldsymbol{O}$ 。每个卷积核的大小 $\boldsymbol{k}$ 、步长 $\boldsymbol{s}$ 、填充设定等都是统一设置，这样才能保证输出的每个通道大小一致，从而满足拼接的条件。</p>
<p><img src="https://img-blog.csdnimg.cn/ffb728e215374554af5454a835e1b13b.png#pic_center" alt="在这里插入图片描述"><br>在CNN中，通常我们会使用大小为奇数的卷积核，因为进行卷积操作时一般会以卷积核模块的一个位置为基准进行滑动，这个基准通常就是卷积核模块的中心。若卷积核为奇数，卷积锚点很好找，自然就是卷积模块中心，但如果卷积核是偶数，这时候就没有办法确定了，让谁是锚点似乎都不怎么好。</p>
<h2 id="2-6-步长"><a href="#2-6-步长" class="headerlink" title="2.6    步长"></a>2.6    步长</h2><p>&emsp;&emsp;步长就是卷积核或过滤器在左边窗口中每次移动的格数 （无论是自左向右移动，或自上向下移动），对于 2D 输入来说，分为沿 $\boldsymbol{x}$ (向右)方向和 $\boldsymbol{y}$ (向下)方向的移动长度。下图中，绿色实线代表的卷积核的位置是当前位置，绿色虚线代表是上一次卷积核所在位置，从上一次位置移动到当前位置的移动长度即是步长的定义。下图中感受野沿 $\boldsymbol{x}$ 方向的步长为 2，表达为步长 $\boldsymbol{s = 2}$。</p>
<p><img src="https://img-blog.csdnimg.cn/7ed94b8d22814d00878f9102b9f827ec.png#pic_center" alt="在这里插入图片描述"><br>当感受野移动至输入 $\boldsymbol{X}$ 右边的边界时，感受野向下移动一个步长 $\boldsymbol{s = 2}$，并回到行首。如下图，感受野向下移动 2 个单位，并回到行首位置，进行相乘累加运算。</p>
<p><img src="https://img-blog.csdnimg.cn/d22dfa54f52e4ec2b220e3ffc3e10361.png" alt="在这里插入图片描述"></p>
<p>循环往复移动，直至达到最下方、最右边边缘位置。然后得到 $\boldsymbol{2×2}$ 的矩阵。</p>
<p><img src="https://img-blog.csdnimg.cn/37e2c77c7f164f8bac2da94939681f64.png" alt="在这里插入图片描述"></p>
<p>可以看到，通过设定步长𝑠，可以有效地控制信息密度的提取。当步长设计的较小时，卷积核以较小幅度移动窗口，有利于提取到更多的特征信息，输出张量的尺寸也更大；当步长设计的较大时，卷积核以较大幅度移动窗口，有利于减少计算代价，过滤冗余信息，输出张量的尺寸也更小。<br>&emsp;&emsp;同时，在卷积核移动过程中，其值始终是不变的，都是卷积核的值。也可以说，卷积核的值在整个过程中都是共享的，所以又把卷积核的值称为共享变 量。卷积神经网络采用参数共享的方法大大降低了参数的数量。在下图中，卷积核如果继续往右移动2格，卷积核窗口部分将在输入矩 阵之外，这种情况该如何处理？具体处理方法就涉及下面的内容——填充（Padding）。</p>
<p><img src="https://img-blog.csdnimg.cn/7c993edb7281400285b2bb80f9204a4a.png" alt="在这里插入图片描述"></p>
<h2 id="2-7-填充"><a href="#2-7-填充" class="headerlink" title="2.7    填充"></a>2.7    填充</h2><p>&emsp;&emsp;在前面我们提过，如果不对图片进行拓展的话，经过卷积运算后的输出 $\boldsymbol{O}$ 的高宽会小于输入 $\boldsymbol{X}$ 的高宽；且当卷积核的步长过大时，卷积核可能会超过图片边界。在实际的网络模型设计时，我们通常希望输出 $\boldsymbol{O}$ 的高宽能够与输入 $\boldsymbol{X}$ 的高宽相同，从而方便网络参数的设计、残差连接等。为了让输出  $\boldsymbol{O}$ 的高宽能够与输入  $\boldsymbol{X}$  的相等，一般通过在原输入  $\boldsymbol{X}$  的高和宽维度上面进行填充(padding)若干无效元素操作，得到增大的输入 $\boldsymbol{X’}$ 。通过精心设计填充单元的数量，在 $\boldsymbol{X’}$ 上面进行卷积运算得到输出 $\boldsymbol{O}$ 的高宽可以和原输入  $\boldsymbol{X}$  相等。这种输入与输出的特征图维度一样的卷积方式，我们称作 <strong>Same卷积</strong>，不填充称作 <strong>Vaild卷积</strong>。<br>&emsp;&emsp;如下图，在高/行方向的上(Top)、下(Bottom)方向，宽/列方向的左(Left)、 右(Right)均可以进行不定数量的填充操作，填充的数值一般默认为 0，也可以填充自定义的数据。上、下方向各填充 1 行，左、右方向各填充 2 列，得到新的输入 $\boldsymbol{X’}$。</p>
<p><img src="https://img-blog.csdnimg.cn/23c1e632e73b4dab974fd86844bd333f.png" alt="在这里插入图片描述"></p>
<p>那么添加填充后的卷积层运算同样，仅仅是把参与运算的输入从  $\boldsymbol{X}$  换成了填充后得到的新张量  $\boldsymbol{X’}$ 。如下图，感受野的初始位置在填充后的  $\boldsymbol{X’}$  的左上方，完成相乘累加运算，得到输出 1，写入输出张量的对应位置。</p>
<p><img src="https://img-blog.csdnimg.cn/e0b0df7456de4ef9a332b5ea981101aa.png" alt="在这里插入图片描述"></p>
<p>循环往复，最终得到 $\boldsymbol{5 × 5}$ 的输出张量。</p>
<p><img src="https://img-blog.csdnimg.cn/eea96add1bf5496489c757891b148bc3.png" alt="在这里插入图片描述"></p>
<h2 id="2-8-卷积的计算公式"><a href="#2-8-卷积的计算公式" class="headerlink" title="2.8    卷积的计算公式"></a>2.8    卷积的计算公式</h2><p>&emsp;&emsp;经过上述对输入图像、卷积核、步长、填充、输出图像的分析，使用张量将它们表示如下：</p>
<ul>
<li>输入图片的尺寸：一般用  $\boldsymbol{n × n}$  表示输入的图像大小；</li>
<li>卷积核的大小：一般用  $\boldsymbol{f × f}$  表示卷积核的大小；</li>
<li>填充（padding）：一般用  $\boldsymbol{p}$  来表示填充大小；</li>
<li>步长(stride)：一般用  $\boldsymbol{s}$  来表示步长大小；</li>
<li>输出图片的尺寸：一般用  $\boldsymbol{o}$  来表示。</li>
</ul>
<p>如果已知 $\boldsymbol{n、f、s}$，则  $\boldsymbol{p = \frac{f - 1}{2}}$，$\boldsymbol{o = \lfloor \frac{n + 2p - f}{s} + 1\rfloor}$。最后再来分析以下卷积核在 <a href="##2.5多通道输入和多卷积核">2.5    多通道输入和多卷积核</a> 的末尾我分析过在实际中卷积核的大小 $\boldsymbol{f}$ 常常使用奇数是为了找到一个好锚点。实际上选择奇数的卷积核也更容易（padding）。当我们使用 <strong>Same卷积</strong> 时，这时候我们就需要用到 padding。但是如果  $\boldsymbol{f}$ 是偶数的话，  $\boldsymbol{p = \frac{f - 1}{2}}$就不是整数了。这意味着我们只需要在输入图像的一边或者两边进行填充。<br>&emsp;&emsp;(这里可选看)在 <strong>PyTorch</strong> 中，假设 <strong>Input</strong> 维度为  $\boldsymbol{(N,C_{in},H_{in}, W_{in})}$，<strong>Output</strong> 维度为  $\boldsymbol{(N,C_{out},H_{out}, W_{out})}$，两维度关系如下：</p>
<ul>
<li>$\boldsymbol{H_{out} = \frac{H_{in} + 2 × padding[0] - dilation[0] ×(kernel\_size[0] - 1) - 1}{stride[0]} + 1}$</li>
<li>$\boldsymbol{W_{out} = \frac{W_{in} + 2 × padding[1] - dilation[1] ×(kernel\_size[1] - 1) - 1}{stride[1]} + 1}$</li>
</ul>
<h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3    总结"></a>3    总结</h1><p>&emsp;&emsp;在本篇博客中，我跟大家分享了卷积神经网络中的基础结构——卷积层，并依次介绍了权值共享、卷积操作、卷积核，填充等内容。卷积神经网络的基础知识有很多，本篇博客介绍不完，剩下的我放到下一篇博客。期待您的访问！感恩！</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络（三）</title>
    <url>/2022/06/05/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&emsp;&emsp;卷积神经网络发展非常迅速，应用非常广阔，所以近几年的卷积神经网络得到了长足的发展，下图为卷积神经网络近几年发展的大致轨迹。</p>
<p><img src="https://img-blog.csdnimg.cn/ef0e1297e2104ce699fb786232ec8928.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;1998年LeCun提出了 <strong>LeNet</strong>，可谓是开山鼻祖，系统地提出了卷积层、 池化层、全连接层等概念。2012年Alex等提出 <strong>AlexNet</strong>，提出 一些训练深度网络的重要方法或技巧，如 Dropout、ReLu、GPU、数据增强方法等，随后各种各样的深度卷积神经网络模型相继被提出，其中比较有代表性的有 <strong>VGG</strong> 系列，<strong>GoogLeNet</strong> 系列，<strong>ResNet</strong> 系列，<strong>DenseNet</strong> 系列等，他们的网络层数整体趋势逐渐增多。以网络模型在 ILSVRC 挑战赛 ImageNet数据集上面的分类性能表现为例，如下图，在 AlexNet 出现之前的网络模型都是浅层的神经网络，<strong>Top-5</strong>（表示神经网络返回的前5个最大概率值代表的内容中有一个是正确的）错误率均在 25%以上，AlexNet 8 层的深层神经网络将 Top-5 错误率降低至 16.4%，性能提升巨大，后续的 VGG、GoogleNet 模型继续将错误率降低至 6.7%；ResNet 的出现首次将网络层数提升至 152 层，错误率也降低至 3.57%。<br><img src="https://img-blog.csdnimg.cn/cdf688521db043d18d72e0a6f063ecb8.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h1><p>&emsp;&emsp;LeNet 是Yann LeCun等人提出的卷积神经网络结构，用于解决手写数字识别的机器视觉任务。1989年。一般来说，LeNet 是指 LeNet-5，是一个简单的卷积神经网络。卷积神经网络是一种前馈神经网络，其人工神经元可以对覆盖范围内的一部分周围细胞做出反应，在大规模图像处理中表现良好。LeNet 作为早期卷积神经网络的代表，拥有卷积神经网络的基本单元，如卷积层、池化层和全连接层，为卷积神经网络的未来发展奠定了基础。<br>&emsp;&emsp;模型架构：LeNet-5 模型结构为 <strong>输入层-卷积层-池化层-卷积层-池化层-全连接层-全 连接层-输出</strong>，为串联模式。</p>
<p><img src="https://img-blog.csdnimg.cn/d372e7fcd6ce40adbcee7ada2e3251e7.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型特点：</p>
<ul>
<li>每个卷积层包括三个部分：卷积、池化和非线性激活函数；</li>
<li>使用卷积提取空间特征；</li>
<li>采用降采样的平均池化层；</li>
<li>使用 <strong>tanh</strong> 激活函数；</li>
<li>使用 <strong>MLP</strong> 作为最后一个分类器；</li>
<li>层间稀疏连接，降低计算复杂度。</li>
</ul>
<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>&emsp;&emsp;2012 年，ILSVRC12 挑战赛 ImageNet 数据集分类任务的冠军 Alex Krizhevsky 提出了 8<br>层的深度神经网络模型 AlexNet。AlexNet 在 ImageNet 取得了 15.3% 的 Top-5 错误率，比第二名在错误率上降低了 10.9%。原始论文的主要结果是模型的深度对其高性能至关重要，这在计算上是昂贵的，但由于在训练过程中使用了图形处理单元(GPU) 而变得可行。<br>&emsp;&emsp;模型架构：接收输入为 <strong>224 × 224</strong> 大小的彩色图片数据，经过五个卷积层和三个全连接层后得到样本属于 <strong>1000</strong> 个类别的概率分布。为了降低特征图的维度，AlexNet 在第 1、2、5 个卷积层后添加了 <strong>Max Pooling</strong> 层，网络的参数量达到了 6000 万个。为了能够在当时的显卡设备 NVIDIA GTX 580(3GB 显存)上训练模型，Alex Krizhevsky 将卷积层、前 2 个全连接层等拆开在两块显卡上面分别训练，最后一层合并到一张显卡上面，进行反向传播更新。</p>
<p><img src="https://img-blog.csdnimg.cn/db67045e9c10452f914803ee8f9d4e3a.png#pic_center" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/de86dbe37b284a799a91dcc191f422b3.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型特点：</p>
<ul>
<li>由 5 层卷积和 3 层全连接组成，输入图像为 3 通道 224×224 大小，网络规 模远大于 LeNet；</li>
<li>采用了 <strong>ReLU</strong> 激活函数，过去的神经网络大多采用 <strong>Sigmoid</strong> 激活函数，计算相对复杂，容易出现梯度弥散现象。</li>
<li>引入 <strong>Dropout</strong> 层。<strong>Dropout</strong> 提高了模型的泛化能力，防止过拟合，提升模型的鲁棒性。</li>
<li>具备一些很好的训练技巧，包括数据增广、学习率策略、<strong>Weight Decay</strong> 等。</li>
</ul>
<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><p>&emsp;&emsp;AlexNet 模型的优越性能启发了业界朝着更深层的网络模型方向研究。2014 年，ILSVRC14 挑战赛 ImageNet 分类任务的亚军牛津大学 VGG 实验室提出了 VGG11、VGG13、VGG16、VGG19 等一系列的网络模型，如下图。VGG可以看成是加深版本的AlexNet，都是 <strong>Conv Layer + FC layer</strong>，VGG16 在 ImageNet 取得了 7.4%的 Top-5 错误率，比 AlexNet 在错误率上降低了 7.9%。</p>
<p><img src="https://img-blog.csdnimg.cn/30942835ff1d4171ad52a34c2f3c6f0b.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型架构：以 VGG16 为例，它接受 <strong>224 × 224</strong> 大小的彩色图片数据，经过 2 个 <strong>Conv-Conv-Pooling</strong> 单元，和 3 个 <strong>Conv-Conv-Conv-Pooling</strong> 单元的堆叠，最后通过 3 层全连接层输出当前图片分别属于 1000 类别的概率分布。</p>
<p><img src="https://img-blog.csdnimg.cn/fd59378f38094452b40210e44f6090fb.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型特点：</p>
<ul>
<li>更深的网络结构：网络层数由 AlexNet 的 8 层增至 16 和 19 层，更深的网络意味着更强大的网络能力，也意味着需要更强大的计算力，不过后来硬件发展也很快，显卡运算力也在快速增长，以此助推深度学习的快速发展。</li>
<li>全部采用更小的 <strong>3 × 3</strong> 卷积核，相对于 AlexNet 中 <strong>7 × 7</strong> 的卷积核，参数量更少，计算代价更低。</li>
<li>采用更小的池化层 <strong>2 × 2</strong> 窗口和步长 $\boldsymbol{s = 2}$，而 AlexNet 中是步长 $\boldsymbol{𝑠 = 2、3 × 3}$ 的池化窗口。</li>
</ul>
<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><p>&emsp;&emsp;在介绍 GoogleNet 之前，我们需要对卷积核进行讨论。再前面说过 VGG 模型使用的卷积核大小均是 <strong>3 × 3</strong>，参数量更少，计算代价更低，同时因为两个 <strong>3 × 3</strong> 卷积核的感受野相当于一个 <strong>5 × 5</strong> 卷积核，能捕获图像更多的细节信息，因此 <strong>3 × 3</strong> 卷积核在性能表现上更优越。因此业界开始探索卷积核最小的情况：<strong>1 × 1</strong> 卷积核。</p>
<p><img src="https://img-blog.csdnimg.cn/123e773fcfc34da79386e563a60b6175.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，输入为 3 通道的 <strong>5 × 5</strong> 图片，与单个 <strong>1 × 1</strong> 的卷积核进行卷积运算，每个通道的数据与对应通道的卷积核运算，得到 3 个通道的中间矩阵，对应位置相加得到最终的输出张量。对于输入 <strong>shape</strong> 为 $\boldsymbol{[b, h,w,c_{in}]}$，<strong>1 × 1</strong> 卷积层的输出为 $\boldsymbol{[b, h,w,c_{out}]}$，其中 $\boldsymbol{c_{in}}$ 为输入数据的通道数，$\boldsymbol{c_{out}}$ 为输出数据的通道数，也是 <strong>1 × 1</strong> 卷积核的数量。 <strong>1 × 1</strong> 卷积核的一个特别之处在于，它可以不改变特征图的宽高，而只对通道数 $\boldsymbol{c}$ 进行变换。这起到了降维的作用，因此 <strong>1 × 1</strong> 卷积核可以帮助我们降低参数数量。<br>&emsp;&emsp;2014 年，ILSVRC14 挑战赛的冠军 Google 提出了大量采用 <strong>3 × 3</strong> 和<strong>1 × 1</strong> 卷积核的网络模型：GoogLeNet，网络层数达到了 22 层。虽然 GoogLeNet 的层数远大于 AlexNet，但是它的参数量却只有 AlexNet 的 $\boldsymbol{\frac{1}{12}}$，同时性能也远好于 AlexNet。在 ImageNet 数据集分类任务上，GoogLeNet 取得了 6.7%的 Top-5 错误率，比 VGG16 在错误率上降低了 0.7%。<br>&emsp;&emsp;模型架构：VGG 是增加网络的深度，但深度达到一个程度时，可能就成为瓶颈。 GoogLeNet 则从另一个维度来增加网络能力，每单元有许多层并行计算，让网络更宽了。GoogLeNet 网络通过大量堆叠 <strong>Inception</strong> 模块，形成了复杂的网络结构，如下图。</p>
<p><img src="https://img-blog.csdnimg.cn/d1021c4e19ea455185e943b1d157feaf.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，<strong>Inception</strong> 模块的输入为 $\boldsymbol{X}$，通过 4 个子网络得到 4 个网络输出，在通道轴上面进行拼接合并，形成 <strong>Inception</strong> 模块的输出。这 4 个子网络为：</p>
<ul>
<li><strong>1 × 1</strong> 卷积层；</li>
<li><strong>1 × 1</strong> 卷积层，再通过一个 <strong>3 × 3</strong> 卷积层；</li>
<li><strong>1 × 1</strong> 卷积层，再通过一个 <strong>5 × 5</strong> 卷积层；</li>
<li><strong>3 × 3</strong> 最大池化层，再通过 <strong>1 × 1</strong> 卷积层。</li>
</ul>
<p>GoogLeNet 的网络结构如下图所示，其中红色框中的网络结构即为 <strong>Inception</strong> 模块的网络结构。</p>
<p><img src="https://img-blog.csdnimg.cn/4e8d066c01e14eff8dad7b5d83fed171.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型特点：</p>
<ul>
<li>引入 <strong>Inception</strong> 结构，这是一种网中网(Network In Network)的结构。通过网络的水平排布，可以用较浅的网络得到较好的模型能力，并进行多特征融合，同时更容易训练。使用了 <strong>1 × 1</strong> 卷积来先对特征通道进行降维，减少计算量。GoogLeNet 就是一个精心设计的性能良好的 <strong>Inception</strong> 网络(Inception v1)的 实例，即 GoogLeNet 是 <strong>Inception v1</strong> 网络的一种。</li>
<li>采用全局平均池化层。将后面的全连接层全部替换为简单的全局平均池化，在最后参数会变得更少。而在 AlexNet 中最后 3 层的全连接层参数差不多占总参数的 90%，使用大网络在宽度和深度上允许 GoogleNet 移除全连接层，但并不会影响到结果的精度。</li>
</ul>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>&emsp;&emsp;AlexNet、VGG、GoogLeNet 等网络模型的出现将神经网络的发展带入了几十层的阶段，研究人员发现网络的层数越深，越有可能获得更好的泛化能力。但是当模型加深以后，网络变得越来越难训练，这主要是由于梯度弥散和梯度爆炸现象造成的。在较深层数的神经网络中，梯度信息由网络的末层逐层传向网络的首层时，传递的过程中会出现梯度接近于 0 或梯度值非常大的现象。网络层数越深，这种现象可能会越严重。对于深层神经网络的梯度弥散和梯度爆炸现象，我们可以想到浅层神经网络不容易出现这些梯度现象，那么可以尝试给深层神经网络添加一种回退到浅层神经网络的机制。当深层神经网络可以轻松地回退到浅层神经网络时，深层神经网络可以获得与浅层神经网络相当的模型性能，而不至于更糟糕。<br>&emsp;&emsp;2015 年，微软亚洲研究院何凯明等人发表了深度残差网络(Residual Neural Network，简称 ResNet)算法 [10]，并提出了 18 层、34 层、50 层、101层、152 层的 ResNet-18、ResNet-34、ResNet-50、ResNet-101 和 ResNet-152 等模型。ResNet 在网络结构上做了一大创新，即采用残差网络结构，而不再是简单地堆积层数，ResNet 在卷积神经网络中提供了一个新思路。ResNet 在 ILSVRC 2015 挑战赛 ImageNet数据集上的分类、检测等任务上面均获得了最好性能。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>&emsp;&emsp;ResNet 通过在卷积层的输入和输出之间添加残差链接实现层数回退机制。</p>
<p><img src="https://img-blog.csdnimg.cn/ae2a26c169914e2db0b84dfdf01b84f5.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，输入 $\boldsymbol{x}$ 通过两个卷积层，得到特征变换后的输出 $\boldsymbol{F(𝒙)}$，与输入 $\boldsymbol{x}$ 进行对应元素的相加运算，得到最终输出 $\boldsymbol{H(x)}$：</p>
<script type="math/tex; mode=display">
\boldsymbol{H(x) = F(x) + x}</script><p> $\boldsymbol{H(x)}$ 叫作残差模块(Residual Block，简称 ResBlock)，由于卷积神经网络需要学习映射 $\boldsymbol{F(x) = H(x) - x}$，故称为残差网络。为了能够满足输入 $\boldsymbol{x}$ 与卷积层的输出出 $\boldsymbol{F(𝒙)}$ 能够相加运算，需要输入 $\boldsymbol{x}$ 的 <strong>shape</strong> 与 $\boldsymbol{F(𝒙)}$ 的完全一致。当出现 <strong>shape</strong> 不一致时，一般通过在残差连接上添加额外的卷积运算环节将输入 $\boldsymbol{x}$ 变换到与 $\boldsymbol{F(𝒙)}$ 相同的 <strong>shape</strong>，如上图中 $\boldsymbol{dentity(𝒙)}$ 函数。因为再卷积过程中我们常常使用 <strong>Vaild卷积</strong>，因此 $\boldsymbol{dentity(𝒙)}$ 以<strong>1 × 1</strong>的卷积运算居多，主要用于调整输入的通道数。<br> &emsp;&emsp;模型架构：34 层的深度残差网络、34 层的普通深度网络以及 19 层的 VGG 网络结构如下图。可以看到，深度残差网络通过堆叠残差模块，达到了较深的网络层数，从而获得了训练稳定、性能优越的深层网络模型。</p>
<p> <img src="https://img-blog.csdnimg.cn/6fffa970f5cb4bc397eab122728c3184.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型特点：</p>
<ul>
<li>层数非常深，已经超过百层；</li>
<li>引入残差单元来解决退化问题。</li>
</ul>
<h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h1><p>&emsp;&emsp;DenseNet 将前面所有层的特征图信息通过 <strong>Skip Connection</strong> 与当前层输出进行聚合，与 ResNet 的对应位置相加方式不同，DenseNet 采用在通道轴 $\boldsymbol{c}$ 维度进行拼接操作，聚合特征信息。</p>
<p><img src="https://img-blog.csdnimg.cn/903767be083a4ecf9e2f7118040059ed.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，输入 $\boldsymbol{X}$ 通过 $\boldsymbol{H_1}$ 卷积层得到输出 $\boldsymbol{X_1}$，$\boldsymbol{X_1}$ 与 $\boldsymbol{X}$ 在通道轴上进行拼接，得到聚合后的特征张量，送入 $\boldsymbol{H_2}$ 卷积层，得到输出 $\boldsymbol{X_2}$，同样的方法，$\boldsymbol{X_2}$ 与前面所有层的特征信息 $\boldsymbol{X_1}$ 与 $\boldsymbol{X}$ 进行聚合，再送入下一层。如此循环，直至最后一层的输出 $\boldsymbol{X_4}$ 和前面所有层的特征信息：$\boldsymbol{\{X_i\}_{i = 0, 1, 2 ,3}}$进行聚合得到模块的最终输出。<br>&emsp;&emsp;模型架构：DenseNet 通过堆叠多个 Dense Block 构成复杂的深层神经网络。</p>
<p><img src="https://img-blog.csdnimg.cn/982bee45930b4c818752a0fb98eb368d.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;模型特点：</p>
<ul>
<li>引入来稠密连接模块解决退化问题。</li>
</ul>
<p>不同版本的 DenseNet 的性能、DenseNet 与 ResNet 的性能比较，以及DenseNet 与 ResNet 训练曲线比较如下：<br><img src="https://img-blog.csdnimg.cn/88f8963b05a14dc19eb03bf1e6b1a517.png#pic_center" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积神经网络（二）</title>
    <url>/2022/06/05/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-池化层"><a href="#1-池化层" class="headerlink" title="1    池化层"></a>1    池化层</h1><p>&emsp;&emsp;在上篇博客中，有跟大家分析过，在卷积层中没有 <strong>padding</strong> 的情况下，可以通过调节步长参数 $\boldsymbol{s}$ 实现特征图的高宽成倍缩小，从而降低了网络的参数量。但是在实际上我们通常使用 <strong>Same卷积</strong>，即输入和输出特征图的维度一样，这样一来将面临巨大的计算量挑战，而且容易产生过拟合的现象，因此我们需要一种专门的网络层可以实现尺寸缩减功能，它就是这里要介绍的 <strong>池化层</strong>(pooling layer)，通常，池化操作也被称作 <strong>下采样</strong>。<br>&emsp;&emsp;池化层同样基于局部相关性的思想，通过从局部相关的一组元素中进行采样或信息聚合，从而得到新的元素值。下面介绍两种池化方式：</p>
<ul>
<li>最大池化（Max Pooling）：选择 <strong>pooling</strong> 窗口中的最大值作为采样值；</li>
<li>均值池化（Mean Pooling）：将 <strong>pooling</strong> 窗口中的所有值相加取平均， 以平均值作为采样值。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/380e2764641f4fd3a00e461b67f04906.png#pic_center" alt="在这里插入图片描述"></p>
<p>不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似 <strong>不变（invariant）</strong>，对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。池化操作就是图像的 <strong>resize</strong>，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。说明池化能够提升模型的尺度不变性、旋转不变性。<br>&emsp;&emsp;以 $\boldsymbol{5 × 5}$ 输入 $\boldsymbol{X}$ 的最大池化层为例，考虑池化窗口大小 $\boldsymbol{ k =2}$ ，步长 $\boldsymbol{s = 2}$ 的情况。</p>
<p><img src="https://img-blog.csdnimg.cn/4a74011efe814fa7935270648d7da830.png#pic_center" alt="在这里插入图片描述"></p>
<p>绿色虚线方框代表第一次池化窗口的位置，感受野元素集合为：</p>
<script type="math/tex; mode=display">
\boldsymbol{\{ 1, -1, -1,-2\}}</script><p>在最大池化采样的方法下，通过:</p>
<script type="math/tex; mode=display">\boldsymbol{x' = }
\boldsymbol{\ max(\{ 1, -1, -1,-2\}) = 1}</script><p>计算出当前位置的输出值为 1，并写入对应位置。若采用的是平均池化操作，则此时的输出值应为:</p>
<script type="math/tex; mode=display">\boldsymbol{x' = }
\boldsymbol{\ avg(\{ 1, -1, -1,-2\}) = -0.75}</script><p>计算完当前位置的池化窗口后，与卷积层的计算步骤类似，将池化窗口按着步长向右移动若干单位，到达上图中就是实现绿色窗口所在位置此时的输出为：</p>
<script type="math/tex; mode=display">\boldsymbol{x' = }
\boldsymbol{\ max(\{ -1, 0, -2,2\}) = 2}</script><p>同样的方法，逐渐移动感受野窗口至最右边，计算出输出 $\boldsymbol{x’ = }<br>\boldsymbol{\ max(\{ 2,0, 3,1\}) = 3}$ 此时窗口已经到达输入边缘，按照卷积层同样的方式，感受野窗口向下移动一个步长，并回到行首。</p>
<p><img src="https://img-blog.csdnimg.cn/a9a0ebacdf0e4afe9c5e0c74ea8ae18a.png#pic_center" alt="在这里插入图片描述"></p>
<p>循环往复，直至最下方、最右边，获得最大池化层的输出，长宽为 $\boldsymbol{4 × 4}$，小于输入 $\boldsymbol{X}$ 的高宽。</p>
<p><img src="https://img-blog.csdnimg.cn/1214352efa374c48b5a3b7ee041a02db.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;池化层没有需要学习的参数，计算简单，在 <strong>CNN</strong> 中可用来减小尺寸，提高运算速度及减小噪声影响，让各特征更具有健壮性。池化层比卷积层更简单，它没有卷积运算，只是在滤 波器算子滑动区域内取最大值或平均值。而池化的作用则体现在降采样：保留显著特征、降低特征维度，增大感受野。深度网络越往后面越能捕捉到物体的语义信息，这种语义信息是建立在较大的感受野基础上。<br>&emsp;&emsp;通过设计池化层感受野的高宽𝑘和步长 $\boldsymbol{s}$ 参数，可以实现各种降维运算。比如，一种常用的池化层设定是池化窗口大小 $\boldsymbol{k = 2}$，步长 $\boldsymbol{s = 2}$，这样可以实现输出只有输入高宽一半的目的。如下图中，池化窗口 $\boldsymbol{k = 3}$，步长 $\boldsymbol{s = 2}$，输入 $\boldsymbol{X}$ 高宽为 $\boldsymbol{5×5}$，输出 $\boldsymbol{O}$ 高宽只有 $\boldsymbol{2×2}$。</p>
<p><img src="https://img-blog.csdnimg.cn/e28168ec964a4af7a4aa4a31946ef60e.png#pic_center" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/1796eb7358864e6da9e533da5651bdd2.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="2-BatchNorm层"><a href="#2-BatchNorm层" class="headerlink" title="2    BatchNorm层"></a>2    BatchNorm层</h1><p>&emsp;&emsp;卷积神经网络的出现，网络参数量大大减低，使得几十层的深层网络成为可能。然而，在残差网络出现之前，网络的加深使得网络训练变得非常不稳定，甚至出现网络长时间不更新甚至不收敛的现象，同时网络对超参数比较敏感，超参数的微量扰动也会导致网络的训练轨迹完全改变。<br>&emsp;&emsp;2015 年，Google 研究人员 Sergey Ioffe 等提出了一种参数标准化(Normalize)的手段，并基于参数标准化设计了 <strong>Batch Nomalization</strong>（简写为 BatchNorm，或 BN）层 。BN 层的提出，使得网络的超参数的设定更加自由，比如更大的学习率、更随意的网络初始化等，同时网络的收敛速度更快，性能也更好。BN 层提出后便广泛地应用在各种深度网络模型上，卷积层、BN 层、ReLU 层、池化层一度成为网络模型的标配单元块，通过堆叠 <strong>Conv-BN-ReLU-Pooling</strong> 方式往往可以获得不错的模型性能。总结 <strong>BN</strong> 层的作用如下三点，在后面过程中会具体怎么实现中这些作用的。</p>
<ul>
<li>加快网络的训练和收敛的速度；</li>
<li>控制梯度爆炸防止梯度消失；</li>
<li>防止过拟合。</li>
<li>&emsp;&emsp;为什么需要对网络中的数据进行标准化操作？我们可以先来看看虑 <strong>Sigmoid</strong> 激活函数和它的梯度分布。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/9bb131337fc14d57966a05c710434b2d.png#pic_center" alt="在这里插入图片描述"><br>由上图知，<strong>Sigmoid</strong> 函数在 $\boldsymbol{𝑥 ∈ [−2 ,2]}$ 区间的导数值在 $\boldsymbol{[0.1,0.25]}$ 区间分布；$\boldsymbol{𝑥 &gt; 2}$ 或 $\boldsymbol{𝑥 &lt;-2}$ 时，<strong>Sigmoid</strong> 函数的导数变得很小，逼近于 0，在反向传播中从而容易出现梯度弥散现象。为了避免因为输入较大或者较小而导致 <strong>Sigmoid</strong> 函数出现梯度弥散现象。如假设网络中每层的学习梯度都小于最大值 $\boldsymbol{0.25}$，网络中有 $\boldsymbol{n}$ 层，因为链式求导的原因，第一层的梯度将会小于 $\boldsymbol{0.25}$ 的 $\boldsymbol{n}$ 次方，所以学习速率相对来说会变的很慢，而对于网络的最后一层只需要对自身求导一次，梯度就大，学习速率就会比较快，这就会造成在一个很深的网络中，浅层基本不学习，权值变化小，而后面几层网络一直学习，后面的网络基本可以表征整个网络，这样失去了深度的意义。因此将函数输入 $\boldsymbol{𝑥 }$ 标准化映射到 0 附近的一段较小区间将变得非常重要，上图中，通过标准化重映射后，值被映射在 0 附近，此处的导数值不至于过小，从而不容易出现梯度弥散现象。同时，假如激活层斜率均为最大值 $\boldsymbol{0.25}$，所有层的权值为 100，这样梯度就会指数增加，但是如果进行了归一化之后，权值就不会很大，梯度爆炸的情况就会减少。<br>&emsp;&emsp;再来分析以下只有 2 个输入节点的线性模型：$\boldsymbol{L = a= x_1w_1 + x_2w_2+b}$：</p>
<p><img src="https://img-blog.csdnimg.cn/cacb8e53f8234918ae1ea7de8ce5de9c.png#pic_center" alt="在这里插入图片描述"></p>
<p> 由于模型相对简单，可以绘制出 2 种 $\boldsymbol{x_1,x_2}$ 下的函数的损失等高线图：</p>
<p> <img src="https://img-blog.csdnimg.cn/6142e3ec2c9041cba9075a4de2319ef7.png#pic_center" alt="在这里插入图片描述"></p>
<p>其中，左边的损失等高线图是 $\boldsymbol{x_1∈[1,10],x_2∈[100,1000]}$ 时的某条优化轨迹线示意图，右边的损失等高线图是 $\boldsymbol{x_1∈[1,10],x_2∈[1,10]}$ 时的某条优化轨迹线示意图，图中的圆环中心即为全局极值点。当 $\boldsymbol{x_1,x_2}$ 输入分布相近时， $\boldsymbol{\frac{\partial L}{w_1},\frac{\partial L}{w_2}}$ 偏导数值相当，收敛更加快速，优化轨迹更理想如上图中的右图；当 $\boldsymbol{x_1,x_2}$ 输入分布差距较大时，比如 $\boldsymbol{x_1≪x_2}$，则$\boldsymbol{\frac{\partial L}{w_1}≪\frac{\partial L}{w_2}}$，损失函数等势线在$\boldsymbol{x_2}$ 轴更加陡峭，某条可能的优化轨迹如上图中的左图。<br>&emsp;&emsp;通过上述的 2 个例子，能够知道网络层输入 $\boldsymbol{x}$ 分布相近，并且分布在较小范围内时(如 0 附近)，更有利于函数的优化。我们可以通过数据标准化操作达到这个目的，通过数据标准化操作可以将数据 $\boldsymbol{x}$ 映射到 $\boldsymbol{\hat{x}}$：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat{x}}= \boldsymbol{\frac{x - \mu_r}{\sqrt{\sigma_r^2+\epsilon}}}</script><p>其中 $\boldsymbol{\mu_r、\sigma_r^2}$ 来自统计的所有数据的均值和方差， $\boldsymbol{\epsilon}$ 是为防止出现除 0 错误而设置的较小数，如 <strong>1e − 8</strong>。在基于 <strong>Batch</strong> 的训练阶段，考虑 <strong>Batch</strong> 内部的均值 $\boldsymbol{\mu_B}$ 和方差 $\boldsymbol{\sigma_B^2}$：</p>
<script type="math/tex; mode=display">
 \boldsymbol{\mu_B}  =  \boldsymbol{\frac{1}{m}\sum_{i = 1}^{m}x_i} \\
\boldsymbol{\sigma_B^2} = \boldsymbol{\frac{1}{m}\sum_{i = 1}^{m}(x_i - \mu_B)^2}</script><p>其中 <strong>m</strong> 为 <strong>Batch</strong> 样本数，而且因为每次网络都是随机取 <strong>Batch</strong>，这样就会使得整个网络不会朝这一个方向使劲学习。一定程度上避免了过拟合。。因此，在训练阶段，通过：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat{x}_{train}}= \boldsymbol{\frac{x_{train} - \mu_B}{\sqrt{\sigma_B^2+\epsilon}}}</script><p>标准化输入，并记录每个 <strong>Batch</strong> 的统计数据 $\boldsymbol{\mu_B、\sigma_B^2}$，用于统计真实的全局 $\boldsymbol{\mu_r、\sigma_r^2}$。在测试阶段，根据记录的每个 <strong>Batch</strong> 的 $\boldsymbol{\mu_B、\sigma_B^2}$ 估计出的所有训练数据的 $\boldsymbol{\mu_r、\sigma_r^2}$，依据：</p>
<script type="math/tex; mode=display">
\boldsymbol{\hat{x}_{test}}= \boldsymbol{\frac{x_{test} - \mu_r}{\sqrt{\sigma_r^2+\epsilon}}}</script><p>将每层的输入标准化。上述的标准化运算并没有引入额外的待优化变量，各个均值和方差均由统计得到，不需要参与梯度更新。但实际上，为了提高 <strong>BN</strong> 层的表达能力，<strong>BN</strong> 层作者引入了 <strong>“scale and shift”</strong> 技巧，将 $\boldsymbol{\hat{x}}$ 变量再次映射变换：</p>
<script type="math/tex; mode=display">
\boldsymbol{\tilde{x} = \hat{x} \cdot\gamma + \beta}</script><p>其中 $\boldsymbol{\gamma}$ 参数实现对标准化后的 $\boldsymbol{\hat{x}}$ 再次进行缩放，$\boldsymbol{\beta}$ 参数实现对标准化的 $\boldsymbol{\hat{x}}$ 进行平移，不同的是，$\boldsymbol{\gamma、\beta}$ 参数均由反向传播算法自动优化即学习参数，实现网络层“按需”缩放平移数据的分布的目的。综上，<strong>BN</strong> 层的流程如下：</p>
<p><img src="https://img-blog.csdnimg.cn/266261b51886466fb14fe20d4ff3d80c.png#pic_center" alt="在这里插入图片描述"></p>
<p>原文的算法流程如下：</p>
<p><img src="https://img-blog.csdnimg.cn/13b202795d7344d28cc5aed5c684c38a.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;最后来分析一下 <strong>BN</strong> 层中的参数共享问题，我在上一篇博客中跟大家分享过，特征图的数量取决于卷积层中的卷积核个数。1 个卷积核产生 1 个 <strong>feature map</strong>，1 个 <strong>feature map</strong> 有 1 对 $\boldsymbol{\gamma}$ 和 $\boldsymbol{\beta}$ 参数，同一 <strong>Batch</strong> 同 <strong>channel</strong> 的 <strong>feature map</strong> 共享同一对 $\boldsymbol{\gamma}$ 和 $\boldsymbol{\beta}$ 参数，若卷积层有 $\boldsymbol{n}$ 个卷积核，则有 $\boldsymbol{n}$ 对 $\boldsymbol{\gamma}$ 和 $\boldsymbol{\beta}$ 参数。<br>&emsp;&emsp;至此，我跟大家介绍完了卷积神经网络的基础网络结构，分了两篇博客，分别讲了卷积层、池化层和 BN 层，在下一篇博客中我会跟大家介绍几种经典的 CNN 模型。期待您的访问！感恩！</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>在使用 tensorboard 时报错</title>
    <url>/2022/06/05/%E5%9C%A8%E4%BD%BF%E7%94%A8-tensorboard-%E6%97%B6%E6%8A%A5%E9%94%99/</url>
    <content><![CDATA[<p>﻿昨天在使用 tensorboard 时， 输入命令之后，一直出现下面的错误</p>
<blockquote>
<p>tensorboard: error: invalid choice: ‘Recognizer\\logs’ (choose from ‘serve’, ‘dev’)</p>
</blockquote>
<p>开始还以为是代码问题，但发现不是，又去找官网和别人的经过，但是最多找到的是路径是否正确的问题，但是我确定我不是。然后突然发现路径中有空格，就怀疑可能是空格的问题，经手一改，然后就可以了(学艺不精)</p>
<p>注意检查 <strong>tensorboard —logdir=”路径”</strong>（注意要是双引号） 中的路径是否空格以及logs所在路径是否填写正确。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>对含有奇异值和高斯噪声的数据进行处理</title>
    <url>/2022/06/05/%E5%AF%B9%E5%90%AB%E6%9C%89%E5%A5%87%E5%BC%82%E5%80%BC%E5%92%8C%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>﻿分别用平均滑动窗口、指数滑动窗口、SG滤波法对含有奇异值和高斯噪声的两列数据进行去奇异值和降噪，最终拟合曲线推测函数表达式。<a href="https://blog.csdn.net/hajlyx/article/details/100580316">去噪方法理论知识参考</a><br>对第一列数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> scio</span><br><span class="line">%matplotlib</span><br><span class="line"><span class="comment">#防止中文乱码</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;font.sans-serif&quot;</span>] = [<span class="string">&quot;Simhei&quot;</span>]</span><br><span class="line">plt.rcParams[<span class="string">&quot;axes.unicode_minus&quot;</span>] = <span class="literal">False</span></span><br><span class="line">data = scio.loadmat(<span class="string">&#x27;2 data_preprocess_practice.mat&#x27;</span>)</span><br><span class="line"></span><br><span class="line">yy3 = data[<span class="string">&quot;yy3&quot;</span>]</span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">20001</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#去除奇异值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Noise_reduction</span>(<span class="params">data_col</span>) :</span><br><span class="line">    lst = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="comment">#此处用的是3sigema的方法</span></span><br><span class="line">    <span class="keyword">while</span> i + <span class="number">12</span> &lt; <span class="number">20001</span> :</span><br><span class="line">        lst1 = data_col[i : i + <span class="number">12</span>]</span><br><span class="line">        mean = np.mean(lst1)</span><br><span class="line">        std = np.std(lst1)</span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> lst1 :</span><br><span class="line">            <span class="keyword">if</span> (value - mean) &gt;= -<span class="number">3</span> * std <span class="keyword">and</span> (value - mean) &lt;= <span class="number">3</span> * std :</span><br><span class="line">                lst.append(value)</span><br><span class="line">        i += <span class="number">12</span></span><br><span class="line">        lst1 = []</span><br><span class="line">    <span class="keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="comment">#平均滑动去噪</span></span><br><span class="line"><span class="comment">#滑动平均法适用于，噪声的均值为0，真实值变化不大或线性变化的场景</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Average_sliding_denoising</span>(<span class="params">arr, window_size</span>) :</span><br><span class="line">    <span class="comment">#对数组进首尾扩展，以滑动窗口可以处理到首尾点，思想与图片滤波算子相似</span></span><br><span class="line">    New_arr = arr[ : ]</span><br><span class="line">    window_size = (window_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(window_size) :</span><br><span class="line">        arr.insert(step, <span class="built_in">sum</span>(arr[ : window_size]) / window_size)</span><br><span class="line">        arr.insert(<span class="built_in">len</span>(arr) - step, <span class="built_in">sum</span>(arr[<span class="built_in">len</span>(arr) - window_size : <span class="built_in">len</span>(arr)]) / window_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(window_size, <span class="built_in">len</span>(arr) - window_size) :</span><br><span class="line">        New_arr[i - window_size] = (<span class="built_in">sum</span>(arr[i - window_size : i + window_size + <span class="number">1</span>])) / (<span class="number">2</span> * window_size + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> New_arr</span><br><span class="line"></span><br><span class="line"><span class="comment">#指数平均滑动去噪</span></span><br><span class="line"><span class="comment">#当误差不受观测值大小影响的话，指数滑动平均比滑动平均好；当误差随观测值大小变化时，滑动平均比指数滑动平均更好。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Exponential_sliding_denoising</span>(<span class="params">arr, weight = <span class="number">0.01</span></span>) :</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(arr)) :</span><br><span class="line">        arr[i] = weight * arr[i] + (<span class="number">1</span> - weight) * arr[i - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line"><span class="comment">#Savitzky-Golay平滑去噪</span></span><br><span class="line"><span class="comment">#SG滤波法对于数据的观测信息保持的更好，在一些注重数据变化的场合会比较适用。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_x</span>(<span class="params">size, rank</span>):</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span> * size + <span class="number">1</span>):</span><br><span class="line">        m = i - size</span><br><span class="line">        row = [m ** j <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(rank)]</span><br><span class="line">        x.append(row) </span><br><span class="line">    x = np.mat(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Savgol_Denosing</span>(<span class="params">arr, window_size, rank</span>) :</span><br><span class="line">    New_arr = arr[ : ]</span><br><span class="line">    m = (window_size - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="comment"># 处理边缘数据，用边缘值首尾增加m个首尾项</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(m) :</span><br><span class="line">        arr.insert(step, arr[<span class="number">0</span>])</span><br><span class="line">        arr.insert(<span class="built_in">len</span>(arr) - step, arr[<span class="built_in">len</span>(arr) - <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 创建X矩阵</span></span><br><span class="line">    X = create_x(m, rank)</span><br><span class="line">    <span class="comment"># 计算加权系数矩阵B</span></span><br><span class="line">    B = (X * (X.T * X).I) * X.T</span><br><span class="line">    <span class="comment">#只用更新第m个点，因此只需取B系数矩阵的第m行即可</span></span><br><span class="line">    A0 = B[m].T</span><br><span class="line">    <span class="comment"># 计算平滑修正后的值</span></span><br><span class="line">    narr = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(New_arr)):</span><br><span class="line">        y = [arr[i + j] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(window_size)]</span><br><span class="line">        y1 = np.mat(y) * A0</span><br><span class="line">        y1 = <span class="built_in">float</span>(y1)</span><br><span class="line">        narr.append(y1)</span><br><span class="line">    <span class="keyword">return</span> narr</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化不同去噪方法的效果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Mapping</span>(<span class="params">lst, arr, arr1, arr2</span>) :</span><br><span class="line">    x = np.array(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(arr), <span class="number">1</span>)))</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    fig.<span class="built_in">set</span>(alpha = <span class="number">0.2</span>)</span><br><span class="line">    plt.subplot2grid((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    plt.plot(x, arr, label = <span class="string">&#x27;Average_sliding_denoising&#x27;</span>)</span><br><span class="line">    plt.legend(loc = <span class="number">1</span>)</span><br><span class="line">    plt.subplot2grid((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    plt.plot(x, arr1, <span class="string">&#x27;g-&#x27;</span>, label = <span class="string">&#x27;Exponential_sliding_denoising&#x27;</span>)</span><br><span class="line">    plt.legend(loc = <span class="number">1</span>)</span><br><span class="line">    plt.subplot2grid((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">0</span>, <span class="number">2</span>))</span><br><span class="line">    plt.plot(x, arr2, <span class="string">&#x27;r-&#x27;</span>, label = <span class="string">&#x27;Savgol_Denosing&#x27;</span>)</span><br><span class="line">    plt.legend(loc = <span class="number">1</span>)</span><br><span class="line">    plt.subplot2grid((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">    plt.plot(x, lst, <span class="string">&#x27;b-&#x27;</span>, x, arr, <span class="string">&#x27;pink&#x27;</span>, x, arr1, <span class="string">&#x27;g&#x27;</span>, x, arr2, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Before Denoising&#x27;</span>, <span class="string">&#x27;Exponential_sliding_denoising&#x27;</span>, <span class="string">&#x27;Average_sliding_denoising&#x27;</span>, <span class="string">&#x27;Savgol_Denosing&#x27;</span>], loc = <span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#小结，单纯从可视化效果来看，指数平均化动的效果是最好的</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#数据重新拟合，推测函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Polynomial_fitting</span>(<span class="params">lst</span>) :</span><br><span class="line">    x1 = np.arange(<span class="number">0</span>, <span class="built_in">len</span>(lst), <span class="number">1</span>).astype(<span class="built_in">float</span>)</span><br><span class="line">    z1 = np.polyfit(x1, lst, <span class="number">11</span>)</span><br><span class="line"><span class="comment">#     print(np.poly1d(z1))</span></span><br><span class="line">    x_points = np.linspace(<span class="number">0</span>, <span class="number">19973</span>, <span class="number">19973</span>)</span><br><span class="line">    y_point = np.polyval(z1, x_points)</span><br><span class="line">    fig1 = plt.figure()</span><br><span class="line">    plt.plot(x1, lst, x_points, y_point, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Before fitting&#x27;</span>, <span class="string">&#x27;After fitting&#x27;</span>], loc = <span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_col1 = []</span><br><span class="line">data_col2 = []</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> yy3 :</span><br><span class="line">    data_col1.append(line[<span class="number">0</span>])</span><br><span class="line">    data_col2.append(line[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">data_col1 = np.array(data_col1)</span><br><span class="line">data_col2 = np.array(data_col2)</span><br><span class="line"></span><br><span class="line">lst1 = Noise_reduction(data_col1)</span><br><span class="line">lst1_A = Average_sliding_denoising(Noise_reduction(data_col1), <span class="number">61</span>)</span><br><span class="line">lst1_E = Exponential_sliding_denoising(Noise_reduction(data_col1))</span><br><span class="line">lst1_S = Savgol_Denosing(Noise_reduction(data_col1), <span class="number">59</span>, <span class="number">2</span>)</span><br><span class="line">Mapping(lst1, lst1_A, lst1_E, lst1_S)</span><br><span class="line">Polynomial_fitting1(lst1_A)</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/104514d8184f40f88c9bae68948c16f5.png" alt="在这里插入图片描述"></p>
<p>对第二列数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Polynomial_fitting2</span>(<span class="params">lst</span>) :</span><br><span class="line">    x1 = np.arange(<span class="number">0</span>, <span class="built_in">len</span>(lst), <span class="number">1</span>).astype(<span class="built_in">float</span>)</span><br><span class="line">    z1 = np.polyfit(x1, lst, <span class="number">50</span>)</span><br><span class="line">    x_points = np.linspace(<span class="number">0</span>, <span class="number">19973</span>, <span class="number">19973</span>)</span><br><span class="line">    y_point = np.polyval(z1, x_points)</span><br><span class="line">    fig1 = plt.figure()</span><br><span class="line">    plt.plot(x1, lst, x_points, y_point, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.legend([<span class="string">&#x27;Before fitting&#x27;</span>, <span class="string">&#x27;After fitting&#x27;</span>], loc = <span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">lst2 = Noise_reduction(data_col2)</span><br><span class="line">lst2_A = Average_sliding_denoising(Noise_reduction(data_col2), <span class="number">61</span>)</span><br><span class="line">lst2_E = Exponential_sliding_denoising(Noise_reduction(data_col2))</span><br><span class="line">lst2_S = Savgol_Denosing(Noise_reduction(data_col2), <span class="number">59</span>, <span class="number">2</span>)</span><br><span class="line">Mapping(lst2, lst2_E, lst2_A, lst2_S)</span><br><span class="line">Polynomial_fitting2(lst2_A)</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/69f8d6d6d4c540d6b81e69278035a21d.png" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据去噪</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络（一）</title>
    <url>/2022/06/05/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1    简介"></a>1    简介</h1><p>&emsp;&emsp;在本系列文章中，我有跟大家分享过神经网络中两种经典层：<strong>卷积层(CNN)</strong> 和 <strong>全连接层(FC)</strong>，这两种层的输入数据分别是：特征向量和图像(张量)，在具体实现时输入的多样本之间是相互独立的，无联系关系。而且，卷积神经网络利用数据的局部相关性和权值共享的思想大大减少了网络的参数量，非常适合于图片这种具有 <strong>空间(Spatial)</strong> 局部相关性的数据，但自然界的信号除了具有空间维度之外，还有一个 <strong>时间(Temporal)</strong> 维度。具有时间维度的数据(也称作序列数据)有以下特点：</p>
<ul>
<li>不同样本之间存在相互关联；</li>
<li>模型的输出不仅    取决于当前输入，也受历史输入的影响。</li>
</ul>
<p>因此卷积神经网络并不擅长处理此类数据，本博客要介绍的循环神经网络可以较好地解决此类问题。<br>&emsp;&emsp;常见的序列数据有：语言、音乐、视频、股票、文字、DNA等等。</p>
<h1 id="2-序列表示方法"><a href="#2-序列表示方法" class="headerlink" title="2    序列表示方法"></a>2    序列表示方法</h1><p>&emsp;&emsp;具有先后顺序的数据一般叫作 <strong>序列(Sequence)</strong>，比如随时间而变化的商品价格数据就是非常典型的序列。考虑某件商品 A 在 1 月到 6 月之间的价格变化趋势，我们记为一维向量：$\boldsymbol{[x_1,x_2,x_3,x_4,x_5,x_6]}$，shape 为 $\boldsymbol{[6]}$。如果要表示 $\boldsymbol{b}$ 件商品在 1 月到 6 月之间的价格变化趋势，可以记为 2 维张量：</p>
<script type="math/tex; mode=display">
\boldsymbol{\left[[x_1^{(1)},x_2^{(1)},\cdots,x_6^{(1)}],[x_1^{(2)},x_2^{(2)},\cdots,x_6^{(2)}],\cdots,[x_1^{(6)},x_2^{(6)},\cdots,x_6^{(6)}]\right]}</script><p>其中 $\boldsymbol{b}$ 表示商品的数量，张量 shape 为 $\boldsymbol{[b,6]}$。因此，表示一个序列信号只需要一个 shape 为 $\boldsymbol{[b,s]}$ 的张量，其中 $\boldsymbol{b}$ 为序列数量，$\boldsymbol{s}$ 为一个序列的长度。但是对于很多信号并不能直接用一个标量数值表示，比如每个时间戳产生长度为 $\boldsymbol{n}$ 的特征向量，则需要 shape 为 $\boldsymbol{[b,s,n]}$ 的张量才能表示。考虑更复杂的文本数据：句子。它在每个时间戳上面产生的单词是一个字符，并不是数值，不能直接用某个标量表示。</p>
<h2 id="2-1-独热表示"><a href="#2-1-独热表示" class="headerlink" title="2.1    独热表示"></a>2.1    独热表示</h2><p>&emsp;&emsp;对于一个含有 $\boldsymbol{n}$ 个单词的句子，单词的一种简单表示方法就是 <strong>One-hot</strong> 编码。以英文句子为例，假设只考虑最常用的 1 万个单词，那么每个单词就可以表示为某位为 1，其它位置为 0 且长度为 1 万的稀疏 <strong>One-hot</strong> 向量；对于中文句子，如果也只考虑最常用的 5000 个汉字，同样的方法，一个汉字可以用长度为 5000 的 <strong>One-hot</strong> 向量表示。如果只考虑 $\boldsymbol{n}$ 个地名单词，可以将每个地名编码为长度为 $\boldsymbol{n}$ 的 <strong>One-hot</strong> 向量。</p>
<p><img src="https://img-blog.csdnimg.cn/c8298b3c836e41d99407383c05ad684c.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;文字编码为数值的过程叫作 <strong>Word Embedding</strong>。One-hot 的编码方式实现 <strong>Word Embedding</strong> 简单直观，编码过程不需要学习和训练。在神经网络中，一般使用 <strong>词袋模型(Bag of words)</strong> 先对单词编号，如 2 表示 “I”，3 表示 “me” 等，然后再对每个进行 <strong>One-hot</strong> 编码，最后将每个词向量拼接成矩阵或者张量。但 <strong>One-hot</strong> 编码有以下缺点：</p>
<ul>
<li>容易受维数灾难的困扰，尤其是将其用于深度学习算法时；</li>
<li>任何两个词都是孤立的，存在语义鸿沟词(任意两个词之间都是孤 立的，不能体现词和词之间的关系)。如：对于单词 “like”、“dislike”、“Rome”、“Paris” 来说，“like” 和 “dislike” 在语义角度就强相关，它们都表示喜欢的程度；“Rome” 和 “Paris” 同样也是强相关，他们都表示欧洲的两个地点。如果采用 One-hot 编码，得到的向量之间没有相关性。</li>
</ul>
<p>为了克服此不足，人们提出了另一种表示方法，即分布式表示。</p>
<h2 id="2-2-分布式表示"><a href="#2-2-分布式表示" class="headerlink" title="2.2    分布式表示"></a>2.2    分布式表示</h2><p>&emsp;&emsp;首先要介绍一种相似度衡量方式。在自然语言处理领域，有专门的一个研究方向在探索如何学习到单词的表示向量(Word Vector)，使得语义层面的相关性能够很好地通过 Word Vector 体现出来。一个衡量词向量之间相关度的方法就是 <strong>余弦相关度(Cosine similarity)</strong>：</p>
<script type="math/tex; mode=display">
\boldsymbol{similarity(a,b)≜ cos(\theta)=\frac{a\cdot b}{|a| \cdot |b|}}</script><p>其中 $\boldsymbol{a}$ 和 $\boldsymbol{b}$ 代表了两个词向量。</p>
<p><img src="https://img-blog.csdnimg.cn/badafe223d1c46cdaf5e0937929cc8cf.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;分布式表示最早由 Hinton 于 1986 年提出的，可以克服独热表示的缺点。 解决词汇与位置无关问题，可以通过计算向量之间的 <strong>距离(欧式距离、余弦距离等)</strong> 来体现词与词的相似性。其基本想法是直接用一个普通的向量表示 一个词，此向量为$\boldsymbol{[0.792,-0.177,-0.107,0.109,-0.542,…]}$，常见维度为 50 或 100。用这种方式表示的向量，“麦克” 和 “话筒” 的距离会远远小于“ 麦克” 和 “天气” 的距离。<br>&emsp;&emsp;词向量的分布式表示的优点是解决了词汇与位置无关问题，不足是学习过程相对复杂且受训练语料的影响很大。训练这种向量表示的方法较多，常 见的有 LSA、PLSA、LDA、Word2Vec等，其中 Word2Vec 是 Google 在 2013 年开源的一个词向量计算工具，同时也是一套生成词向量的算法方案。</p>
<h2 id="2-3-Embedding-层"><a href="#2-3-Embedding-层" class="headerlink" title="2.3    Embedding 层"></a>2.3    Embedding 层</h2><p>&emsp;&emsp;在神经网络中，单词的表示向量可以直接通过训练的方式得到，我们把单词的表示层叫作 Embedding 层。Embedding 层负责把单词编码为某个词向量 $\boldsymbol{v}$，它接受的是采用数字编码的单词编号$\boldsymbol{i}$， 也就是词袋模型编码后的结果。系统总单词数量记为$\boldsymbol{N_{vocab}}$，输出长度为 $\boldsymbol{n}$ 的向量 $\boldsymbol{v}$：</p>
<script type="math/tex; mode=display">
\boldsymbol{v = f_\theta(i|N_{vocab}, n)}</script><p>构建一个 shape 为 $\boldsymbol{[N_{vocab}, n]}$ 的查询表对象 $\boldsymbol{table}$，对于任意的单词编号 $\boldsymbol{i}$，只需要查询到对应位置上的向量并返回即可：</p>
<script type="math/tex; mode=display">
 \boldsymbol{v = table[i]}</script><p>在 <strong>PyTorch</strong> 中，有一个 <strong>nn.Embedding(vocab_size，n)</strong> 类，它是 Module 类的子类，这里它接受最重要的两个初始化参数：词汇量大小，每个词汇向量表示的向量维度。Embedding 类返回的是一个形状为 <strong>[每句词个数，词维度]</strong> 的矩阵。</p>
<h1 id="3-一个例子"><a href="#3-一个例子" class="headerlink" title="3    一个例子"></a>3    一个例子</h1><p>&emsp;&emsp;现在我们来考虑如何处理序列信号，以文本序列为例，考虑一个句子：</p>
<script type="math/tex; mode=display">
“I\ hate\ this\ boring\ movie”</script><p>通过 Embedding 层，可以将它转换为 shape 为 $\boldsymbol{[b,s,n]}$ 的张量，$\boldsymbol{b}$ 为句子数量，$\boldsymbol{s}$ 为句子长度，$\boldsymbol{n}$ 为词向量长度。上述句子可以表示为 shape 为$\boldsymbol{[1,5,10]}$ 的张量，其中 5 代表句子单词长度，10 表示词向量长度。<br>&emsp;&emsp;接下来以情感分类任务为例来逐步探索能够处理序列信号的网络模型。情感分类任务通过分析给出的文本序列，提炼出文本数据表达的整体语义特征，从而预测输入文本的情感类型：<strong>正面评价</strong> 或者 <strong>负面评价</strong>。从分类角度来看，情感分类问题就是一个简单的二分类问题，与图片分类不一样的是，由于输入是文本序 列，传统的卷积神经网络并不能取得很好的效果。</p>
<p><img src="https://img-blog.csdnimg.cn/037699a2f9624e7c861cb51effa6f09d.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-1-考虑全连接"><a href="#3-1-考虑全连接" class="headerlink" title="3.1    考虑全连接"></a>3.1    考虑全连接</h2><p>&emsp;&emsp;对于每个词向量，分别使用一个全连接层网络来提取语义特征：</p>
<script type="math/tex; mode=display">
\boldsymbol{o = \sigma(W_tx_t + b_t)}</script><p><img src="https://img-blog.csdnimg.cn/f7b6f2fe099f4dd987aa592fdac6bce7.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，各个单词的词向量通过 $\boldsymbol{s}$ 个全连接层分类网络 1 提取每个单词的特征，所有单词的特征最后合并，并通过分类网络 2 输出序列的类别概率分布，对于长度为 $\boldsymbol{s}$ 的句子来说，至少需要 $\boldsymbol{s}$ 个全网络层。但这种网络结构的缺点是：</p>
<ul>
<li>网络参数量是巨大，内存占用和计算代价较高，同时由于每个序列的长度 $\boldsymbol{s}$ 并不相同，网络结构是动态变化的；</li>
<li>每个全连接层子网络 $\boldsymbol{W_i}$ 和 $\boldsymbol{b_i}$ 只能感受当前词向量的输入，并不能感知之前和之后的语境信息，导致句子整体语义的缺失，每个子网络只能根据自己的输入来提取高层特征。</li>
</ul>
<h2 id="3-2-考虑权重共享"><a href="#3-2-考虑权重共享" class="headerlink" title="3.2    考虑权重共享"></a>3.2    考虑权重共享</h2><p>&emsp;&emsp;针对全连接的第一个缺点，我们知道卷积神经网络之所以在处理局部相关数据时优于全连接网络，是因为它充分利用了权值共享的思想，大大减少了网络的参数量，使得网络训练起来更加高效。那在处理序列信号的问题上，我们可以借鉴权值共享的思想。</p>
<p><img src="https://img-blog.csdnimg.cn/d2e43e2f62e64cadaa1abaa7b26225a5.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，将这 $\boldsymbol{s}$ 个网络层参数共享，这样其实相当于使用一个全连接网络来提取所有单词的特征信息。通过权值共享后，参数量大大减少，网络训练变得更加稳定高效。但是，这种网络结构并没有考虑序列之间的先后顺序，将词向量打乱次序仍然能获得相同的输出，无法获取有效的全局语义信息。</p>
<h2 id="3-3-考虑全局语义"><a href="#3-3-考虑全局语义" class="headerlink" title="3.3    考虑全局语义"></a>3.3    考虑全局语义</h2><p>&emsp;&emsp;针对全连接的第二个缺点，我们让网络能够提供一个单独的内存变量，每次提取词向量的特征并刷新内存变量，直至最后一个输入完成，此时的内存变量即存储了所有序列的语义特征，并且由于输入序列之间的先后顺序，使得内存变量内容与序列顺序紧密关联。</p>
<p><img src="https://img-blog.csdnimg.cn/6378facb85614b75953d5a4bee30d6fe.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中，将内存变量实现为一个状态张量 $\boldsymbol{h}$，除了原来的 $\boldsymbol{W_{xh}}$ 参数共享外，这里额外增加了一个 $\boldsymbol{W_{hh}}$ 参数，每个时间戳 $\boldsymbol{t}$ 上状态张量刷新为：</p>
<script type="math/tex; mode=display">
\boldsymbol{h_t = \sigma(W_{xh}x_t+W_{hh}h_{t-1}+b) }</script><p>其中状态张量 $\boldsymbol{h_0}$ 为初始的内存状态，可以初始化为全 0，$\boldsymbol{\sigma}$ 为激活函数，经过 $\boldsymbol{s}$ 个词向量的输入后得到网络最终的状态张量  $\boldsymbol{h_s}$，$\boldsymbol{h_s}$ 较好地代表了句子的全局语义信息，基于 $\boldsymbol{h_s}$ 通过某个全连接层分类器即可完成情感分类任务。</p>
<h2 id="3-4-循环神经网络"><a href="#3-4-循环神经网络" class="headerlink" title="3.4    循环神经网络"></a>3.4    循环神经网络</h2><p>&emsp;&emsp;经过上面的一步步分析探索，可以得到一种新型的网络结构，在每个时间戳 $\boldsymbol{t}$，网络层接受当前时间戳的输入 $\boldsymbol{x_t}$ 和上一个时间戳的网络状态向量 $\boldsymbol{h_{t-1}}$ ，经过：</p>
<script type="math/tex; mode=display">
 \boldsymbol{h_t = f_\theta(h_{t-1},x_t)}</script><p>变换后得到当前时间戳的新状态向量 $\boldsymbol{h_t}$，并写入内存状态中，其中$\boldsymbol{f_\theta}$ 代表了网络的运算逻辑，$\boldsymbol{\theta}$ 为网络参数集。如果在每个时间戳上，网络层均有输出产生 $\boldsymbol{o_t}$，$\boldsymbol{o_t = g_\gamma(h_t)}$，即将网络的状态向量变换后输出。</p>
<p><img src="https://img-blog.csdnimg.cn/5bda3b2b2b6044c6a6ac57c868471e96.png#pic_center" alt="在这里插入图片描述"></p>
<p>上述网络结构在时间戳上折叠，可简化成：</p>
<p><img src="https://img-blog.csdnimg.cn/e837b3cf7f3f4efc96ec4975583393a7.png#pic_center" alt="在这里插入图片描述"></p>
<p>网络循环接受序列的每个特征向量 $\boldsymbol{x_t}$，并刷新内部状态向量 $\boldsymbol{h_t}$，同时形成输出 $\boldsymbol{o_t}$。对于这种网络结构，我们把它叫做 <strong>循环神经网络(Recurrent Neural Network，简称 RNN)</strong>。<br>&emsp;&emsp;如果使用张量 $\boldsymbol{W_{xh}}$、$\boldsymbol{W_{hh}}$ 和偏置 $\boldsymbol{b}$ 来参数化 $\boldsymbol{f_\theta}$ 网络，并按照如下方式更新内存状态：</p>
<script type="math/tex; mode=display">
\boldsymbol{h_t = \sigma(W_{xh}x_t+W_{hh}h_{t-1}+b) }</script><p>我们把这种网络叫做基本的循环神经网络，如无特别说明，一般说的循环神经网络即指这种实现。在循环神经网络中，激活函数更多地采用 <strong>tanh</strong> 函数。并且可以选择不使用偏执 $\boldsymbol{b}$ 来进一步减少参数量。状态向量 $\boldsymbol{h_t}$ 可以直接用作输出，即 $\boldsymbol{o_t = h_t}$，也可以对 $\boldsymbol{h_t}$ 做一个简单的线性变换 $\boldsymbol{o_t = W_{ho}h_t}$ 后得到每个时间戳上的网络输出 $\boldsymbol{o_t}$。</p>
<h2 id="3-5-小结"><a href="#3-5-小结" class="headerlink" title="3.5    小结"></a>3.5    小结</h2><p>&emsp;&emsp;经过上面的推到，我们知道 RNN 是怎么来的的，内部的公式如何计算的。由上面再次总结 RNN 的核心公式如下：</p>
<ul>
<li>$\boldsymbol{z_t = W_{xh}x_t+W_{hh}h_{t-1}+b}$；</li>
<li>$\boldsymbol{h_t =  \sigma(z_t) }$；</li>
<li>$\boldsymbol{o_t = W_{ho}h_t}$，若是分类问题的话，还可以是 $\boldsymbol{o_t = softmax(h_t)}$。</li>
</ul>
<p>要注意的是 RNN 中的激活函数可以是 ReLU、sigmoid、tanh，不过一般来说使用 tanh 较多。</p>
<h1 id="4-RNN-类型"><a href="#4-RNN-类型" class="headerlink" title="4    RNN 类型"></a>4    RNN 类型</h1><p>&emsp;&emsp;循环神经网络适合于处理序列数据，序列长度一般不固定，比如我们在前面举的文本分类的是有多个输入，只有一个输出；在讲解 RNN 结构时每一个输入都有一个输出，因此，RNN 应用非常广泛。我们可以根据应用场景将 RNN 分成以下几种类型：<br><img src="https://img-blog.csdnimg.cn/44cb35900c0841ab97ff5d0cdb765fa0.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中每一个矩形是一个向量，箭头则表示函数(比如矩阵相乘)。 其中最下层为输入向量，最上层为输出向量，中间层表示 RNN 的状态。从左到右：</p>
<ul>
<li>没有使用 RNN 的 Vanilla 模型，从固定大小的输入得到固定大小输出(比如图像分类)；</li>
<li>序列输出(比如图片字幕，输入一张图片输出一段文字序列)；</li>
<li>序列输入(比如情感分析，输入一段文字，然后将它分类成积极或者消极情感，包括雷达和我们上面举的文本分类)；</li>
<li>序列输入和序列输出(比如机器翻译：一个 RNN 读取一条英文语 句，然后将它以法语形式输出)；</li>
<li>同步序列输入输出(比如视频分类，对视频中每一帧打标签)。</li>
</ul>
<p>此外，RNN 也可以堆叠几层，如下：</p>
<p><img src="https://img-blog.csdnimg.cn/c6336fc568c547a694b0bccb4b92bdd1.png#pic_center" alt="在这里插入图片描述"></p>
<p>计算公式跟一层 RNN 完全一样，只是输出上有些区别，这里不展开讲述。</p>
<h1 id="5-反向传播-只是探讨梯度问题"><a href="#5-反向传播-只是探讨梯度问题" class="headerlink" title="5    反向传播(只是探讨梯度问题)"></a>5    反向传播(只是探讨梯度问题)</h1><p>&emsp;&emsp;这篇博客就不跟大家探讨 RNN 的梯度传播了，只是探讨一下传播过程中的问题。RNN 跟别的网络结构一样，在反向传播中依然存在梯度爆炸和梯度消失的问题，主要的解决方法如下：</p>
<ul>
<li>梯度爆炸：进行 <strong>梯度裁剪(Gradient Clipping)</strong>，梯度裁剪与张量限幅非常类似，也是通过将梯度张量的数值或者范数限制在某个较小的区间内，从而将远大于 1 的梯度值减少，避免出现梯度爆炸。</li>
<li>梯度消失：对网络结构进行改进，即 <strong>GRU</strong> 和 <strong>LSTM</strong>，这两种 RNN 的变种也是我下一篇要介绍的网络结构。</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络（二）</title>
    <url>/2022/06/05/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-RNN-的缺点"><a href="#1-RNN-的缺点" class="headerlink" title="1    RNN 的缺点"></a>1    RNN 的缺点</h1><p>&emsp;&emsp;我在<a href="https://blog.csdn.net/weixin_53598445/article/details/124615312">上一篇博客</a>中跟大家一步一步探索了 RNN 模型的网络结构，最后面也介绍了 RNN 的应用场景。但在实际应用中，标准 RNN 训练的优化算法面临一个很大的难题，就是长期依赖问题——由于网络结构的变深使得模型丧失了学习到先前信息的能力，通俗的说，标准的 RNN 虽然有了记忆，但很健忘，也即标准 RNN 只有短时记忆。循环神经网络在处理较长的句子时，往往只能够理解有限长度内的信 息，而对于位于较长范围类的有用信息往往不能够很好的利用起来。我们把这种现象叫做短时记忆。<br>&emsp;&emsp;针对标准 RNN 短时记忆的问题，最直接的想法就是延长这种短时记忆，使得 RNN 可以有效利用较大范围内的训练数据，从而提升性能。这时，一种基于 RNN 改进的新型网络模型——<strong>LSTM</strong> 该登场了。同时在上篇博客的最后面谈到了 RNN 的梯度消失问题，LSTM 模型可以有效地解决这个问题。</p>
<h1 id="2-LSTM"><a href="#2-LSTM" class="headerlink" title="2    LSTM"></a>2    LSTM</h1><p>&emsp;&emsp;1997 年，瑞士人工智能科学家 Jürgen Schmidhuber 提出了 <strong>长短时记忆网络</strong>(Long Short-Term Memory，简称 LSTM)。LSTM 相对于基础的 RNN 网络来说，记忆能力更强，更擅长处理较长的序列信号数据，LSTM 提出后，被广泛应用在序列预测、自然语言处理等任务中，几乎取代了基础的 RNN 模型。<br>&emsp;&emsp;首先回顾一下基础的 RNN 网络结构：<br><img src="https://img-blog.csdnimg.cn/6b26167dd50b4959a0454dcb9b6e4bda.png" alt="在这里插入图片描述"></p>
<p>上一个时间戳的状态向量 $\boldsymbol{h_{t-1}}$ 与当前时间戳的输入 $\boldsymbol{x_t}$ 经过线性变换后，通过激活函数 $\boldsymbol{tanh}$ 后得到新的状态向量 $\boldsymbol{h_{t}}$。相对于基础的 RNN，网络只有一个状态向量 $\boldsymbol{h_{t}}$，LSTM 新增了一个状态向量 $\boldsymbol{C_{t}}$，同时引入了 <strong>门控(Gate)机制</strong>，通过门控单元来控制信息的遗忘和刷新：<img src="https://img-blog.csdnimg.cn/9c06bb9b83374b9d86de18b5b36e763a.png" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;在 LSTM Cell 中，有两个状态向量 $\boldsymbol{c}$ 和 $\boldsymbol{h}$，其中  $\boldsymbol{c}$ 作为 LSTM 的内部状态向量，可以理解为 LSTM 的 <strong>内存状态向量 Memory</strong>，而  $\boldsymbol{h}$ 表示 LSTM 的输出向量。相对于基础的 RNN 来说，LSTM 把内部 Memory 和输出分开为两个变量，同时利用三个门控：<strong>输入门</strong>(Input Gate)、<strong>遗忘门</strong>(Forget Gate)和<strong>输出门</strong>(Output Gate)来控制内部信息的流动。<br>&emsp;&emsp;门控机制可以理解为控制数据流通量的一种手段，类比于水阀门：当水阀门全部打开时，水流畅通无阻地通过；当水阀门全部关闭时，水流完全被隔断。在 LSTM 中，阀门开和程度利用门控值向量 $\boldsymbol{g}$ 表示：</p>
<p><img src="https://img-blog.csdnimg.cn/9cd23d7ac7384dabb0b211023da816d8.png#pic_center" alt="在这里插入图片描述"></p>
<p>上图中通过 $\boldsymbol{\sigma(g)}$ 激活函数将门控值压缩到 $\boldsymbol{[0, 1]}$ 之间，当 $\boldsymbol{\sigma(g) = 0}$ 时，门控全部关闭，输出 $\boldsymbol{o = 0}$；当 $\boldsymbol{\sigma(g) = 1}$ 时，门控全部打开，输出 $\boldsymbol{o = x}$。通过门控机制可以较好地控制数据的流量程度。</p>
<hr>
<p><strong><font color=red>注</font></strong>：到此您可以阅读完 GUR 的原理之后再回来阅读 LSTM，因 GRU 结构较为简单。</p>
<hr>
<h2 id="2-1-遗忘门"><a href="#2-1-遗忘门" class="headerlink" title="2.1    遗忘门"></a>2.1    遗忘门</h2><p>&emsp;&emsp;遗忘门作用于 LSTM 状态向量 $\boldsymbol{c}$，用于控制上一个时间戳的记忆 $\boldsymbol{c_{t - 1}}$ 对当前时间戳的影响。</p>
<p><img src="https://img-blog.csdnimg.cn/2ee0d073b9a1440c94b350f5e7d4e00c.png#pic_center" alt="在这里插入图片描述"></p>
<p>遗忘门的控制变量 $\boldsymbol{g_f}$ 计算过程如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{g_f = \sigma(W_f[h_{t- 1};x_t]+b_f)}</script><p>其中 $\boldsymbol{W_f}$ 和 $\boldsymbol{b_f}$ 为遗忘门的参数张量，可由反向传播算法自动优化，$\boldsymbol{\sigma}$ 为激活函数，一般使用 <strong>Sigmoid</strong> 函数。当 $\boldsymbol{g_f = 1}$ 时，遗忘门全部打开，LSTM 接受上一个状态 $\boldsymbol{c_{t-1}}$ 的所有信息 ；当 $\boldsymbol{g_f = 0}$ 时，遗忘门关闭，LSTM 直接忽略 $\boldsymbol{c_{t-1}}$ 的所有信息输出为 0 的向量。这也是遗忘门的名字由来。经过遗忘门后，LSTM 的状态向量 $\boldsymbol{c_t}$ 变为 $\boldsymbol{g_fc_{t-1}}$。</p>
<h2 id="2-2-输入门"><a href="#2-2-输入门" class="headerlink" title="2.2    输入门"></a>2.2    输入门</h2><p>&emsp;&emsp;输入门用于控制 LSTM 对输入的接收程度。</p>
<p><img src="https://img-blog.csdnimg.cn/5c894335d30e4ec68c7be897d66a22fe.png#pic_center" alt="在这里插入图片描述"></p>
<p>首先通过对当前时间戳的输入 $\boldsymbol{x_t}$ 和上一个时间戳的输出 $\boldsymbol{h_{t - 1}}$ 做非线性变换得到新的输入向量  $\boldsymbol{\tilde{c_t}}$：</p>
<script type="math/tex; mode=display">
\boldsymbol{\tilde{c_t} = tanh(W_c[h_{t -1};x_t] +b_c)}</script><p>其中 $\boldsymbol{W_c}$ 和 $\boldsymbol{b_c}$ 为输入门的参数，需要通过反向传播算法自动优化，$\boldsymbol{tanh}$ 为激活函数，用于将输入标准化到 $\boldsymbol{[-1,1]}$ 区间。$\boldsymbol{\tilde{c_t}}$ 并不会全部刷新进入 LSTM 的 Memory，而是通过输入门控制接受输入的量。输入门的控制变量同样来自于输入 $\boldsymbol{x_t}$ 和输出 $\boldsymbol{h_{t - 1}}$：</p>
<script type="math/tex; mode=display">
 \boldsymbol{g_i  = \sigma(W_i[h_{t - 1};x_t]+b_i)}</script><p> 其中 $\boldsymbol{W_i}$ 和 $\boldsymbol{b_i}$ 为输入门的参数，可由反向传播算法自动优化，$\boldsymbol{\sigma}$ 为激活函数，一般使用 <strong>Sigmoid</strong> 函数。输入门控制变量 $\boldsymbol{g_i}$ 决定了 LSTM 对当前时间戳的新输入 $\boldsymbol{\tilde{c_t}}$ 的接受程度：当 $\boldsymbol{g_i = 0}$ 时，LSTM 不接受任何的新输入 $\boldsymbol{\tilde{c_t}}$；当 $\boldsymbol{g_i = 1}$ 时，LSTM 全部接受新输入 $\boldsymbol{\tilde{c_t}}$。经过输入门之后，待写入 Memory 的向量为 $\boldsymbol{g_i\tilde{c_t}}$。<br> &emsp;&emsp;在遗忘门和输入门的控制下，LSTM 有选择地读取了上一个时间戳的记忆 $\boldsymbol{c_t}$ 和当前时间戳的新输入 $\boldsymbol{\tilde{c_t}}$，状态向量 $\boldsymbol{c_t}$ 的刷新方式为：</p>
<script type="math/tex; mode=display">
\boldsymbol{c_t = g_i\tilde{c_t} + g_fc_{t-1}}</script><p>得到的新状态向量 $\boldsymbol{c_t}$ 即为当前时间戳的状态向量：<br><img src="https://img-blog.csdnimg.cn/ba7bb0ffcfea4cf2b9506c2215e19292.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-3-输出门"><a href="#2-3-输出门" class="headerlink" title="2.3    输出门"></a>2.3    输出门</h2><p>&emsp;&emsp;LSTM 的内部状态向量 $\boldsymbol{c_t}$ 并不会直接用于输出，这一点和基础的 RNN 不一样。标准的 RNN 网络的状态向量 $\boldsymbol{h_t}$ 既用于记忆，又用于输出，所以基础的 RNN 可以理解为状态向量 $\boldsymbol{c_t}$ 和输出向量 $\boldsymbol{h_t}$ 是同一个对象。</p>
<p><img src="https://img-blog.csdnimg.cn/5beb171ddbc049cdbf7f13e34a4e76a9.png#pic_center" alt="在这里插入图片描述"></p>
<p>在 LSTM 内部，状态向量并不会全部输出，而是在输出门的作用下有选择地输出。输出门的门控变量 $\boldsymbol{g_o}$： </p>
<script type="math/tex; mode=display">
 \boldsymbol{g_o= \sigma(W_o[h_{t - 1};x_t]+b_o)}</script><p> 其中 $\boldsymbol{W_o}$ 和 $\boldsymbol{b_o}$ 为输出门的参数，可由反向传播算法自动优化，$\boldsymbol{\sigma}$ 为激活函数，一般使用 <strong>Sigmoid</strong> 函数。当 $\boldsymbol{g_o = 0}$ 时输出关闭，LSTM 的内部记忆完全被隔断，无法用作输出，此时输出为 0 的向量；当 $\boldsymbol{g_o = 1}$ 时，输出完全打开，LSTM 的状态向量 $\boldsymbol{c_t}$ 全部用于输出。LSTM 的输出为：</p>
<script type="math/tex; mode=display">
\boldsymbol{h_t = g_o\cdot tanh(c_t)}</script><p>即内存向量 $\boldsymbol{c_t}$ 经过 <strong>tanh</strong> 激活函数后与输入门作用，得到 LSTM 的输出。由于 $\boldsymbol{ 𝒈_o ∈ [0,1]}$，$\boldsymbol{tanh(c_t) ∈ [-1,1]}$，因此 LSTM的输出 $\boldsymbol{h_t∈ [-1,1]}$。</p>
<h2 id="2-4-小结"><a href="#2-4-小结" class="headerlink" title="2.4    小结"></a>2.4    小结</h2><p>&emsp;&emsp;LSTM 虽然状态向量和门控数量较多，计算流程相对复杂。但是由于每个门控功能清晰明确，每个状态的作用也比较好理解。LSTM 的核心公式记录如下：</p>
<ul>
<li>遗忘门：$\boldsymbol{g_f = \sigma(W_f[h_{t- 1};x_t]+b_f)}$；</li>
<li>输入向量更新：$\boldsymbol{\tilde{c_t} = tanh(W_c[h_{t -1};x_t] +b_c)}$；</li>
<li>输入门： $\boldsymbol{g_i  = \sigma(W_i[h_{t - 1};x_t]+b_i)}$；</li>
<li>状态向量更新：$\boldsymbol{c_t = g_i\tilde{c_t} + g_fc_{t-1}}$；</li>
<li>输出门：$\boldsymbol{g_o= \sigma(W_o[h_{t - 1};x_t]+b_o)}$；</li>
<li>输出向量更新：$\boldsymbol{h_t = g_o\cdot tanh(c_t)}$。</li>
</ul>
<p>总的来说，可以总结三个门的输出值都是 $\boldsymbol{[0,1]}$ 之间，都是为了控制不同量的”多少”进入下一个 Cell。LSTM 有效地克服了传统 RNN 的一 些不足，比较好地解决了梯度消失、长期依赖等问题。不过，LSTM 也有一 些不足，如结构比较复杂、计算复杂度较高等问题。能否继续改进？</p>
<h1 id="3-GRU"><a href="#3-GRU" class="headerlink" title="3    GRU"></a>3    GRU</h1><p>&emsp;&emsp;针对 LSTM 的缺点，我们尝试简化 LSTM 内部的计算流程，特别是减少门控数量。研究发现，遗忘门是 LSTM 中最重要的门控，甚至发现只有遗忘门的简化版网络在多个基准数据集上面优于标准 LSTM 网络。其中，<strong>门控循环网络</strong>(Gated Recurrent Unit，简称 GRU)是应用最广泛的 RNN 变种之一，GRU 对 LSTM 做了很多简化，比 LSTM 少一个 Gate，因此，计算效率更高，占用内存也相对较少(这也是一件非常有意思的事情，GRU 比 LSTM 提出的更晚，却更简单，且效率更高)。在实际使用中，GRU 和 LSTM差异不大，因此，GRU最近变得越来越流行。GRU 对 LSTM 做了两个大改动：</p>
<ul>
<li>将内部状态向量与输出合并为一个状态：$\boldsymbol{h_t}$；</li>
<li>将输入门、遗忘门、输出门变为两个门：更新门(Update Gate)和重置门(Reset Gate)。</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/f649cb4bb6b54d9f985d00b7bbdaf6f4.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-1-复位门"><a href="#3-1-复位门" class="headerlink" title="3.1    复位门"></a>3.1    复位门</h2><p>&emsp;&emsp;复位门用于控制上一个时间戳的状态 $\boldsymbol{h_{t - 1}}$ 进入 GRU 的量。</p>
<p><img src="https://img-blog.csdnimg.cn/3ea836757e7141239136e880957e18ce.png#pic_center" alt="在这里插入图片描述"></p>
<p>门控向量 $\boldsymbol{g_r}$ 由当前时间戳输入 $\boldsymbol{x_t}$ 和上一时间戳状态  $\boldsymbol{h_{t-1}}$ 变换得到，关系如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{g_r = \sigma(W_r[h_{t-1};x_t]+b_r)}</script><p>其中 $\boldsymbol{W_r}$ 和 $\boldsymbol{b_r}$ 为复位门的参数，可由反向传播算法自动优化，$\boldsymbol{\sigma}$ 为激活函数，一般使用 <strong>Sigmoid</strong> 函数。门口向量 $\boldsymbol{g_r}$ 只控制 $\boldsymbol{h_{t-1}}$ ，而不会控制输入  $\boldsymbol{x_t}$,也就是说，输入会全部进入状态向量中：</p>
<script type="math/tex; mode=display">
\boldsymbol{\tilde{h_t} = tanh(W_h[g_rh_{t-1};x_t]+b_h)}</script><p>当 $\boldsymbol{g_r = 0}$ 时，新输入 $\boldsymbol{\tilde{h_t}}$ 全部来自于输入 $\boldsymbol{x_t}$，不接受 $\boldsymbol{h_{t-1}}$，此时相当于复位 $\boldsymbol{h_{t-1}}$。当 $\boldsymbol{g_r \not = 1}$ 时， $\boldsymbol{h_{t-1}}$ 和输入 $\boldsymbol{x_t}$ 共同产生新输入 $\boldsymbol{\tilde{h_t}}$。</p>
<h2 id="3-2-更新门"><a href="#3-2-更新门" class="headerlink" title="3.2    更新门"></a>3.2    更新门</h2><p>&emsp;&emsp;更新门用控制上一时间戳状态 $\boldsymbol{h_{t-1}}$ 和新输入 $\boldsymbol{\tilde{h_t}}$ 对新状态向量 $\boldsymbol{h_{t}}$ 的影响程度。</p>
<p><img src="https://img-blog.csdnimg.cn/49308e38251b42589857985c754dcd42.png#pic_center" alt="在这里插入图片描述"></p>
<p>更新门控向量 $\boldsymbol{g_z}$ 计算如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{g_z = \sigma(W_z[h_{t-1};x_t] + b_z)}</script><p>其中 $\boldsymbol{W_z}$ 和 $\boldsymbol{b_z}$ 为更新门的参数，可由反向传播算法自动优化，$\boldsymbol{\sigma}$ 为激活函数，一般使用 <strong>Sigmoid</strong> 函数。$\boldsymbol{g_z}$ 用与控制新输入 $\boldsymbol{\tilde{h_t}}$ 信号， $\boldsymbol{1 - g_z}$ 用于控制状态 $\boldsymbol{h_{t-1}}$ 信号：</p>
<script type="math/tex; mode=display">
\boldsymbol{h_t = g_z\tilde{h_t} + (1-g_z)h_{t-1}}</script><p>可以看到，$\boldsymbol{\tilde{h_t}}$ 和 $\boldsymbol{h_{t-1}}$ 对 $\boldsymbol{h_{t}}$ 的更新量处于相互竞争、此消彼长的状态。当更新门 $\boldsymbol{g_z = 0}$ 时，$\boldsymbol{h_{t}}$ 全部来自上一时间戳状态 $\boldsymbol{h_{t-1}}$；当更新门 $\boldsymbol{g_z =1}$ 时，$\boldsymbol{h_{t}}$ 全部来自新输入 $\boldsymbol{\tilde{h_t}}$。</p>
<h2 id="3-3-小结"><a href="#3-3-小结" class="headerlink" title="3.3    小结"></a>3.3    小结</h2><p>&emsp;&emsp;GRU 的核心公式总结如下：</p>
<ul>
<li>复位门：$\boldsymbol{g_r = \sigma(W_r[h_{t-1};x_t]+b_r)}$；</li>
<li>输入向量更新：$\boldsymbol{\tilde{h_t} = tanh(W_h[g_rh_{t-1};x_t]+b_h)}$；</li>
<li>更新门：$\boldsymbol{g_z = \sigma(W_z[h_{t-1};x_t] + b_z)}$；</li>
<li>状态向量更新：$\boldsymbol{h_t = g_z\tilde{h_t} + (1-g_z)h_{t-1}}$。</li>
</ul>
<p>能够发现，GRU 和 LSTM 的门控制向量的计算方式都一样，只不过 GRU 比 LSTM 更加简洁一些，只要按照 Cell 里面的结构一步一步进行推理，也是不难的。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习任务</title>
    <url>/2022/06/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h1 id="一、机器学习任务攻略"><a href="#一、机器学习任务攻略" class="headerlink" title="一、机器学习任务攻略"></a>一、机器学习任务攻略</h1><h2 id="1-1-Framework-of-ML"><a href="#1-1-Framework-of-ML" class="headerlink" title="1.1 Framework of ML"></a>1.1 Framework of ML</h2><p>  神经网络一共包含三个模块：训练模块、验证模块、预测模块。其中训练和验证模块共用数据是Training data，但要注意的是要把Training data分成训练和验证数据。<br>训练步骤包括三个步骤：<br>  1、要先写出一个有未知参数的函数f（x），x为input，也叫做feature<br>  2、定义Loss函数，输入为一组参数，计算这组参数所造成的误差<br>  3、定义optimization，找到一组最为合适的参数θ*，使得Loss最小<br>预测部分是用训练好的θ对Testing data进行预测结果。</p>
<h2 id="1-2-提高结果的准确性"><a href="#1-2-提高结果的准确性" class="headerlink" title="1.2 提高结果的准确性"></a>1.2 提高结果的准确性</h2><p>检查training data的Loss</p>
<h3 id="1-2-1-Loss过大"><a href="#1-2-1-Loss过大" class="headerlink" title="1.2.1 Loss过大"></a>1.2.1 Loss过大</h3><p>显然在训练资料上效果并不是很好，这时要从两方面出发检查：<br>1.model bias的原因：<br>  model过于简单，导致在神经网络的函数集中没有可以让Loss变的足够小的函数，就像在海中捞针<br>  解决方法：重新设计model让其更加深，鲁棒性更强<br>  1.增加输入的feature，或者使用Deep Learning<br>  <img src="https://img-blog.csdnimg.cn/fe38e307c8cc4583b1429e2ddb50f121.png" alt="在这里插入图片描述"><br>  <img src="https://img-blog.csdnimg.cn/728166a8620e4c77871276b7949dcbff.png" alt="在这里插入图片描述"><br>2.optimization做的不好<br> 在训练过程中可能卡到一个局部最优点（local minima）的地方，虽然网络中存在一个最好的函数，但是你无法找到让Loss最小的一组参数θ*<br> <img src="https://img-blog.csdnimg.cn/d9d0bbbebada476fb6ed815b5c7ffbfd.png#pic_center" alt="在这里插入图片描述"><br> 如何区分是model bias还是optimization的问题<br> 当一个神经网络的层数比另外一个神经网络要大，但Loss值却更大，则是optimization的问题，因为越深的网络的鲁棒性会更大；此外则是model bias的问题<br> <img src="https://img-blog.csdnimg.cn/091969533b60468e8be93969563b29a1.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="1-2-2-Loss足够小，但testing-data的Loss过大"><a href="#1-2-2-Loss足够小，但testing-data的Loss过大" class="headerlink" title="1.2.2 Loss足够小，但testing data的Loss过大"></a>1.2.2 Loss足够小，但testing data的Loss过大</h3><p>这里也要从两方面分析<br>1.over fitting<br>举一个极端的例子：<br>我们的testing data为：<br><img src="https://img-blog.csdnimg.cn/ad56387e9fb2422d9a9b7098bed4722d.png" alt="在这里插入图片描述"><br>通过training找出这样一个函数：<br><img src="https://img-blog.csdnimg.cn/d3bd1ecf77024e53b21edc839516b1b7.png#pic_center" alt="在这里插入图片描述"><br>当能够在训练资料中找到x的话就输出yi，否则就随机输出一个值。基于此，这个函数在training过程中的Loss为0，但是在test过程中就什么也没干，所以在testing过程中的Loss会很大，这就是over fitting。<br>在真实的实验过程中，往往是因为model的鲁棒性太强，导致在training后得到的模型有很大的“自由区”而不能很好的拟合testing数据<br><img src="https://img-blog.csdnimg.cn/98f7bbfa3dd943bcad15edc36787c35d.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/e709d8efa02f4d139940caf0e27d6c28.png#pic_center" alt="在这里插入图片描述"><br>导致Loss过大<br>解决方法：<br>1.增加数据量（Data augmentation）<br><img src="https://img-blog.csdnimg.cn/bd247ec02e7642d2992d7cd19fe47027.png#pic_center" alt="在这里插入图片描述"></p>
<p>2.限制model的鲁棒性<br>根据training的数据设定函数，少一些参数<br><img src="https://img-blog.csdnimg.cn/3b8b0f61c2ee40dfb5f1518cd5a8d6b6.png#pic_center" alt="在这里插入图片描述"><br>但要注意的是不能给model太多的限制，否则会导致model bias的问题<br>综上，我们可以得出model的复杂程度和Loss的一个非线性关系<br><img src="https://img-blog.csdnimg.cn/1564b47d4edb4778914f5d7175bd3d73.png#pic_center" alt="在这里插入图片描述"><br>当model越来越复杂时，虽然Training loss越来越小，但是可能会出现over fitting的问题，导致预测结果不准<br>我们在前面说过，我们有三大模块，训练、验证和预测，且将training data随机分成训练数据和验证数据，当我们在每一个epoch训练后得到的θ参数和loss，我们要将θ进行验证，如果验证组得到的loss更小，说明这组参数是较优的，将此loss替换训练得到的loss，最终找到一组最优解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dev_mse = dev(model, dev_data)</span><br><span class="line"><span class="keyword">if</span> dev_mse &lt; min_mse :</span><br><span class="line">   	min_mse = dev_mse</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dev</span>(<span class="params">model, dev_data</span>) :</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss = []</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, label <span class="keyword">in</span> dev_data :</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        total_loss.append(model.cal_loss(output, label))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(total_loss) / <span class="built_in">len</span>(total_loss)</span><br></pre></td></tr></table></figure>
<p>此外我们还可以用N折交叉验证的方法进行分数据和找θ*<br><img src="https://img-blog.csdnimg.cn/56cceab802cc4d93b098313d7c53a29d.png#pic_center" alt="在这里插入图片描述"><br>2.认为影响，这种问题不能怪model，机器是死的，这种问题一般来说叫做mismatch</p>
<h1 id="二、局部最小值（local-minima）与鞍点（saddle-point）"><a href="#二、局部最小值（local-minima）与鞍点（saddle-point）" class="headerlink" title="二、局部最小值（local minima）与鞍点（saddle point）"></a>二、局部最小值（local minima）与鞍点（saddle point）</h1><p>当optimization失败时一般有两点原因：<br>1.loss值无法再小，此时已经找到了一组最优解θ*<br>2.gradient等于0<br><img src="https://img-blog.csdnimg.cn/cf025c9accf44c99bfb743418a7b9b5d.png#pic_center" alt="在这里插入图片描述">这里我们对gradient==0的情况进行分析，当gradient等于0时，这个点有三种情况：<br>处于local minima，此时loss处于局部最小，没办法走到其他地方<br><img src="https://img-blog.csdnimg.cn/98b70267da1a402d9d966eb5d878d41f.png#pic_center" alt="在这里插入图片描述"><br>值得注意的是，local minima也有好坏之分当minima处于尖端的时候，testing loss是十分大的，而在平缓区时，testing loss就没那么大<br><img src="https://img-blog.csdnimg.cn/f38512838eb842be82f213ad4557aabe.png#pic_center" alt="在这里插入图片描述"></p>
<p>处于saddle point，此时loss并不是局部最小值，有别的地方可以走，这样就有机会到达全局最小点<br><img src="https://img-blog.csdnimg.cn/056c72263b5a4d2bb8483b56a787b92a.png#pic_center" alt="在这里插入图片描述"><br>Saddle Point vs Local Minima<br>他们两者之间谁更加常见呢？<br>如果在从二维空间去看的话Saddle Point就可能被认为是Local Minima<br><img src="https://img-blog.csdnimg.cn/dfdd51cc86e743838559786628dcbf56.png#pic_center" alt="在这里插入图片描述"><br>反过来假如我们从更高维度去看的话Local Minima却是Saddle Point<br><img src="https://img-blog.csdnimg.cn/1c32aaf3bda34b058cd818e323cb639c.png#pic_center" alt="在这里插入图片描述"><br>也就是说假如我们在某个维度没路可走的时候，我们可以提高维度来使其变成Saddle Point从而有路可走。这就是现在为什么神经网络如此复杂、参数众多的重要原因</p>
<h1 id="三、Batch和momentum"><a href="#三、Batch和momentum" class="headerlink" title="三、Batch和momentum"></a>三、Batch和momentum</h1><h2 id="3-1-Batch"><a href="#3-1-Batch" class="headerlink" title="3.1 Batch"></a>3.1 Batch</h2><p>现在假如说有一笔N资料，我们可以把N笔资料一次性全部跑完再计算loss和更新一次参数θ，但是的话我们也可以将N笔资料分成许多个batch资料，我们每跑完一个batch资料就计算更新一次参数<br><img src="https://img-blog.csdnimg.cn/91949cf989944b53a2ce9d290e0a4d0c.png#pic_center" alt="在这里插入图片描述"><br>1 epoch等于把全部的batch都看一遍<br>为什么要用batch？<br>现在有20笔资料，我们分别看看用和不用batch的参数更新效果<br><img src="https://img-blog.csdnimg.cn/2941be02cc9e4db5ae7b0e54028b59a6.png#pic_center" alt="在这里插入图片描述"><br>可以看出没有用batch的model的蓄力时间比较长，但每走一步都比较稳；用了batch的model蓄力时间短，但每次走的时候方是十分乱的。但是如果考虑gpu的平行预算，没有用batch耗费的时间不一定比用了batch所花时间长。在MNIST机器学习任务（一共有六万笔资料）中：<br>跑每一个不同batch所用时间如下，（6000代表不用batch）<br><img src="https://img-blog.csdnimg.cn/0d9369eca1ad471389b3fd91dc1d1430.png#pic_center" alt="在这里插入图片描述"><br>我们再看跑完一个epoch和跑完一个batch所需时间对比：<br><img src="https://img-blog.csdnimg.cn/028e4aa0dd214a7298b60a30dcb8d0ad.png#pic_center" alt="在这里插入图片描述"><br>我们可以看出一个epoch大的batch花的时间反而是比较少的<br>但是与我们直觉不同的是，分了batch的任务最终预测的正确率是比没分batch是要高的<br><img src="https://img-blog.csdnimg.cn/3038616128e843e6bf6037f537bba320.png#pic_center" alt="在这里插入图片描述"><br>为什么分batch会带来更好的结果？<br>我们先来考虑Full batch的情况<br>我们沿着Loss函数来更新参数，当陷入一个local minima之后就停止更新参数<br><img src="https://img-blog.csdnimg.cn/575bfa93f08d475d8e346d92c3992239.png#pic_center" alt="在这里插入图片描述"><br>再考虑small batch时，当我们用θ1参数组来算gradient的时候，他的loss函数是L1当遇到gradient等于0时就卡住了，但不会停止更新函数，可以用下一个batch来train优化model<br><img src="https://img-blog.csdnimg.cn/924a99ef89964f72a4700479a13f88b4.png#pic_center" alt="在这里插入图片描述"><br>Small Batch VS Large Batch<br><img src="https://img-blog.csdnimg.cn/7f28ab5d30a54fb5858ddbc5cf5cd6fa.png#pic_center" alt="在这里插入图片描述"><br>正因他们有各自的优点，则Batch size变成一个hyper parameter（超参数）</p>
<h2 id="3-2-Momentum"><a href="#3-2-Momentum" class="headerlink" title="3.2 Momentum"></a>3.2 Momentum</h2><p>momentum是一门可能可以对抗local minima的技术。他的概念可以想象成物理世界中的惯性，我们可以想象，当一个小球沿着loss函数走，当走到local minima时因为他有惯性而不会在minima处停下<br><img src="https://img-blog.csdnimg.cn/4ec31491daeb49c6bb65ae02b0b072fa.png#pic_center" alt="在这里插入图片描述"><br>对于普通的gradient descent，假设开始在θ0的点，计算该点gradient为g(0)，沿着gradient的反方向移动到θ1 =   θ0 - ng(0) 然后计算g(1)…..<br><img src="https://img-blog.csdnimg.cn/4a865146cedb4ad9a1df22327b4f4207.png#pic_center" alt="在这里插入图片描述"><br>结合momentum的gradient descent<br>我们在更新gradient时不单单沿着gradient的反方向，还要加上前一步(momentum)的方向进行更新，类似于物理中的力的合成，计算过程如下图：<br><img src="https://img-blog.csdnimg.cn/2be951e89eed4036ac7c7ab29915a82f.png#pic_center" alt="在这里插入图片描述"><br>当我们走到local minima和和saddle point时虽然gradient==0，但是由于我们还有前一步的momentum所以还会继续走下去<br><img src="https://img-blog.csdnimg.cn/9419e31cc1414edbb95afd1a247d221b.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="四、结语"><a href="#四、结语" class="headerlink" title="四、结语"></a>四、结语</h1><p>以上是我机器学习学习笔记的第二篇，如有不对之处，还望指出，与君共勉。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习概述</title>
    <url>/2022/06/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="一、机器学习基本概念简介"><a href="#一、机器学习基本概念简介" class="headerlink" title="一、机器学习基本概念简介"></a>一、机器学习基本概念简介</h1><h2 id="1-1机器学习的定义"><a href="#1-1机器学习的定义" class="headerlink" title="1.1机器学习的定义"></a>1.1机器学习的定义</h2><p>我们所要求机器所能为我们做的事情均离不开两个最要的模块：输入和输出。比如对于无人驾驶来说，汽车必须根据路段信息来决定车辆的行驶，在此过程中，路段信息就是输入，车俩的行驶就是输出，但问题是他是怎么让输入变成输出的呢。而这就是机器学习所需要做的事情：寻找一个函数根据输入而输出合理的操作，表示为f（输入）—&gt; 输出，这也是机器学习的定义。</p>
<h2 id="1-2函数的分类"><a href="#1-2函数的分类" class="headerlink" title="1.2函数的分类"></a>1.2函数的分类</h2><p>在机器学习中我们知道其定义是找一个函数来根据输入做出合理的输出，在这里介绍两种常见函数：<br>1.Regression（回归函数）：其输出是一个数值。如我们需要预测未来的PM2.5的浓度，当天的PM2.5浓度、气温、臭氧的浓度为输入然后经过一个回归函数来预测明天的PM2.5浓度。<br><img src="https://img-blog.csdnimg.cn/9489ec10768842349149893e675bc924.PNG#pic_center" alt="在这里插入图片描述"><br>2.Classification（分类函数）：其输出为一个类别。如我们在收到邮件时，常常有些骚扰或者垃圾邮件，这些邮件便是输入，经过分类函数可以将这些归类到“垃圾邮件”类别之中<br><img src="https://img-blog.csdnimg.cn/5b0de484b4574e60be7847a10579f8d1.PNG#pic_center" alt="在这里插入图片描述"></p>
<h2 id="1-3如何寻找函数"><a href="#1-3如何寻找函数" class="headerlink" title="1.3如何寻找函数"></a>1.3如何寻找函数</h2><h3 id="1-3-1定义函数"><a href="#1-3-1定义函数" class="headerlink" title="1.3.1定义函数"></a>1.3.1定义函数</h3><p>根据之前视频播放资讯预测未来的播放量即其他比例<br><img src="https://img-blog.csdnimg.cn/65f158ba7833419cb252a913ca1b31c5.PNG#pic_center" alt="在这里插入图片描述"><br>函数的输入是前一天的资讯，输出是今天的播放量的预测值。我们可以把函数设成一次函数:<br>y = b +wx1 其中b我们称为偏置参数bias，w为权重参数weight。x1是前一天的播放量，是我们所知道的，称作feature，w和b是未知的，需要我们通过学习得到的，而这样的一个带有未知参数的函数我们称作Model（模型）</p>
<h3 id="1-3-2定义Loss函数"><a href="#1-3-2定义Loss函数" class="headerlink" title="1.3.2定义Loss函数"></a>1.3.2定义Loss函数</h3><p>Loss函数的输入是我们在1.31所定义函数的未知参数（parameters）即：L(b, w)，输出我们预测的数据更跟实际数据的差别。如：<br>令L=L(500, 1)，则y = b + wx1 -&gt; y = 500 + x1 假设有下面时间的播放量：<br><img src="https://img-blog.csdnimg.cn/af94ddc9002e41c996fe40936249bde8.PNG#pic_center" alt="在这里插入图片描述"><br>对于2017/1/2这一天，当b=500，w=1时，x1等于2017/1/1的播放量根据所设函数计算y=500+4800 = 5300，即我们预测01/02号这天的播放量是5300，但从开始的资讯中我们可知实际这天的实际播放量是4900，则en=abs（5300 - 4900）。同样的方法，我们都可以算出每一天的误差。则最终的Loss = 1/N ∑en，N表示有N份学习资料。我们也可以推出在不同b和w值下Loss是不一样的，而机器要做的就是寻找一组最合适的b，w值从而使Loss最小，提高预测的精确度</p>
<h3 id="1-3-3定义Optimization（优化函数）"><a href="#1-3-3定义Optimization（优化函数）" class="headerlink" title="1.3.3定义Optimization（优化函数）"></a>1.3.3定义Optimization（优化函数）</h3><p>在1.3.2中我们知道要寻找一组最合适的b，w值的过程就是不断优化未知函数的过程，而最常用的方法就是梯度下降法（Gradient Descent）</p>
<h4 id="1-3-3-1假设只有w需要优化时"><a href="#1-3-3-1假设只有w需要优化时" class="headerlink" title="1.3.3.1假设只有w需要优化时"></a>1.3.3.1假设只有w需要优化时</h4><p>w<em> = arg minLoss，假设Loss函数与w关系如下：<br><img src="https://img-blog.csdnimg.cn/aff76bb9336e4f9aaa97957a127fe402.PNG#pic_center" alt="在这里插入图片描述"><br>首先随机找一个w的初始值w0，计算在w0的L对w的微分∂L/∂w（w=w0）也就是斜率：<br><img src="https://img-blog.csdnimg.cn/1b64a43477f1485a842770f58a9642fa.PNG?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>从图中我们可以观察到此偏导是负的，所以在w0左边的虚线比较高，右边比较低，这样的话我们可以提高w的值来使Loss值减小（偏导如果是正的则相反），有：<br><img src="https://img-blog.csdnimg.cn/0027698ef8a1475d81f5d91cd2797179.PNG#pic_center" alt="在这里插入图片描述"><br>在图中又有了一个新的参数为η，我们称作学习率（Learing rate），η值越大，w值更新越快，越小则w值更新越慢。而η是需要我们在实验过程中手动设置的参数，我们将这一类参数叫做超参数（hyperparameters），由图我们也能得出w与Loss的微分关系：w1-w0=η</em>∂L/∂w（w=w0），以此方法训练数据得到Loss的最小值，此时gradient为0<br>同样，b的值也是以这种方法不断更新</p>
<h4 id="1-3-3-2将b，w的方向组合在一起时"><a href="#1-3-3-2将b，w的方向组合在一起时" class="headerlink" title="1.3.3.2将b，w的方向组合在一起时"></a>1.3.3.2将b，w的方向组合在一起时</h4><p>w<em>，b</em> = arg minLoss，可视化后：<br><img src="https://img-blog.csdnimg.cn/e32ed89a3b7c444c82360f61684c7b1d.PNG?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>当w<em> = 0.97（接近1），b</em> = 100（一般设的比较小）Loss值达到最小480，因此我们拿出这组数据来预测一下;<br><img src="https://img-blog.csdnimg.cn/2bce80fcb60f452f93c8975b1f9493c1.PNG#pic_center" alt="在这里插入图片描述"><br>可以看出大致的方向是能够拟合的，但是预测的播放量的低估总是比实际的出现晚一两天，这可能出现的原因是我们在预测是只用前一天的数据就行预测，数据量太少，因此我们就要修改我们的Model。</p>
<h2 id="1-4函数的修改"><a href="#1-4函数的修改" class="headerlink" title="1.4函数的修改"></a>1.4函数的修改</h2><h3 id="1-4-1取多天的资讯进行训练"><a href="#1-4-1取多天的资讯进行训练" class="headerlink" title="1.4.1取多天的资讯进行训练"></a>1.4.1取多天的资讯进行训练</h3><p>考虑7天时，函数由y = b + wx1 变成y = b + ∑wjxj（j从1到7）每一天的播放量都乘上对应的w值，Loss值变化及对应的b，w值如下：<br><img src="https://img-blog.csdnimg.cn/4ade2987fd394efe911f7acab9c6aa40.PNG#pic_center" alt="在这里插入图片描述"><br>L表示训练数据上的损失，L‘表示预测未知数据的损失。随着参考天数的增加，Loss值实在减小，而表格中w有正由负，负表示前一天的播放量与我们预测的那天的播放量是成反比的，从函数的表达式我们也能得到这个结论，w为正则相反。<br>当我们继续考虑28天和56天时：<br><img src="https://img-blog.csdnimg.cn/34e259fe7ed3404a872b6e7d992ffd7c.PNG#pic_center" alt="在这里插入图片描述"><br>我们发现考虑天数对Loss值的影响已经变化不大，因此这应该是极限了。<br>而以上我们所设和修改后的函数模型称作线性模型（Linear Models）</p>
<h1 id="二、深度学习基本概念简介"><a href="#二、深度学习基本概念简介" class="headerlink" title="二、深度学习基本概念简介"></a>二、深度学习基本概念简介</h1><p>前言：Linear Models对我们解决应用场景问题来说太过于简单了，因为Linear Medols永远都只是一条直线，你可以通过修改w值来改变斜率和修改b来修改斜距，但是其永远是直线。<img src="https://img-blog.csdnimg.cn/002df87309e540aa8f12385a1e8dfbcc.PNG#pic_center" alt="在这里插入图片描述"><br>而我们需要的可能是一段斜率是正的，而一段是负的，此时线性模型就永远不能满足这一点，显然Linear Models有着许多的限制，而这种来自于Models的限制叫做Model Bias，注意不是我们上面所设函数中的b参数。因此，我们需要更加有弹性的函数来实现图中红色线段。</p>
<h2 id="2-1sigmoid函数定义"><a href="#2-1sigmoid函数定义" class="headerlink" title="2.1sigmoid函数定义"></a>2.1sigmoid函数定义</h2><p>上述的红色曲线可以由一个常数项加上一堆s型折线的和来实现<br><img src="https://img-blog.csdnimg.cn/76504b95a0b948a7bf189386c5fa4f1e.PNG#pic_center" alt="在这里插入图片描述"><br>从0，1，2，3蓝色曲线分别取与红色曲线相对应的部分即可构成红色曲线。依照这种方法，无论所求的曲线有多复杂，折点有多多，我们都可以用一个常数项加不同数的s性折线构成。而要写出s性折线的函数表达式并不是很容易，所以我们可以用一条光滑的曲线去逼近它，这个曲线函数就是sigmoi函数：<br><img src="https://img-blog.csdnimg.cn/46b187fd0c714980a6a23489ade209fb.PNG#pic_center" alt="在这里插入图片描述"><br>从sigmoid函数表达式中y = c<em>[1 / (1 + e-(b + wx1))]（注意表达式中的b是Models bias）看出当x1非常大是函数值会收敛到c的位置，当x1非常小时，函数值会收敛到0。sigmoid函数可写成y = c </em> sigmoid(b + wx1)</p>
<h2 id="2-2sigmoid函数如何逼近各种线段"><a href="#2-2sigmoid函数如何逼近各种线段" class="headerlink" title="2.2sigmoid函数如何逼近各种线段"></a>2.2sigmoid函数如何逼近各种线段</h2><p>1.改变w值可以改变sigmoid函数图像的斜率<br>2.改变b的值可以让sigmoid函数图像左右移动<br>3.修改c的值可以改变sigmoid函数图像的高度<br><img src="https://img-blog.csdnimg.cn/03b2d7840d4b486196814d3214cf30c1.PNG#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-3建立更加弹性的函数"><a href="#2-3建立更加弹性的函数" class="headerlink" title="2.3建立更加弹性的函数"></a>2.3建立更加弹性的函数</h2><h3 id="2-3-1更多的Model-Features"><a href="#2-3-1更多的Model-Features" class="headerlink" title="2.3.1更多的Model Features"></a>2.3.1更多的Model Features</h3><p>当我们用很多的sigmoid函数去形成一条复杂的函数图像时，每一个sigmoid函数中参数c，b，w都是不一样的。<br>当我们只用一天的数据去预测未来的播放量时，函数可变成<br><img src="https://img-blog.csdnimg.cn/27632afc2bc14869b3bbe0b553d5afbc.PNG#pic_center" alt="在这里插入图片描述"><br>其中i表示sigmoid函数的个数<br>当用多天数据时，函数可变成<br><img src="https://img-blog.csdnimg.cn/f5100617cf51465184fd30f6169bd1f7.PNG#pic_center" alt="在这里插入图片描述"><br>其中i代表sigmoid函数个数，j代表天数，xj表示第前j天的播放量</p>
<h3 id="2-3-2sigmoid函数的计算方式"><a href="#2-3-2sigmoid函数的计算方式" class="headerlink" title="2.3.2sigmoid函数的计算方式"></a>2.3.2sigmoid函数的计算方式</h3><p>当j：1，2，3；i：1，2，3时，计算图如下：<br><img src="https://img-blog.csdnimg.cn/1b40be5955784d94bd03e3334c4c334e.PNG#pic_center" alt="在这里插入图片描述"></p>
<p>其中x1，x2，x3表示该天的播放量，wij表示乘给xj的播放量的weight。把b1+w11x1+w12x2+w13x3相加送到第一个sigmoid函数中计算，第二、三个sigmoid函数也是这样计算。为了简化计算过程，我们可以用矩阵的方法来计算，令<br>r1=b1+w11x1+w12x2+w13x3<br>r2=b2+w21x1+w22x2+w23x3<br>r3=b3+w31x1+w32x2+w33x3<br><img src="https://img-blog.csdnimg.cn/8ecba25138e74a69b0ef9ee6bd8b0f12.PNG#pic_center" alt="在这里插入图片描述"><br>再令a=sigmoid（r）=1 / （1 + e-r），再乘每个sigmoid函数的c再相加有!<br><img src="https://img-blog.csdnimg.cn/8fd8aef351f74ef7a988e3de65a8428a.PNG#pic_center" alt="在这里插入图片描述"><br>如果将三个sigmoid函数的过程整合成矩阵计算的话，如下：<br><strong>r</strong> = <strong>b</strong> + <strong>wx</strong><br><strong>a</strong> = sigmoid（<strong>r</strong>）<br><strong>y</strong> = <strong>b</strong> + <strong>c</strong>T（<strong>c</strong>矩阵的转置）<strong>a</strong><br>即 <strong>y</strong> = <strong>b</strong> + <strong>c</strong>T sigmoid（<strong>b</strong> + <strong>wx</strong>）</p>
<h2 id="2-4新Loss"><a href="#2-4新Loss" class="headerlink" title="2.4新Loss"></a>2.4新Loss</h2><p>在<strong>y</strong> = <strong>b</strong> + <strong>c</strong>T sigmoid（<strong>b</strong> + <strong>wx</strong>）中，我们将所有矩阵的每一列或者每一行整合在一起得到一个大的矩阵<strong>θ</strong><br><img src="https://img-blog.csdnimg.cn/8e38edc5d096454f93346448395cbf52.PNG#pic_center" alt="在这里插入图片描述"></p>
<p>所以Loss函数每一组的参数可以用<strong>L</strong>(θ)来表示，计算方法跟只有w，b时是一样的，只不过现在可能是几百，几千个参数</p>
<h2 id="2-5新Optimization"><a href="#2-5新Optimization" class="headerlink" title="2.5新Optimization"></a>2.5新Optimization</h2><p>计算方法参数的更新方法跟只有w，b时是一样的<br><strong>θ*</strong> = arg minLoss   <strong>θ</strong> = [θ1 θ2 θ3…]T</p>
<h2 id="2-6batch与epoch"><a href="#2-6batch与epoch" class="headerlink" title="2.6batch与epoch"></a>2.6batch与epoch</h2><p>当我们有一笔N资料时，要去计算Loss时，我们可以将N笔资料分成M份，每一份有N/M笔资料，N/M笔资料就叫做一个batch，而我们可以先计算每一个batch的Loss’，然后更新参数<strong>θ</strong>，计算出gradient。当所有batch都看过一遍之后叫做epoch</p>
<h2 id="2-7ReLU（Rectified-Linear-Unit）函数"><a href="#2-7ReLU（Rectified-Linear-Unit）函数" class="headerlink" title="2.7ReLU（Rectified Linear Unit）函数"></a>2.7ReLU（Rectified Linear Unit）函数</h2><p>ReLU是现在深度学习最常用的激活函数，表达式为<br>y = b + ∑ci max(0, ∑wijxj)<br>可以看出ReLU是分段函数，当∑wijxj &gt; 0时，y = ∑wijxj，当∑wijxj &lt;= 0时，y = 0 图像为<br><img src="https://img-blog.csdnimg.cn/3a17d89f025143858c181a7628f8b49e.PNG#pic_center" alt="在这里插入图片描述"><br>且可以看出两个ReLU函数才能组成一个sigmoid函数<br><img src="https://img-blog.csdnimg.cn/8ba370b8b6a5400283cc69923033282d.png" alt="在这里插入图片描述"><br>它们都是激活函数，一般来说ReLU的拟合效果比sigmoid函数好，因此ReLU更加常用</p>
<h2 id="2-8多层网络及深度学习的定义"><a href="#2-8多层网络及深度学习的定义" class="headerlink" title="2.8多层网络及深度学习的定义"></a>2.8多层网络及深度学习的定义</h2><p>之前所有的讨论都是只经过一次激活函数，但是我们可以通过多层的激活函数进行预测结果，一层激活函数的输出可以作为下一层的输入，所以层与层之间的参数是不相同的<br><img src="https://img-blog.csdnimg.cn/990b0409cf6f4b30a20cd899bed23bc3.png" alt="在这里插入图片描述"><br>注意两个<strong>a</strong>向量是不同的<br>在预测我们一开始的视频播放量时，当经过一百层的ReLU函数后，Loss与预测和实际播放量的拟合程度如下图：<br><img src="https://img-blog.csdnimg.cn/0e781da4e8cd4c20bda10c4c463540ae.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/c979016206b8436a986642124bda9aed.png" alt="在这里插入图片描述"><br>在上面的许多层的ReLU或者sigmoid，被称作Neuron（神经元），很多的Neuron就叫做Neural Network，许多许多的隐藏层就叫做Deep，而这一套分析计算技术就叫做Deep Learning</p>
<h1 id="三、后话"><a href="#三、后话" class="headerlink" title="三、后话"></a>三、后话</h1><p>此系列文章是我学习深度学习的一些笔记，可能过程中有些错误，欢迎大家指正，不胜感激！与君共勉</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>生成对抗网络（GAN）</title>
    <url>/2022/06/05/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%EF%BC%88GAN%EF%BC%89/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&emsp;&emsp;在生成对抗网络(Generative Adversarial Network，简称 GAN)发明之前，变分自编码器(VAE)被认为是理论完备，实现简单，使用神经网络训练起来很稳定，生成的图片逼近度也较高，但是人眼还是可以很轻易地分辨出真实图片与机器生成的图片。但在2014年GAN被提出之后，在之后的几年里面里迅速发展，生成的图片越来越逼真。</p>
<h1 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1    GAN"></a>1    GAN</h1><h2 id="1-1-相关介绍"><a href="#1-1-相关介绍" class="headerlink" title="1.1    相关介绍"></a>1.1    相关介绍</h2><p>&emsp;&emsp;GAN模型的核心思想就是博弈思想，是生成器(造假者)和判别器(鉴别者)之间的博弈，在提出GAN的原始论文中，作者举了货币制造的例子。即像一台验钞机和一台制造假币的机器之间的博弈，两者不断博弈，博弈的结果假币越来越像真币，直到验钞机无法识别一张货币是假币还是真币为止。</p>
<h2 id="1-2-原理"><a href="#1-2-原理" class="headerlink" title="1.2    原理"></a>1.2    原理</h2><h3 id="1-2-1-网络架构"><a href="#1-2-1-网络架构" class="headerlink" title="1.2.1    网络架构"></a>1.2.1    网络架构</h3><p>&emsp;&emsp;生成对抗网络包含了两个子网络：生成网络(Generator，简称 G)和判别网络(Discriminator，简称 D)，其中生成网络 G 负责学习样本的真实分布，判别网络 D 负责将生成网络采样的样本与真实样本区分开来。<br>&emsp;&emsp;<strong>生成网络G(𝒛)</strong> ：生成网络 G 和自编码器的 Decoder 功能类似，从先验分布$p_z$(∙)采样获得潜在空间点向量，经过网络生成图片样本$\bar{x}$~$𝑝_𝑔(x|z)$。<img src="https://img-blog.csdnimg.cn/f0a07617c2ce41ada27a39cfbcf0b1f6.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;生成器的网络($𝑝_𝑔(x|z)$)可以由深度神经网络来参数化，如：卷积网络和转置卷积网络。下图中从均匀分布$𝑝𝒛$(∙)中采样出隐藏变量$z$，经过多层转置卷积层网络参数化的$𝑝_𝑔(x|z)$分布中采样出样本$x_f$，从输入输出层面来看，生成器 G 的功能是将隐向量𝒛通过神经网络转换为样本向量$x_f$，下标𝑓代表假样本(Fake samples)。<img src="https://img-blog.csdnimg.cn/f74001c39bfa414ea921a2420f4f96f8.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;<strong>判别网络D(𝒙)</strong>：判别网络和普通的二分类网络功能类似，网络的输入数据集由采样自真实数据分布$p_𝑟$(∙)的样本$x_𝑟$ ~ $𝑝_𝑟$(∙)和采样自生成网络的假样本$x_𝑓$ ~ $𝑝_𝑔(x|z)$组成。判别网络输出为$x$属于真实样本的概率𝑃($x$为真|$x$)，我们把所有真实样本$x_r$的标签标注为真(1)，所有生成网络产生的样本，所有生成网络产生的样本$x_f$标注为假(0)，通过最小化判别网络 D 的预测值与标签之间的误差来优化判别网络参数。<img src="https://img-blog.csdnimg.cn/e986cb9be6344b68889460f74812b572.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="1-2-2-网络训练"><a href="#1-2-2-网络训练" class="headerlink" title="1.2.2    网络训练"></a>1.2.2    网络训练</h3><p>&emsp;&emsp;GAN 博弈学习的思想体现在在它的训练方式上，由于生成器 G 和判别器 D 的优化目标不一样，不能和之前的网络模型的训练一样，只采用一个损失函数。所以我们要分别对生成器和判别器进行训练。<br>&emsp;&emsp;<strong>判别网络D(𝒙)</strong>：它的目标是能够很好地分辨出真样本$x_r$与假样本$x_f$。则其损失函数既要考虑识别真图像能力，又要考虑识别假图像能力，而不能只考虑一方面，故判别器的损失函数为两者的和。因此 D 的分类问题是二分类问题，以图片生成来说，交叉熵损失函数定义为：<img src="https://img-blog.csdnimg.cn/73d5853d0a524cddb3e665e367b22d1d.png#pic_center" alt="在这里插入图片描述"><br>因此判别网络 D 的优化目标是：<br><img src="https://img-blog.csdnimg.cn/c0b19fec09f74f4890fae16f63799ac0.png#pic_center" alt="在这里插入图片描述"><br>将最小化转成最大化的问题并写成期望的形式：<br><img src="https://img-blog.csdnimg.cn/550fc7f16cf543a8b1f2c449fce08d90.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;具体代码如下：D表示判别器、G为生成器、real_labels、fake_labels分别表示真图像标签、假图像标签。images是真图像，z是从潜在空间随机采样的向量，通过生成器得到假图像。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义判断器对真图像的损失函数 </span></span><br><span class="line">outputs = D(images)</span><br><span class="line">d_loss_real = criterion(outputs, real_labels) </span><br><span class="line">real_score = outputs </span><br><span class="line"><span class="comment"># 定义判别器对假图像（即由潜在空间点生成的图像）的损失函数 </span></span><br><span class="line">z = torch.randn(batch_size, latent_size).to(device) </span><br><span class="line">fake_images = G(z) </span><br><span class="line">outputs = D(fake_images) </span><br><span class="line">d_loss_fake = criterion(outputs, fake_labels) </span><br><span class="line">fake_score = outputs </span><br><span class="line"><span class="comment"># 得到判别器总的损失函数 </span></span><br><span class="line">d_loss = d_loss_real + d_loss_fake</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;<strong>生成网络G(𝒛)</strong> ：我们希望$x_f$ = 𝐺(𝒛)能够很好地骗过判别网络 D，假样本$x_f$在判别网络的输出越接近真实的标签越好。也就是说，在训练生成网络时，希望判别网络的输出𝐷(𝐺(𝒛))越逼近 1 越好，最小化𝐷(𝐺(𝒛))与 1 之间的交叉熵损失函数：<img src="https://img-blog.csdnimg.cn/9baccb14957b4738b4a1232a959761b7.png#pic_center" alt="在这里插入图片描述"><br>将最小化转成最大化的问题并写成期望的形式：<img src="https://img-blog.csdnimg.cn/0c4e02351e04459da5825f9d60edce2e.png#pic_center" alt="在这里插入图片描述"><br>等价成：<br><img src="https://img-blog.csdnimg.cn/9774d234ecff4d1e978ef7046ac875dc.png#pic_center" alt="在这里插入图片描述"><br>其中𝜙为生成网络 G 的参数集，可以利用梯度下降算法来优化参数𝜙。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.randn(batch_size, latent_size).to(device) </span><br><span class="line">fake_images = G(z) </span><br><span class="line">outputs = D(fake_images) </span><br><span class="line">g_loss = criterion(outputs, real_labels)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;通过对生成器和判别器的损失函数的求解，GAN的架构如下：<br><img src="https://img-blog.csdnimg.cn/46f8a03977c44c909c48c030a38e18ff.png#pic_center" alt="在这里插入图片描述"><br>算法流程为：<br><img src="https://img-blog.csdnimg.cn/73987cbfaf8c46d48437cabfe0d417bd.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="1-3-用GAN生成图像"><a href="#1-3-用GAN生成图像" class="headerlink" title="1.3    用GAN生成图像"></a>1.3    用GAN生成图像</h2><p>&emsp;&emsp;本次实验为了方便，我使用的是 MNIST 手写数字数据集，下面进行每部分的代码实现。</p>
<h3 id="1-3-1-判别器"><a href="#1-3-1-判别器" class="headerlink" title="1.3.1    判别器"></a>1.3.1    判别器</h3><p>&emsp;&emsp;定义判别器网络结构，这里使用LeakyReLU为激活函数，输出一个节点 并经过Sigmoid后输出，用于真假二分类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) :</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.D = nn.Sequential(nn.Linear(IMAGE_SIZE, HIDDEN_SIZE),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Linear(HIDDEN_SIZE, <span class="number">1</span>),</span><br><span class="line">                               nn.Sigmoid())</span><br></pre></td></tr></table></figure>
<h3 id="1-3-2-生成器"><a href="#1-3-2-生成器" class="headerlink" title="1.3.2    生成器"></a>1.3.2    生成器</h3><p>&emsp;&emsp;生成器与AVE的生成器类似，不同的地方是输出为nn.tanh，使用nn.tanh 将使数据分布在[-1,1]之间。其输入是潜在空间的向量z，输出维度与真图像相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.G = nn.Sequential(nn.Linear(Z_SIZE, HIDDEN_SIZE),</span><br><span class="line">                               nn.ReLU(),</span><br><span class="line">                               nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),</span><br><span class="line">                               nn.ReLU(),</span><br><span class="line">                               nn.Linear(HIDDEN_SIZE, IMAGE_SIZE),</span><br><span class="line">                               nn.Tanh())</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>) :</span><br><span class="line">        <span class="keyword">return</span> self.G(z)</span><br></pre></td></tr></table></figure>
<h3 id="1-3-3-训练模型"><a href="#1-3-3-训练模型" class="headerlink" title="1.3.3    训练模型"></a>1.3.3    训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPOCH) :</span><br><span class="line">    <span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(Dataloader) :</span><br><span class="line"></span><br><span class="line">        images = images.reshape(BATCH_SIZE, -<span class="number">1</span>).cuda()</span><br><span class="line">        <span class="comment">#真样本与生成样本的标签设置</span></span><br><span class="line">        real_labels = torch.ones(BATCH_SIZE, <span class="number">1</span>).cuda()</span><br><span class="line">        fake_labels = torch.zeros(BATCH_SIZE, <span class="number">1</span>).cuda()</span><br><span class="line">        <span class="comment">#训练判别器</span></span><br><span class="line">        d_optimizer.zero_grad()</span><br><span class="line">        g_optimizer.zero_grad()</span><br><span class="line">        out = D(images)</span><br><span class="line">        real_score = out</span><br><span class="line">        d_loss_real = criterion(out, real_labels)</span><br><span class="line"></span><br><span class="line">        z = torch.randn(BATCH_SIZE, Z_SIZE).cuda()</span><br><span class="line">        fake_images = G(z)</span><br><span class="line">        out = D(fake_images)</span><br><span class="line">        fake_score = out</span><br><span class="line">        d_loss_fake = criterion(out, fake_labels)</span><br><span class="line"></span><br><span class="line">        d_loss = d_loss_fake + d_loss_real</span><br><span class="line"></span><br><span class="line">        d_loss.backward()</span><br><span class="line">        d_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#训练生成器</span></span><br><span class="line">        d_optimizer.zero_grad()</span><br><span class="line">        g_optimizer.zero_grad()</span><br><span class="line">        z = torch.randn(BATCH_SIZE, Z_SIZE).cuda()</span><br><span class="line">        fake_images = G(z)</span><br><span class="line">        out = D(fake_images)</span><br><span class="line">        g_loss = criterion(out, real_labels)</span><br><span class="line"></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        g_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], d_loss: &#123;:.4f&#125;, g_loss: &#123;:.4f&#125;, D(x): &#123;:.2f&#125;, D(G(z)): &#123;:.2f&#125;&#x27;</span></span><br><span class="line">                  .<span class="built_in">format</span>(epoch, MAX_EPOCH, i + <span class="number">1</span>, <span class="built_in">len</span>(Dataloader), d_loss.item(), g_loss.item(),</span><br><span class="line">                          real_score.mean().item(), fake_score.mean().item()))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存真图片</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) == <span class="number">1</span>:</span><br><span class="line">            images = images.reshape(images.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">            save_image(denorm(images), os.path.join(sample_dir, <span class="string">&#x27;real_images.png&#x27;</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存假图片</span></span><br><span class="line">        fake_images = fake_images.reshape(fake_images.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        save_image(denorm(fake_images), os.path.join(sample_dir, <span class="string">&#x27;fake_images-&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(G.state_dict(), <span class="string">&#x27;G.ckpt&#x27;</span>)</span><br><span class="line">    torch.save(D.state_dict(), <span class="string">&#x27;D.ckpt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;效果，分别展示epoch为1、100、200时生成的图片，其中当epoch为200时噪声就已经很少了，但是对数字的分布结构并不能很好的描述出来。<br><img src="https://img-blog.csdnimg.cn/3790f4c34ea848deab7c737aca0c482b.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="2-GAN变种"><a href="#2-GAN变种" class="headerlink" title="2    GAN变种"></a>2    GAN变种</h1><h2 id="2-1-CGAN"><a href="#2-1-CGAN" class="headerlink" title="2.1    CGAN"></a>2.1    CGAN</h2><p>&emsp;&emsp;AVE和GAN都能基于潜在空间的随机向量z生成新图片，GAN生成的图 像比AVE的更清晰，质量更好些。不过它们生成的都是随机的，无法预先控制你要生成的哪类或哪个数。我们希望 生成某个数字，生成某个主题或类别的图像，实现按需生成的目的，这样的应用应该非常广泛。CGAN正是针对这类问题而提出的。</p>
<h3 id="2-1-1-原理"><a href="#2-1-1-原理" class="headerlink" title="2.1.1    原理"></a>2.1.1    原理</h3><p>&emsp;&emsp;在GAN这种完全无监督的方式加上一个标签或一点监督信息，使整个网络就可看成半监督模型。其基本架构与GAN类似，只要添加一个条件y即可，y就是加入的监督信息，比如说MNIST数据集可以提供某个数字的标签 信息，人脸生成可以提供性别、是否微笑、年龄等信息，带某个主题的图像 等标签信息。<br><img src="https://img-blog.csdnimg.cn/c5a1b78cb4ff4ca3a9cf7a02cee1a6b5.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;对生成器输入一个从潜在空间随机采样的一个向量z及一个条件y，生成 一个符合该条件的图像G(z/y)。对判别器来说，输入一张图像x和条件y，输 出该图像在该条件下的概率D(x/y)。</p>
<h3 id="2-1-2-PyTorch实现"><a href="#2-1-2-PyTorch实现" class="headerlink" title="2.1.2    PyTorch实现"></a>2.1.2    PyTorch实现</h3><p>&emsp;&emsp;CGAN实现采用的数据集依然是 MNIST 手写数字数据集，其实现过程与原始的GAN的相差不大，主要差异时是标注信息的添加。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置超参数</span></span><br><span class="line">MAX_EPOCH = <span class="number">50</span></span><br><span class="line">LR_RATE = <span class="number">0.0001</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(log_dir = <span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">sample_dir = <span class="string">&#x27;samples_CGAN&#x27;</span></span><br><span class="line">os.makedirs(sample_dir, exist_ok = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Dataset = datasets.MNIST(root = <span class="string">&#x27;data&#x27;</span>,</span><br><span class="line">                         download = <span class="literal">False</span>,</span><br><span class="line">                         train = <span class="literal">True</span>,</span><br><span class="line">                         transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                                         transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>])]))</span><br><span class="line"></span><br><span class="line">Dataloader = DataLoader(Dataset, batch_size = BATCH_SIZE, shuffle = <span class="literal">True</span>, drop_last = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        self.G = nn.Sequential(nn.Linear(<span class="number">110</span>, <span class="number">256</span>),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Linear(<span class="number">512</span>, <span class="number">1024</span>),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Linear(<span class="number">1024</span>, <span class="number">784</span>),</span><br><span class="line">                               nn.Tanh())</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z, labels</span>) :</span><br><span class="line">        y = self.embedding(labels)</span><br><span class="line">        x = torch.cat([z, y], dim = <span class="number">1</span>)</span><br><span class="line">        out = self.G(x)</span><br><span class="line">        <span class="keyword">return</span> out.view(z.size(<span class="number">0</span>), <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#判别器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) :</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        self.D = nn.Sequential(nn.Linear(<span class="number">794</span>, <span class="number">1024</span>),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Dropout(<span class="number">0.4</span>),</span><br><span class="line">                               nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Dropout(<span class="number">0.4</span>),</span><br><span class="line">                               nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">                               nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                               nn.Dropout(<span class="number">0.4</span>),</span><br><span class="line">                               nn.Linear(<span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">                               nn.Sigmoid())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, labels</span>):</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        y = self.embedding(labels)</span><br><span class="line">        x = torch.cat([x, y], dim = <span class="number">1</span>)</span><br><span class="line">        out = self.D(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment">#Clamp函数x限制在区间[min, max]内</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">denorm</span>(<span class="params">x</span>):</span><br><span class="line">    out = (x + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> out.clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">D = Discriminator().cuda()</span><br><span class="line">G = Generator().cuda()</span><br><span class="line">d_optimizer = optim.Adam(D.parameters(), lr = LR_RATE)</span><br><span class="line">g_optimizer = optim.Adam(G.parameters(), lr = LR_RATE)</span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPOCH) :</span><br><span class="line">    <span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(Dataloader) :</span><br><span class="line">        step = epoch * <span class="built_in">len</span>(Dataloader) + i + <span class="number">1</span></span><br><span class="line">        images, labels = images.reshape(BATCH_SIZE, -<span class="number">1</span>).cuda(), labels.cuda()</span><br><span class="line">        real_labels = torch.ones(BATCH_SIZE, <span class="number">1</span>).cuda()</span><br><span class="line"></span><br><span class="line">        d_optimizer.zero_grad()</span><br><span class="line">        g_optimizer.zero_grad()</span><br><span class="line">        out = D(images, labels)</span><br><span class="line">        real_score = out</span><br><span class="line">        d_loss_real = criterion(out, real_labels)</span><br><span class="line"></span><br><span class="line">        z = torch.randn(BATCH_SIZE, <span class="number">100</span>).cuda()</span><br><span class="line">        fake_labels = torch.randint(<span class="number">0</span>, <span class="number">10</span>, (BATCH_SIZE, )).cuda()</span><br><span class="line"></span><br><span class="line">        fake_images = G(z, fake_labels)</span><br><span class="line">        out = D(fake_images, fake_labels)</span><br><span class="line">        fake_score = out</span><br><span class="line">        d_loss_fake = criterion(out, torch.zeros(BATCH_SIZE, <span class="number">1</span>).cuda())</span><br><span class="line"></span><br><span class="line">        d_loss = d_loss_fake + d_loss_real</span><br><span class="line"></span><br><span class="line">        d_loss.backward()</span><br><span class="line">        d_optimizer.step()</span><br><span class="line"></span><br><span class="line">        d_optimizer.zero_grad()</span><br><span class="line">        g_optimizer.zero_grad()</span><br><span class="line">        z = torch.randn(BATCH_SIZE, <span class="number">100</span>).cuda()</span><br><span class="line">        fake_images = G(z, fake_labels)</span><br><span class="line">        out = D(fake_images, fake_labels)</span><br><span class="line">        g_loss = criterion(out, real_labels)</span><br><span class="line"></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        g_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], d_loss: &#123;:.4f&#125;, g_loss: &#123;:.4f&#125;, D(x): &#123;:.2f&#125;, D(G(z)): &#123;:.2f&#125;&#x27;</span></span><br><span class="line">                  .<span class="built_in">format</span>(epoch, MAX_EPOCH, i + <span class="number">1</span>, <span class="built_in">len</span>(Dataloader), d_loss.item(), g_loss.item(),</span><br><span class="line">                          real_score.mean().item(), fake_score.mean().item()))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存真图片</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) == <span class="number">1</span>:</span><br><span class="line">            images = images.reshape(images.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">            save_image(denorm(images), os.path.join(sample_dir, <span class="string">&#x27;real_images.png&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存假图片</span></span><br><span class="line">        fake_images = fake_images.reshape(fake_images.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        save_image(denorm(fake_images), os.path.join(sample_dir, <span class="string">&#x27;fake_images-&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 可视化损失值</span></span><br><span class="line">        writer.add_scalars(<span class="string">&#x27;scalars&#x27;</span>, &#123;<span class="string">&#x27;d_loss&#x27;</span>: d_loss.item(), <span class="string">&#x27;g_loss&#x27;</span>: g_loss.item()&#125;, step)</span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(G.state_dict(), <span class="string">&#x27;G.ckpt&#x27;</span>)</span><br><span class="line">    torch.save(D.state_dict(), <span class="string">&#x27;D.ckpt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用网格（10×10）的形式显示指定条件下生成的图像。</span></span><br><span class="line">z = torch.randn(<span class="number">100</span>, <span class="number">100</span>).cuda()</span><br><span class="line">labels = torch.LongTensor([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]).cuda()</span><br><span class="line">images = G(z, labels).unsqueeze(<span class="number">1</span>)</span><br><span class="line">grid = make_grid(images, nrow = <span class="number">10</span>, normalize = <span class="literal">True</span>)</span><br><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">ax.imshow(grid.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).detach().cpu().numpy(), cmap = <span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化指定单个数字条件下生成的数字</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_digit</span>(<span class="params">generator, digit</span>) :</span><br><span class="line">    z = torch.randn(<span class="number">1</span>, <span class="number">100</span>).cuda()</span><br><span class="line">    label = torch.LongTensor([digit]).cuda()</span><br><span class="line">    img = generator(z, label).detach().cpu()</span><br><span class="line">    img = <span class="number">0.5</span> * img + <span class="number">0.5</span></span><br><span class="line">    <span class="keyword">return</span> transforms.ToPILImage()(img)</span><br><span class="line">generate_digit(G, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>利用网格（10×10）的形式显示指定条件下生成的图像：<br><img src="https://img-blog.csdnimg.cn/e07f02b2a62c4121a3c9508964bf8645.png#pic_center" alt="在这里插入图片描述"><br>可视化指定单个数字条件下生成的数字：<br><img src="https://img-blog.csdnimg.cn/8e844bf24dda4184b3c7f68404dc2cc3.png#pic_center" alt="在这里插入图片描述"><br>可视化生成器和判别器损失值如下 ：<br><img src="https://img-blog.csdnimg.cn/9b041640e36b4c8d9adf7825ed0b09f1.png#pic_center" alt="在这里插入图片描述"><br>由上图可知，CGAN的训练过程不像一般神经网络的过程，它是判别 器和生成器互相竞争的过程，最后两者达成一个平衡。</p>
<h2 id="2-2-DCGAN"><a href="#2-2-DCGAN" class="headerlink" title="2.2    DCGAN"></a>2.2    DCGAN</h2><p>&emsp;&emsp;在前面中无论是原始的GAN还是CGAN我们建立的网络都是基于全连接网络构建的，这样的网络由于图片的维度较高，网络参数量巨大，不能很好地学习到图片地特征，导致训练效果不佳。DCGAN提出了使用转置卷积层实现的生成网络，普通卷积层来实现的判别网络，大大地降低了网络参数量，同时图片的生成效果也大幅提升，展现了 GAN 模型在图片生成效果上超越 VAE 模型的潜质。注：虽然使用卷积网络会大大降低参数量，但是所需要的样本数要更多一些。<br><img src="https://img-blog.csdnimg.cn/5e90c21b8b094029a3871a8d40e57433.png#pic_center" alt="加粗样式"></p>
<h2 id="2-3-CycleGAN"><a href="#2-3-CycleGAN" class="headerlink" title="2.3    CycleGAN"></a>2.3    CycleGAN</h2><p>CycleGAN 是一种无监督方式，主要用于图片风格相互转换的。CycleGAN 基本的思想是，如果由图片 A 转换到图片 B，再从图片 B 转换到A′，那么A′应该和 A 是同一张图片。因此除了设立标准的 GAN 损失项外，CycleGAN 还增设了循环一致性损失(Cycle Consistency Loss)，来保证A′尽可能与 A 逼近。<img src="https://img-blog.csdnimg.cn/4ec023cd50bc4123a2c2d3031746937b.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-4-WGAN"><a href="#2-4-WGAN" class="headerlink" title="2.4    WGAN"></a>2.4    WGAN</h2><p>&emsp;&emsp;GAN 的训练问题一直被诟病，很容易出现训练不收敛和模式崩塌的现象。WGAN 从理论层面分析了原始的 GAN 使用 JS 散度存在的缺陷，并提出了可以使用 Wasserstein 距 离来解决这个问题。在 WGAN-GP 中，作者提出了通过添加梯度惩罚项，从工程层面很好的实现了 WGAN 算法，并且实验性证实了 WGAN 训练稳定的优点。</p>
<h1 id="3-训练GAN的技巧"><a href="#3-训练GAN的技巧" class="headerlink" title="3    训练GAN的技巧"></a>3    训练GAN的技巧</h1><ul>
<li>批量加载和批规范化，有利于提升训练过程中博弈的稳定性。</li>
<li>使用tanh激活函数作为生成器最后一层，将图像数据规范在-1和1之间，一般不用sigmoid。</li>
<li>选用Leaky ReLU作为生成器和判别器的激活函数，有利于改善梯度的稀疏性，稀疏的梯度会妨碍GAN的训练。 </li>
<li>使用卷积层时，考虑卷积核的大小能被步幅整除，否则，可能导致生成的图像中存在棋盘状伪影。</li>
</ul>
<p>全部代码可以参考<a href="https://github.com/aishangcengloua/MLData/tree/master/PyTorch/GenerateNetwork">此处</a><br><strong>参考</strong></p>
<ul>
<li><strong>《Python深度学习基于PyTorch》</strong></li>
<li><strong>《TensorFlow深度学习》</strong></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>PyTorch</tag>
        <tag>生成对抗网络</tag>
      </tags>
  </entry>
  <entry>
    <title>自监督学习</title>
    <url>/2022/06/05/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="1-Transformer"><a href="#1-Transformer" class="headerlink" title="1    Transformer"></a>1    Transformer</h1><p>Transformer是Sequence-toSequence（Seq2Seq）的一个模型，我们之前在作一些实验的时候，当我们输入一个Sequence时，我们的输出也会是一个Sequence，而输入和输出结果的长度是一样的，当我们不知道输出结果是有多长时，我们便要机器自己决定要输出多长，这就会用到Seq2Seq，特别是在语音辨识及机器翻译中。<br><img src="https://img-blog.csdnimg.cn/8420b887f89d4bfbbc3a69b3cabf76c7.png#pic_center" alt="在这里插入图片描述"><br>一般的Seq2Seq模型是由Encoder和Decoder组成，Encoder接受外界的输入，然后把输出的结果丢给Decoder，再由Decoder决定要输出的Sequence的大小<img src="https://img-blog.csdnimg.cn/0c7feb608f9d448081eed81a79aaff31.png#pic_center" alt="在这里插入图片描述"><br>Seq2seq最早在14年的时候就有了，那时是长的很眉清目秀，后面就变得一言难尽了<br><img src="https://img-blog.csdnimg.cn/2df919b70adc4d648668668bb89c45b8.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/6fd8e35d462b41578578a96462ed6cdc.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder要做的事情就是输入一排向量然后输出一排长度相同向量，这个用RNN或者CNN都能做得到，Encoder用的是self-attention（在我第四篇笔记中有记录到，欢迎大家指正）<img src="https://img-blog.csdnimg.cn/799b3825287f42f3bf6fbc985faf306d.png#pic_center" alt="在这里插入图片描述"><br>现在的Encoder会被分成很多个block，每一个block先做一个self-attention，接受一排向量的输入，考虑全部的资讯，然后输出一排向量，再把结果丢到全连接层再输出一排向量，这就是每一个block的输出，<img src="https://img-blog.csdnimg.cn/b7b21b5cc1414980aa9437ef22899360.png#pic_center" alt="在这里插入图片描述"><br>实际上原来的transformer中，block做的事情更加复杂，在经过self-attention得到一排向量之后，并不会直接丢给全连接层，而是将输入加进来得到新的向量，当成新的output，这种架构叫做residual connection，再将得到的新output做layer normalization（不需要考虑batch），layer normalization在同一个feature中计算不同维度的mean，standard，用公式x’i = （xi - mean）/ std归一化，得到要输入到全连接层的结果，<img src="https://img-blog.csdnimg.cn/683d780331e142f7a415edf6d24b9d63.png#pic_center" alt="在这里插入图片描述"><br>同样的，全连接层里面也有residual connection的架构和normalization，然后才得到一个block的输出</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>这里记录两种常用的Decoder</p>
<h3 id="Autoregressive（AT）"><a href="#Autoregressive（AT）" class="headerlink" title="Autoregressive（AT）"></a>Autoregressive（AT）</h3><p>语音辨识例子：<br>给Encoder吃入”机器学习“的一段语音，Encoder会输出四排向量；对于Decoder来说，他的输入就是encoder的输出，首先Decoder有一个代表开始的符号BEGIN，缩写是BOS<img src="https://img-blog.csdnimg.cn/715b8a0576d1412eaadd11ec7880deed.png#pic_center" alt="在这里插入图片描述"><br>Decoder接受的每一个输入都可以用One-hot-vector来表示，然后Decoder会吐出一排向量，大小是一个字典的长度，比如说做的是中文的语音辨识，中文大概有三四千常用字，所以Decoder吐出向量的长度是三四千。在Decoder中，结果还会经过softmax，最终会给每一个文字一个分数，分数最高的为所需结果<br><img src="https://img-blog.csdnimg.cn/1404668e0ef648f2b3007db4ac9dfc4c.png#pic_center" alt="在这里插入图片描述"><br>同样的，第一个的输出再作为第二次的输入，进行同样过程。<img src="https://img-blog.csdnimg.cn/ce101b78da504ed9b826fd5a90a678cb.png#pic_center" alt="在这里插入图片描述"><br>我们能够发现Decoder的输出会被当成下一次的输入，这也会导致一个问题，就是当Deocder在某一次的输出错误的话就可能会导致后面的结果全部错误。接下来看一下Decoder的内层结果<img src="https://img-blog.csdnimg.cn/9f95166088654f9e86bd18c9aa3bacca.png#pic_center" alt="在这里插入图片描述"><br>可以看出，Decoder除了中间部分和结果处的softmax之外，跟Encoder是差不多的。还有一个地方就是在Decoder里面self-attention变成了Masked self-attention，Masked其实是对self-attention的一个限制，就是让网络由可以考虑全部的资讯变成只能考虑左边的资讯<img src="https://img-blog.csdnimg.cn/1c3451b3285c4f30a74e92b7dc2f184c.png#pic_center" alt="在这里插入图片描述"><br>例如下面，在输出b2时，只用第二个query和第一、二个的key相乘，而不考虑key3和key4<br><img src="https://img-blog.csdnimg.cn/0977980c78be4cee969f63d12c61ddbd.png#pic_center" alt="在这里插入图片描述"><br>因为在Decoder中，输入不是一次性全部输入的，他是先有a1，再有a2，a3，a4，所以当你要输出b2时，是没有a3，a4的。开始的时候我们讲到Decoder是要有一个开始符号的，那类似的，Decoder也有结束的符号end<img src="https://img-blog.csdnimg.cn/175fce77a9314ab59d101724efa51106.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="Non-autoregressive（NAT）"><a href="#Non-autoregressive（NAT）" class="headerlink" title="Non-autoregressive（NAT）"></a>Non-autoregressive（NAT）</h3><p>对于NAT Decoder，他是一次性吃一整排的向量，然后直接生成一个 句子，就很直接，<img src="https://img-blog.csdnimg.cn/c49f93cf2db7409684fb4887c7a4dc90.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Encoder和Decoder之间的桥梁"><a href="#Encoder和Decoder之间的桥梁" class="headerlink" title="Encoder和Decoder之间的桥梁"></a>Encoder和Decoder之间的桥梁</h2><p>Decoder通过产生一个query，到Encoder中抽取资讯，然后当作Decoder中的全连接层的输入，这个过程叫做Cross attention<br><img src="https://img-blog.csdnimg.cn/a29c634964fb4d99a10f88f51e34fd40.png#pic_center" alt="在这里插入图片描述"><br>Decoder的输入接下来的处理是一样的。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>我们还是用语音辨识的例子，假如输入的是”机器学习“，每一个字用一个独热向量表示，但我们经过Decoder得到第一个输出，这个输出是一个概率分布，我们会用”机“对应的独热向量跟输出进行cross entropy的计算<img src="https://img-blog.csdnimg.cn/124fca4530af4c51af1a41c188054dd7.png#pic_center" alt="在这里插入图片描述"><br>每一个输出都有一个cross entropy，而我们就要使总cross entropy loss越小越好，但是要注意的是，我们还要输出end（结束）向量<img src="https://img-blog.csdnimg.cn/bb1ad953d6d34a37af836c9fd9bc3c6f.png#pic_center" alt="在这里插入图片描述"><br>可以观察到我们在训练的时候，Decoder的输入都是正确的，这个叫Teacher Forcing：using the ground truth as input。但我们在测试时看的是自己的输入，可能有时候的输出是错误的，比如说将“器”输出成“气”，就可能导致后面全部错，所以我们在训练过程中需要给model加一些错误的信息让他去训练<img src="https://img-blog.csdnimg.cn/a982959eed6d4ef1b9b81822e6cdf7f8.png#pic_center" alt="在这里插入图片描述"><br>这个叫做Scheduled Sampling。</p>
<h1 id="2-结语"><a href="#2-结语" class="headerlink" title="2    结语"></a>2    结语</h1><p>以上是我本人学习机器学习的学习笔记的第五篇，有错误的地方还望指出，共勉！</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自监督学习</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>自编码器</title>
    <url>/2022/06/05/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&emsp;&emsp;目前我们可以通过爬虫等方式获取海量的样本数据𝒙，如照片、语音、文本等，是相对容易的，但困难的是获取这些数据所对应的标签信息，例如机器翻译，除了收集源语言的对话文本外，还需要待翻译的目标语言文本数据。数据的标注工作目前主要还是依赖人的先验知识来完成。因此，面对海量的无标注数据，我们需要从中学习到数据的分布𝑃(𝒙)的算法，而无监督算法模型就是针对这类问题而发展的。特别地，如果算法把𝒙作为监督信号来学习，这类算法称为自监督学习，本博客介绍的自编码器就属于自监督学习范畴。</p>
<h1 id="1-自编码器"><a href="#1-自编码器" class="headerlink" title="1    自编码器"></a>1    自编码器</h1><h2 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1    原理"></a>1.1    原理</h2><p>&emsp;&emsp;自编码器是通过对输入$x$进行编码后得到一个低维的向量$z$，然后根据 这个向量还原出输入$x$。通过对比$x$与$\bar{x}$的误差，再利用神经网络去训练使得误差逐渐减小，从而达到非监督学习的目的。结构如下图所示。<br><img src="https://img-blog.csdnimg.cn/ad29328da64c473bacd2b582ad6f817e.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;其中我们将数据𝒙本身作为监督信号来指导网络的训练，即希望神经网络能够学习到映射${𝑓_θ}$: $x$ → $x$，我们把网络𝑓𝜃切分为两个部分，前面的子网络尝试学习映射关系：$g_{θ1}$: $x$ → $z$，后面的子网络尝试学习映射关系：$h_{θ2}$：$z$ → $x$。我们把$g_{θ1}$看成一个数据编码(Encode)的过程，作用就是将输入$x$编码成低纬度的隐藏变量$z$，$h_{θ2}$看成一个数据解码(Dncode)的过程，作用是将隐藏变量$z$重塑成高纬度的$x$。编码器和解码器共同完成了输入数据$x$的编码和解码过程，我们把整个网路模型${𝑓_θ}$叫做自动编码器(Auto-Encoder)，如果网络含有多个隐藏层，则称为深度自编码器(Deep Auto-encoder)。<img src="https://img-blog.csdnimg.cn/32d703628a7344b88533bbc59f8d41d2.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;自编码器的编码器通过编码器压缩得到的隐藏变量$z$重塑$\bar{x}$，我们希望解码器的输出能够完美地或者近似恢复出原来的输入，即$x$约等于$\bar{x}$，则自编码器的损失函数可定义为</p>
<script type="math/tex; mode=display">
\begin{aligned}
&Minimize  ℒ = dist(x, \bar{x})\\
&\bar{x} = h_{θ2}(g_{θ1}(x))\\
\end{aligned}</script><p>&emsp;&emsp;其中$dist(x, \bar{x})$表示$x$与 $\bar{x}$ 的距离，常见的距离度量函数为欧氏距离(也即均方差):</p>
<script type="math/tex; mode=display">
\begin{aligned}
&ℒ = \sum_{i}(x - \bar{x})^2\\
\end{aligned}</script><h2 id="1-2-PyTorch实现图片重塑"><a href="#1-2-PyTorch实现图片重塑" class="headerlink" title="1.2    PyTorch实现图片重塑"></a>1.2    PyTorch实现图片重塑</h2><h3 id="1-2-1-Fashion-MNIST-数据集"><a href="#1-2-1-Fashion-MNIST-数据集" class="headerlink" title="1.2.1    Fashion MNIST 数据集"></a>1.2.1    Fashion MNIST 数据集</h3><p>&emsp;&emsp;Fashion MNIST 是一个定位在比 MNIST 图片识别问题稍复杂的数据集，它的设定与MNIST 几乎完全一样，包含了 10 类不同类型的衣服、鞋子、包等灰度图片，图片大小为28 × 28，共 70000 张图片，其中 60000 张用于训练集，10000 张用于测试集。Fashion MNIST 除了图片内容与 MNIST 不一样，其它设定都相同，大部分情况可以直接替换掉原来基于 MNIST 训练的算法代码，而不需要额外修改。由于 Fashion MNIST 图片识别相对于 MNIST 图片更难，因此可以用于测试稍复杂的算法性能。<br><img src="https://img-blog.csdnimg.cn/6d62bb75794546f2b72c93b2cdf5831d.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;在PyTorch中可以直接使用torchvision包进行在线下载。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = datasets.FashionMNIST(root = <span class="string">&#x27;data2&#x27;</span>,</span><br><span class="line">                        train = <span class="literal">False</span>,</span><br><span class="line">                        download = <span class="literal">True</span>,</span><br><span class="line">                        transform = transforms.ToTensor())</span><br></pre></td></tr></table></figure>
<h3 id="1-2-2-网络结构"><a href="#1-2-2-网络结构" class="headerlink" title="1.2.2    网络结构"></a>1.2.2    网络结构</h3><p>&emsp;&emsp;我们利用编码器将输入图片$x ∈ 𝑅^{784}降维到较低维度的隐藏向量z∈ 𝑅^{20}$，并基于隐藏向量 利用解码器重建图片，自编码器模型如图所示，编码器由 3 层全连接层网络组成，输出节点数分别为 256、128、20，解码器同样由 3 层全连接网络组成，输出节点数分别为 128、256、784。<br><img src="https://img-blog.csdnimg.cn/05c32307ae444242a8bb8181aa18991b.png#" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AE</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">20</span>, <span class="number">128</span>)</span><br><span class="line">        self.relu3 = nn.ReLU()</span><br><span class="line">        self.fc5 = nn.Linear(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.relu4 = nn.ReLU()</span><br><span class="line">        self.fc6 = nn.Linear(<span class="number">256</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Encoder</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = self.relu1(self.fc1(x))</span><br><span class="line">        h2 = self.relu2(self.fc2(h1))</span><br><span class="line">        <span class="keyword">return</span> self.fc3(h2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Decoder</span>(<span class="params">self, z</span>):</span><br><span class="line">        h1 = self.relu3(self.fc4(z))</span><br><span class="line">        h2 = self.relu4(self.fc5(h1))</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.fc6(h2))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        z = self.Encoder(x)</span><br><span class="line">        <span class="comment"># print(z.shape)</span></span><br><span class="line">        x_reconst = self.Decoder(z)</span><br><span class="line">        <span class="keyword">return</span> x_reconst</span><br></pre></td></tr></table></figure>
<h3 id="1-2-3-训练"><a href="#1-2-3-训练" class="headerlink" title="1.2.3    训练"></a>1.2.3    训练</h3><p>&emsp;&emsp;自编码器的训练过程与分类器的基本一致，通过误差函数计算出重建向量$\bar{x} 与原始输入向量x$之间的距离。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPOCH) :</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i, (x, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader) :</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        x = x.view(-<span class="number">1</span>, image_size)</span><br><span class="line">        x_reconst = model(x)</span><br><span class="line">        <span class="comment"># 重构损失，使用二元分类损失</span></span><br><span class="line">        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        reconst_loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<h3 id="1-2-4-图片重塑"><a href="#1-2-4-图片重塑" class="headerlink" title="1.2.4    图片重塑"></a>1.2.4    图片重塑</h3><p>&emsp;&emsp;与分类问题不同的是，自编码器的模型性能一般不好量化评价，尽管ℒ值可以在一定程度上代表网络的学习效果，但我们最终希望获得还原度较高、样式较丰富的重建样本。对于图片来说，一般依赖于人工主观的评估。在这次实践中正确做法应是将数据集划分为训练集和测试集，用测试集来进行图片重塑对比，但我为了方便就直接使用训练集来进行重塑了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad() :</span><br><span class="line">    out = model(x)</span><br><span class="line">    <span class="comment">#将与原图与重塑图像进行拼接，奇数列为原图像，偶数列为重塑图像</span></span><br><span class="line">    x_concat = torch.cat([x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), out.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)], dim = <span class="number">3</span>)</span><br><span class="line">    save_image(x_concat, os.path.join(samples_dir, <span class="string">f&#x27;reconst-<span class="subst">&#123;epoch </span></span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/f062f51cf95c490781564ea4c4ec2ed3.png#pic_center" alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;重塑图像如上图，其中奇数列为原图像，偶数列为重塑图像。可以看到，第一个 Epoch 时，图片重建效果较差，图片非常模糊，逼真度较差；随着训练的进行，重建图片边缘越来越清晰，第 100 个 Epoch时，重建的图片效果已经比较接近真实图片。<br>全部代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">MAX_EPOCH = <span class="number">100</span></span><br><span class="line">lr_learning = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">image_size = <span class="number">784</span></span><br><span class="line">os.makedirs(<span class="string">&#x27;samples_AE&#x27;</span>, exist_ok = <span class="literal">True</span>)</span><br><span class="line">samples_dir = <span class="string">&#x27;samples_AE&#x27;</span></span><br><span class="line"></span><br><span class="line">dataset = datasets.FashionMNIST(root = <span class="string">&#x27;data2&#x27;</span>,</span><br><span class="line">                        train = <span class="literal">False</span>,</span><br><span class="line">                        download = <span class="literal">True</span>,</span><br><span class="line">                        transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">data_loader = DataLoader(dataset, shuffle = <span class="literal">True</span>, batch_size = batch_size, drop_last = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AE</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">20</span>, <span class="number">128</span>)</span><br><span class="line">        self.relu3 = nn.ReLU()</span><br><span class="line">        self.fc5 = nn.Linear(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.relu4 = nn.ReLU()</span><br><span class="line">        self.fc6 = nn.Linear(<span class="number">256</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Encoder</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = self.relu1(self.fc1(x))</span><br><span class="line">        h2 = self.relu2(self.fc2(h1))</span><br><span class="line">        <span class="keyword">return</span> self.fc3(h2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Decoder</span>(<span class="params">self, z</span>):</span><br><span class="line">        h1 = self.relu3(self.fc4(z))</span><br><span class="line">        h2 = self.relu4(self.fc5(h1))</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.fc6(h2))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        z = self.Encoder(x)</span><br><span class="line">        <span class="comment"># print(z.shape)</span></span><br><span class="line">        x_reconst = self.Decoder(z)</span><br><span class="line">        <span class="keyword">return</span> x_reconst</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = AE().to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr = lr_learning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPOCH) :</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i, (x, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader) :</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        x = x.view(-<span class="number">1</span>, image_size)</span><br><span class="line">        x_reconst = model(x)</span><br><span class="line">        <span class="comment"># 重构损失，使用二元分类损失</span></span><br><span class="line">        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        reconst_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad() :</span><br><span class="line">        out = model(x)</span><br><span class="line">        <span class="comment">#将与原图与重塑图像进行拼接，奇数列为原图像，偶数列为重塑图像</span></span><br><span class="line">        x_concat = torch.cat([x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), out.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)], dim = <span class="number">3</span>)</span><br><span class="line">        save_image(x_concat, os.path.join(samples_dir, <span class="string">f&#x27;reconst-<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&#x27;</span>))</span><br><span class="line"></span><br><span class="line">img1 = cv.imread(<span class="string">&#x27;samples_AE/reconst-1.png&#x27;</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">&#x27;samples_AE/reconst-50.png&#x27;</span>)</span><br><span class="line">img3 = cv.imread(<span class="string">&#x27;samples_AE/reconst-100.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">images = [img1, img2, img3]</span><br><span class="line">xlabels = [<span class="string">&#x27;epoch : 1&#x27;</span>, <span class="string">&#x27;epoch : 50&#x27;</span>, <span class="string">&#x27;epoch : 100&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>) :</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i], <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.title(xlabels[i])</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="2-自编码器变种"><a href="#2-自编码器变种" class="headerlink" title="2    自编码器变种"></a>2    自编码器变种</h1><h2 id="2-1-降噪自编码器-DAE"><a href="#2-1-降噪自编码器-DAE" class="headerlink" title="2.1    降噪自编码器(DAE)"></a>2.1    降噪自编码器(DAE)</h2><p>&emsp;&emsp;DAE是通过改变重构误差项来获得一个能学到有用信息的自编码器。对于传统的自编码器最小优化目标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&θ^∗ = argmin _θ \quad\ dist(ℎθ2(𝑔θ1(x)), x)\\
\end{aligned}</script><p>&emsp;&emsp;对于这个函数如果模型被赋予过大的容量，损失函数仅仅使得 g ◦ f 学成一个恒等函数。也即网络会简单地复制输入，网络没有学习特征的能力。DAE给网络输入$x$添加采样自高斯分布的噪声$\alpha$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&x^\prime = x + \alpha, \alpha∈𝒩(0, var)\\
\end{aligned}</script><p>则优化目标变成：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&θ^∗ = argmin _θ \quad\ dist(ℎ_{θ2}(𝑔_{θ1}(x^\prime)), x)\\
\end{aligned}</script><p>&emsp;&emsp;其中$x^\prime$是被某种噪声损坏的$x$的副本。因此去噪自编码器必须撤消这些损坏，而不是简单地复制输入。<br><img src="https://img-blog.csdnimg.cn/d5fc0ade729346b794fec89c1219f75d.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="2-2-对抗自编码器-AAE"><a href="#2-2-对抗自编码器-AAE" class="headerlink" title="2.2    对抗自编码器(AAE)"></a>2.2    对抗自编码器(AAE)</h2><p>&emsp;&emsp;为了能够方便地从某个已知的先验分布中𝑝(𝒛)采样隐藏变量𝒛，方便利用𝑝(𝒛)来重建输 入，对抗自编码器利用额外的判别器网络(Discriminator，简称 D网络)来判定降维的隐藏变量𝒛是否采样自先验分布𝑝(𝒛)。判别器网络的输出为一个属于[0,1]区间的变量，表征隐藏向量是否采样自先验分布𝑝(𝒛)：所有采样自先验分布𝑝(𝒛)的𝒛标注为真，采样自编码器的条件概率𝑞(𝒛|𝒙)的𝒛标注为假。通过这种方式训练，除了可以重建样本，还可以约束条件概率分布𝑞(𝒛|𝒙)逼近先验分布𝑝(𝒛)。<br><img src="https://img-blog.csdnimg.cn/4f54d125906e4eae952b11f263763ac8.png" alt="在这里插入图片描述"></p>
<h2 id="2-3-变分自编码器-VAE"><a href="#2-3-变分自编码器-VAE" class="headerlink" title="2.3    变分自编码器(VAE)"></a>2.3    变分自编码器(VAE)</h2><h3 id="2-3-1-原理"><a href="#2-3-1-原理" class="headerlink" title="2.3.1    原理"></a>2.3.1    原理</h3><p>&emsp;&emsp;自编码器因不能随意产生合理的潜在变量，从而导致它无法产生新的内容。因为潜在变量$z$都是编码器从原始图片中产生的。为解决这一问题，研究人员对潜在空间$z$(潜在变量对应的空间)增加一些约束，使$z$满足正态分布，由此就出现了VAE模型，VAE对编码器添加约束，就是强迫它产生服从单位正态分布的潜在变量。正是这种约束，把VAE和自编码器区分开来。<br>&emsp;&emsp;从神经网络的角度来看，VAE 相对于自编码器模型，同样具有编码器和解码器两个子网络。解码器接受输入$x$，输出为隐变量$z$；解码器负责将隐变量$z$解码为重建的$x$。不同的是，VAE 模型对隐变量$z$的分布有显式地约束，希望隐变量$z$符合预设的先验分布P($z$)。因此，在损失函数的设计上，除了原有的重建误差项外，还添加了隐变量$z$分布的约束项。也即我们优化目标希望$z$的分布接近于正态分布。度量图像的相似度一般采用交叉熵(如nn.BCELoss)，度量两个分布 的相似度一般采用KL散度(Kullback-Leibler divergence)。这两个度量的和 构成了整个模型的损失函数。变分自编码器的结构如下：<br><img src="https://img-blog.csdnimg.cn/15df6f69a6084f62ac52c4db7dd35e23.png#pic_center" alt="在这里插入图片描述"><br>模块1：把输入样本$x$通过编码器输出两个m维向量(mu、log_var)，这两个向量是潜在空间(假设满足正态分布)的两个参数(相当于均值和方差)。<br>模块2：从标准正态分布N(0,I)中采样 一个ε。<br>模块3：使得$z$=mu+exp(log_var)*ε。<br>模块4：$z$通过解码器生成一个样本$\bar{x}$。</p>
<p>损失函数的具体代码如下，推导过程：<a href="https://arxiv.org/pdf/1606.05908.pdf">https://arxiv.org/pdf/1606.05908.pdf</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义重构损失函数及KL散度 </span></span><br><span class="line">reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=<span class="literal">False</span>)</span><br><span class="line">kl_div = - <span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + log_var - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - log_var.exp()) </span><br><span class="line"><span class="comment">#两者相加得总损失 loss= reconst_loss+ kl_div</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-2-VAE图片生成"><a href="#2-3-2-VAE图片生成" class="headerlink" title="2.3.2    VAE图片生成"></a>2.3.2    VAE图片生成</h3><p>&emsp;&emsp;此次我们基于 VAE 模型实战MNIST手写数字图片的重建与生成。输入为 MNIST手写数字图片向量，经过 3 个全连接层后得到隐向量𝐳的均值与方差，分别用两个输出节点数为 20 的全连接层表示，FC2 的 20 个输出节点表示 20 个特征分布的均值向量，FC3 的 20 个输出节点表示 20 个特征分布的取log后的方差向量。采样获得长度为 20 的隐向量𝐳，并通过 FC4 和 FC5 重建出样本图片。<br>&emsp;&emsp;VAE 作为生成模型，除了可以重建输入样本，还可以单独使用解码器生成样本。通过从先验分布𝑝(𝐳)中直接采样获得隐向量𝐳，经过解码后可以产生生成的样本。<br><img src="https://img-blog.csdnimg.cn/e8bb31276b44466e88709706a27a1301.png#pic_center" alt="在这里插入图片描述"><br>&emsp;&emsp;此过程的实现与图片的重塑过程相差不大，主要差异在损失函数部分和潜在变量$z$的采样部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">MAX_EPOCH = <span class="number">100</span></span><br><span class="line">lr_learning = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">hidden_size = <span class="number">400</span></span><br><span class="line">z_size = <span class="number">20</span></span><br><span class="line">image_size = <span class="number">784</span></span><br><span class="line">os.makedirs(<span class="string">&#x27;samples&#x27;</span>, exist_ok = <span class="literal">True</span>)</span><br><span class="line">samples_dir = <span class="string">&#x27;samples&#x27;</span></span><br><span class="line"></span><br><span class="line">dataset=datasets.MNIST( root = <span class="string">&#x27;data&#x27;</span>,</span><br><span class="line">                        train = <span class="literal">False</span>,</span><br><span class="line">                        download = <span class="literal">True</span>,</span><br><span class="line">                        transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">data_loader = DataLoader(dataset, shuffle = <span class="literal">True</span>, batch_size = batch_size, drop_last = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, hidden_size, z_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(image_size, hidden_size)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, z_size)</span><br><span class="line">        self.fc3 = nn.Linear(hidden_size, z_size)</span><br><span class="line">        self.fc4 = nn.Linear(z_size, hidden_size)</span><br><span class="line">        self.relu2 = nn.ReLU()</span><br><span class="line">        self.fc5 = nn.Linear(hidden_size, image_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Encoder</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = self.relu1(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc2(h), self.fc3(h)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Reparameterize</span>(<span class="params">self, mu, Log_var</span>):</span><br><span class="line">        std = torch.exp(Log_var / <span class="number">2</span>)</span><br><span class="line">        eps = torch.randn_like((std))</span><br><span class="line">        <span class="keyword">return</span> mu + eps * std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Decoder</span>(<span class="params">self, z</span>):</span><br><span class="line">        h = self.relu2(self.fc4(z))</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.fc5(h))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu, Log_var = self.Encoder(x)</span><br><span class="line">        z = self.Reparameterize(mu, Log_var)</span><br><span class="line">        x_reconst = self.Decoder(z)</span><br><span class="line">        <span class="keyword">return</span> x_reconst, mu, Log_var</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = VAE(image_size, hidden_size, z_size).to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr = lr_learning)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPOCH) :</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i, (x, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader) :</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        x = x.view(-<span class="number">1</span>, image_size)</span><br><span class="line">        x_reconst, mu, Log_var = model(x)</span><br><span class="line">        <span class="comment"># 计算重构损失和KL散度</span></span><br><span class="line">        <span class="comment"># 重构损失</span></span><br><span class="line">        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># KL散度</span></span><br><span class="line">        kl_div = - <span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + Log_var - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - Log_var.exp())</span><br><span class="line">        loss = reconst_loss + kl_div</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if i % 10 == 0 :</span></span><br><span class="line">        <span class="comment">#     print(f&#x27;reconst_loss : &#123;reconst_loss : 0.3f&#125;, kl_div : &#123;kl_div : 0.3f&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad() :</span><br><span class="line">        <span class="comment">#图片生成</span></span><br><span class="line">        z = torch.randn(batch_size, z_size).to(device)</span><br><span class="line">        out = model.Decoder(z).view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        save_image(out, os.path.join(samples_dir, <span class="string">f&#x27;sampled-<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&#x27;</span>))</span><br><span class="line">        <span class="comment">#图片重塑</span></span><br><span class="line">        out, _, _ = model(x)</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        x_concat = torch.cat([x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), out.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)], dim = <span class="number">3</span>)</span><br><span class="line">        save_image(x_concat, os.path.join(samples_dir, <span class="string">f&#x27;reconst-<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>.png&#x27;</span>))</span><br><span class="line"></span><br><span class="line">img1 = cv.imread(<span class="string">&#x27;samples/sampled-1.png&#x27;</span>)</span><br><span class="line">img2 = cv.imread(<span class="string">&#x27;samples/sampled-50.png&#x27;</span>)</span><br><span class="line">img3 = cv.imread(<span class="string">&#x27;samples/sampled-100.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img4 = cv.imread(<span class="string">&#x27;samples/reconst-1.png&#x27;</span>)</span><br><span class="line">img5 = cv.imread(<span class="string">&#x27;samples/reconst-50.png&#x27;</span>)</span><br><span class="line">img6 = cv.imread(<span class="string">&#x27;samples/reconst-100.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">images = [img1, img2, img3, img4, img5, img6]</span><br><span class="line">xlabels = [<span class="string">&#x27;images sample epoch : 1&#x27;</span>, <span class="string">&#x27;epoch : 50&#x27;</span>, <span class="string">&#x27;epoch : 100&#x27;</span>, <span class="string">&#x27;images reconst epoch : 1&#x27;</span>, <span class="string">&#x27;epoch : 50&#x27;</span>, <span class="string">&#x27;epoch : 100&#x27;</span>]</span><br><span class="line">plt.figure(figsize = (<span class="number">15</span>, <span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>) :</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.imshow(images[i], <span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.title(xlabels[i])</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;效果如下，其中重塑图片中奇数列是原图，偶数列为重塑图像。由潜在空间点$z$生成的图像随着epoch的增加是越来越清晰的。<br><img src="https://img-blog.csdnimg.cn/b51ea0edf97f445893d9fc98b50337bd.png#pic_center" alt="在这里插入图片描述"><br><strong>参考</strong><br>《TensorFlow深度学习》<br>《Python深度学习基于PyTorch》</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>PyTorch</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title>详解 Tensorboard 及使用教程</title>
    <url>/2022/06/05/%E8%AF%A6%E8%A7%A3-Tensorboard-%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="一、什么是Tensorboard"><a href="#一、什么是Tensorboard" class="headerlink" title="一、什么是Tensorboard"></a>一、什么是Tensorboard</h1><p>Tensorboard原本是Google TensorFlow的可视化工具，可以用于记录训练数据、评估数据、网络结构、图像等，并且可以在web上展示，对于观察神经网络的过程非常有帮助。PyTorch也推出了自己的可视化工具，一个是<strong>tensorboardX</strong>包，一个是<strong>torch.utils.tensorboard</strong>，二者的使用相差不大，这里介绍后者</p>
<h2 id="二、配置Tensorboard"><a href="#二、配置Tensorboard" class="headerlink" title="二、配置Tensorboard"></a>二、配置Tensorboard</h2><h3 id="环境要求"><a href="#环境要求" class="headerlink" title="环境要求"></a>环境要求</h3><ul>
<li>操作系统：Windows</li>
<li>Python3</li>
<li>PyTorch &gt;= 1.0.0 &amp;&amp; torchvision &gt;= 0.2.1 &amp;&amp; tensorboard &gt;= 1.12.0 1</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><strong>注：</strong> 虽说PyTorch中直接有tensorboard的包，但是有时用的时候还是会报错，所以安装TensorFlow之后torch.utils.tensorboard就可以直接使用且稳定，所以这里介绍安装TensorFlow的方法。</p>
<ul>
<li>pip环境(相对麻烦需要下载依赖项)</li>
</ul>
<blockquote>
<p>pip install tensorflow_gpu=2.5.0 -i <a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a><br>pip install six numpy wheel<br>pip install keras_applications=1.0.6 —no-deps<br>pip install keras_preprocessing=1.0.5 —no-deps</p>
</blockquote>
<ul>
<li>conda环境安装</li>
</ul>
<blockquote>
<p>conda install —channel <a href="https://conda.anaconda.org/anaconda">https://conda.anaconda.org/anaconda</a> tensorflow=2.5.0</p>
</blockquote>
<h1 id="三、Tensorboard的使用"><a href="#三、Tensorboard的使用" class="headerlink" title="三、Tensorboard的使用"></a>三、Tensorboard的使用</h1><p>首先展示该包的使用的大致流程</p>
<blockquote>
<p><strong><em>1)导入tensorboard，实例化SummaryWriter类，指明记录日记路径等信息</em></strong><br>from torch.utils.tensorboard import SummaryWriter</p>
<h1 id="实例化SummaryWriter，并指明日志存放路径。在当前目录如果每月logs目录将自动创建"><a href="#实例化SummaryWriter，并指明日志存放路径。在当前目录如果每月logs目录将自动创建" class="headerlink" title="实例化SummaryWriter，并指明日志存放路径。在当前目录如果每月logs目录将自动创建"></a>实例化SummaryWriter，并指明日志存放路径。在当前目录如果每月logs目录将自动创建</h1><h1 id="如果不写log-dir，系统将会创建runs目录"><a href="#如果不写log-dir，系统将会创建runs目录" class="headerlink" title="如果不写log_dir，系统将会创建runs目录"></a>如果不写log_dir，系统将会创建runs目录</h1><p>writer = SummaryWriter(log_dir = ‘logs’)</p>
<h1 id="调用实例"><a href="#调用实例" class="headerlink" title="调用实例"></a>调用实例</h1><p>writer.add_xxx()</p>
<h1 id="关闭writer"><a href="#关闭writer" class="headerlink" title="关闭writer"></a>关闭writer</h1><p>writer.close()<br><strong><em>2）调用相应的API，接口一般格式为：</em></strong><br>add_xxx(tag_name, object, iteration-number)<br><strong><em>3)启动tensorboard，在命令行中输入</em></strong><br>tensorboard —logdir=r’加logs所在路径’<br><strong><em>4)复制网址在浏览器中打开</em></strong></p>
</blockquote>
<h2 id="使用各种add方法记录数据"><a href="#使用各种add方法记录数据" class="headerlink" title="使用各种add方法记录数据"></a>使用各种add方法记录数据</h2><h3 id="单条曲线-scalar"><a href="#单条曲线-scalar" class="headerlink" title="单条曲线(scalar)"></a>单条曲线(scalar)</h3><blockquote>
<p>add_scalar(tag, scalar_value, global_step=None, walltime=None)</p>
</blockquote>
<p><strong>参数：</strong></p>
<ul>
<li>tag ( string ) – 数据标识符</li>
<li>scalar_value ( float或string/blobname ) – 要保存的值</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒</li>
<li>new_style ( boolean ) – 是使用新样式（张量字段）还是旧样式（simple_value 字段）。新样式可能会导致更快的数据加载。<br>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>) :</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y = 2x&#x27;</span>, x, <span class="number">2</span> * x)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>效果<br><img src="https://img-blog.csdnimg.cn/84389ad9f39d42b8acdb3d56afa737e3.png" alt="在这里插入图片描述"></p>
<h3 id="多条曲线-scalars"><a href="#多条曲线-scalars" class="headerlink" title="多条曲线(scalars)"></a>多条曲线(scalars)</h3><blockquote>
<p>add_scalars( main_tag , tag_scalar_dict , global_step = None , walltime = None)</p>
</blockquote>
<p><strong>参数</strong></p>
<ul>
<li>main_tag ( string ) – 标签的父名称</li>
<li>tag_scalar_dict ( dict ) – 存储标签和对应值的键值对</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒<br>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line">r = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>) :</span><br><span class="line">    writer.add_scalars(<span class="string">&#x27;run_14h&#x27;</span>, &#123;<span class="string">&#x27;xsinx&#x27;</span> : x * np.sin(x / r), </span><br><span class="line">                                  <span class="string">&#x27;xcosx&#x27;</span> : x * np.cos(x / r), </span><br><span class="line">                                  <span class="string">&#x27;xtanx&#x27;</span> : x * np.tan(x / r)&#125;, x)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/0557d6357ab240f7abc4705bbc088b44.png" alt="在这里插入图片描述"></p>
<h3 id="直方图-histogram"><a href="#直方图-histogram" class="headerlink" title="直方图(histogram)"></a>直方图(histogram)</h3><blockquote>
<p>add_histogram( tag , values , global_step = None , bins = ‘tensorflow’ , walltime = None , max_bins = None )</p>
</blockquote>
<p><strong>参数：</strong></p>
<ul>
<li>tag ( string ) – 数据标识符</li>
<li>值（torch.Tensor、numpy.array或string/blobname）– 构建直方图的值</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>bins ( string ) – {‘tensorflow’,’auto’, ‘fd’, …} 之一。这决定了柱的制作方式。</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒<br>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>) :</span><br><span class="line">    x = np.random.randn(<span class="number">1000</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&#x27;distribution of gaussion&#x27;</span>, x, step)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/b7ac08406f24467b94161e9348432b45.png" alt="在这里插入图片描述"></p>
<h3 id="图片-image"><a href="#图片-image" class="headerlink" title="图片(image)"></a>图片(image)</h3><blockquote>
<p>add_image(tag, img_tensor, global_step=None, walltime=None, dataformats = ‘CHW’)</p>
</blockquote>
<p><strong>参数：</strong></p>
<ul>
<li>tag ( string ) – 数据标识符</li>
<li>img_tensor ( torch.Tensor , numpy.array , or string/blobname ) – 图像数据</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒<br>例子：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">&#x27;zhou.jpg&#x27;</span>, cv.IMREAD_COLOR)<span class="comment">#输入图像要是3通道的，所以读取彩色图像</span></span><br><span class="line">img = cv.cvtColor(img, cv.COLOR_BGR2RGB)</span><br><span class="line">img = torch.tensor(img.transpose(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))<span class="comment">#cv读取为numpy图像为(H * W * C)，所以要进行轴转换</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;zhou_ge&#x27;</span>, img, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/2fe87fea05814845867ee262fb405ded.png" alt="在这里插入图片描述"></p>
<h3 id="渲染-figure"><a href="#渲染-figure" class="headerlink" title="渲染(figure)"></a>渲染(figure)</h3><blockquote>
<p>add_figure( tag , figure , global_step = None , close = True , walltime = None )</p>
</blockquote>
<p><strong>参数：</strong></p>
<ul>
<li>tag ( string ) – 数据标识符</li>
<li>image( matplotlib.pyplot.figure ) – 图或图列表</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>close ( bool ) – 自动关闭图形的标志</li>
<li><p>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒</p>
<p>例子：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1000</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line">figure1 = plt.figure()</span><br><span class="line">plt.plot(x, y, <span class="string">&#x27;r-&#x27;</span>)</span><br><span class="line">writer.add_figure(<span class="string">&#x27;my_figure&#x27;</span>, figure1, <span class="number">0</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>效果:<br><img src="https://img-blog.csdnimg.cn/8c9769e21dce47efbe45e082ba787f6c.png" alt="在这里插入图片描述"></p>
<h3 id="网络-graph"><a href="#网络-graph" class="headerlink" title="网络(graph)"></a>网络(graph)</h3><blockquote>
<p>add_graph(model, input_to_model=None, verbose=False, use_strict_trace = True)</p>
</blockquote>
<p><strong>参数：</strong></p>
<ul>
<li>model( torch.nn.Module ) – 要绘制的模型。</li>
<li>input_to_model ( torch.Tensor or list of torch.Tensor ) – 要输入的变量或变量元组</li>
<li>verbose(bool）– 是否在控制台中打印图形结构。</li>
<li>use_strict_trace ( bool ) – 是否将关键字参数严格传递给 torch.jit.trace。当您希望跟踪器记录您的可变容器类型（列表、字典）时传递 False.</li>
</ul>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module) :</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.Net = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">784</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.view(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">        <span class="keyword">return</span> self.Net(<span class="built_in">input</span>)</span><br><span class="line">model = MLP()</span><br><span class="line"><span class="built_in">input</span> = torch.FloatTensor(np.random.rand(<span class="number">32</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">writer.add_graph(model, <span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>效果：<br><img src="https://img-blog.csdnimg.cn/8d4d7286049d4f67a2fd3e306817bf2e.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f9b11c2ca66a4cdba573c6b1b9e1c4f4.png" alt="在这里插入图片描述"></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><strong>嵌入：</strong></p>
<blockquote>
<p>add_embedding(mat, metadata=None, label_img=None, global_step = None, tag=’default’, metadata_header=None)</p>
</blockquote>
<p><strong>参数：</strong></p>
<ul>
<li>mat ( torch.Tensor or numpy.array ) – 一个矩阵，每一行都是数据点的特征向量</li>
<li>metadata ( list ) – 标签列表，每个元素都将转换为字符串</li>
<li>label_img ( torch.Tensor ) – 图像对应每个数据点</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>tag ( string ) – 嵌入的名称</li>
</ul>
<h1 id="三、结语"><a href="#三、结语" class="headerlink" title="三、结语"></a>三、结语</h1><p>以上是关于一些tensorboard可视化的操作，如果您有收获的话，就给👨一个三连。就此谢过！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
</search>
